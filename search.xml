<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[决策树算法介绍及python实现]]></title>
      <url>/2017/08/22/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>决策树算法是机器学习中经典的算法之一，既可以作为分类算法，也可以作为回归算法。在做开始入门机器学习这方面内容时，自然就接触到了这方面的知识。因此，本文对决策树算法进行了一些整理，首先对决策树算法的原理进行介绍，并且用python对决策树算法进行代码实现。<br><a id="more"></a></p>
<h2 id="决策树算法的概述"><a href="#决策树算法的概述" class="headerlink" title="决策树算法的概述"></a>决策树算法的概述</h2><p>决策树算法的思想很类似于我们写代码经常用到的if，else if，else，用什么特征来判断if，这就是决策树算法的精髓所在。</p>
<div align="center"><br>    <img src="http://i.imgur.com/5xkMuwo.jpg" width="400" height="400" alt="决策树示例"><br></div><br>上图是一个结构简单的决策树，用于判断贷款用户是否具备偿款能力，根据不同的条件，从根节点拥有房产开始，根据特征不断判断不断分裂到子节点，叶子节点代表最终的分类结果，树枝分裂的地方就是要根据特征来判断。由此可知，决策树最关键的地方在于树枝分叉的地方，即合理选择特征，从根节点开始，经历子节点的分支后，最终到达叶子。<br><br>从上面来说，决策树算法的基本步骤为：<br>1.从根节点开始<br>2.遍历所有特征作为一种分裂方式，找到最好的分裂特征<br>3.分类成两个或者多个节点<br>4.对子节点同样进行2-3的操作，其实就是第一个递归建树的操作，直到到达叶子节点，即得到分类结果。<br>常见的几种决策树算法有ID3、C4.5、CART，下面将分别进行介绍。<br><br>##   ID3算法<br>理论上来说，其实可以有很多种树能够完成这个分类问题，但是如何将这个分类做的比较优呢？有个叫昆兰的大牛将信息论中的熵引入到决策树算法，解决了这个问题。<br><br>首先，什么是熵？熵的概念源于在物理的热力学，主要是用于描述一个热力学系统的无序程度。信息论的创始人香农将熵引入信息论中，表示不确定性的度量，直观理解为信息含量越多，熵越大，也就是分布越无序。熵的数值定义为：<br><br><img src="http://i.imgur.com/CPbE2UB.gif" alt=""><br><br>X为样例的集合，P(xi)为样例xi的出现概率。<br>分类特征的选择应该使得分类后更加有序，熵减小，在这里引入了熵的增益这个概念。<br><br><img src="http://i.imgur.com/z1tT5ej.gif" alt=""><br><br>其中，Sv为采用特征A分类后的，某一个类别样例数占原有的样例数的比例，H(Sv)该分类后类别的熵，用原来的上减去特征A分类后的每个类别的熵乘于比例权重之后的和。这个时候便得到了选取分类特征的方法，即选取使得熵的增益为最大的特征A作为树枝分裂的特征。ID3(Iterative Dichotomiser 3）其实就是上面介绍的基于熵增益最大的算法。<br>###  ID3算法计算实例<br>下面以一个经典的打网球的例子说明如何构建决策树。例子中打网球（play）主要由天气（outlook)、温度（temperature）、湿度（humidity)、风（windy）来确定，样本数据如下：<br><div align="center"><br>    <img src="http://i.imgur.com/i4FloIa.png" width="400" height="270" alt="样本数据"><br></div><br>在本例中S有14个样例，目标变量是是否打球，play=yes or no,yes有9个样例，no有5个样例，首先计算从根节点的信息熵：<br>H(S)=-(9/14<em>log(9/14))-(5/14</em>log(5/14))=0.28305<br>从根节点开始，有四个特征（outlook temperature humidity windy)可以用来进行树枝分裂，首先需要计算这四个特征的信息熵增益，以outlook为例进行计算。<br><br>特征A（outlook）有三个不同的取值{sunny,overcast,rainy}，这样将原先的数据集S分为3类，其中<br>sunnny有5个样本，2个play=yes，3个play=no<br>overcast有4个样本，4个play=yes<br>rainy有5个样本，3个play=yes，2个play=no<br>根据上面的公式，按outlook分类后的熵为：<br>H（S,A)=5/14sunny熵+4/14overcast熵+5/14rainy熵<br>&emsp;&emsp;&emsp;&emsp;=5/14<em>（-2/5</em>log(2/5)-3/5<em>log(3/5))+4/14</em>(-1<em>log1)+5/14</em>(-3/5<em>log(3/5)-2/5</em>log(2/5))=0.20878<br><br>所以对于outlook的信息熵增益为G(S,outlook)=H(S)-H(S,A)=0.07427<br>用类似的方法，算出，A取其他三个特征时候的信息熵增益分别为：<br>G(S,temperature)=0.00879<br>G(S,humidity)=0.04567<br>G(S,windy)=0.01448<br>显然，用outlook作为特征A时候，熵增益最大，因此作为第一个节点，即为根节点。这样，S就换分为3个子集，sunny，overcast，rainy，其中overcast熵为0，已经分好类，直接作为叶子节点，而sunny，rainy熵都大于0，采取类似于上述的过程继续选择特征，最后可得决策树为：<br><div align="center"><br>    <img src="http://i.imgur.com/ewGSoY2.png" width="400" height="270" alt="最终决策树"><br></div>

<h3 id="ID3算法-python代码实现"><a href="#ID3算法-python代码实现" class="headerlink" title="ID3算法 python代码实现"></a>ID3算法 python代码实现</h3><pre><code>import pandas as pd
import math

def Tree_building(dataSet):
    tree = []
    if(Calculate_Entropy(dataSet) == 0): #熵为0说明分类已经到达叶子节点
        if(dataSet[&apos;play&apos;].sum()==0):  #根据play的值到达0或者1叶子节点
            tree.append(0)
        else:
            tree.append(1)
        return tree
    numSamples=len(dataSet) #样例数
    Feature_Entropy={} #记录按特征A分类后的熵值的字典
    for i in range(1,len(dataSet.columns)-1):
        Set=dict(list(dataSet.groupby(dataSet.columns[i]))) #取出不同的特征
        Entropy=0.0
        for key,subSet in Set.items():
            Entropy+=(len(subSet)/numSamples)*Calculate_Entropy(subSet) #计算熵
        Feature_Entropy[dataSet.columns[i]]=Entropy

    #选最小熵值的特征分类点，这样熵值增益最大    
    Feature = min(zip(Feature_Entropy.values(),Feature_Entropy.keys()))[1] 
    Set=dict(list(dataSet.groupby(Feature)))
    for key,value in Set.items():
        subTree=[]
        subTree.append(Feature)
        subTree.append(key)
        subTree.append(Tree_building(value)) #树枝扩展函数的迭代
        tree.append(subTree)

    return tree

def Calculate_Entropy(data):
    numSamples=len(data)  #样本总数
    P=data.sum()[&apos;play&apos;]  #正例数量
    N=numSamples-P   #反例数量
    if((N==0)or(P==0)):  
        Entropy=0
        return Entropy
    Entropy = -P/numSamples*math.log(P/numSamples)-N/numSamples*math.log(N/numSamples)
    return Entropy

if __name__ == &apos;__main__&apos;:
    data=pd.read_csv(&apos;tennis.csv&apos;)
    tree=Tree_building(data)
    print(tree)
</code></pre><h2 id="C4-5-和CART算法"><a href="#C4-5-和CART算法" class="headerlink" title="C4.5 和CART算法"></a>C4.5 和CART算法</h2><p>ID3算法有一个弊端，因为是选择信息增益最大的特征来分裂，所以更偏向于具有大量属性的特征进行分裂，这样做其实有时候是没有意义的，针对此，有了C4.5算法，对ID3进行了改进。C4.5采用信息增益率来选择分裂特征即：<br>gr(S,A)=gain(S,A)/H(A),<br>其中，gain(S,A)为ID3算法的熵的增益，H(A)为取特征为A的信息熵<br>取A为outlook即为H（outlook）。</p>
<p>未完待续。。。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[也看战狼2：爬取豆瓣影评做词云]]></title>
      <url>/2017/08/08/%E4%B9%9F%E7%9C%8B%E6%88%98%E7%8B%BC2%EF%BC%9A%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84%E5%81%9A%E8%AF%8D%E4%BA%91/</url>
      <content type="html"><![CDATA[<p><img src="http://i.imgur.com/x7xGFON.jpg" alt=""></p>
<p>《战狼2》上映的第二天就去看了，当时觉得还不错，不管是打斗场景还是故事情节，看的都很过瘾，个人觉得可以给4星半。但是这段时间一直看到晚上对《战狼2》各种各样的不同的评论，因此闲暇之余，用爬虫获取了截止于2017.8.8号的豆瓣用户的近14万的评论，对其中的关键词做成了词云。<br><a id="more"></a></p>
<h2 id="python爬虫爬取评论代码"><a href="#python爬虫爬取评论代码" class="headerlink" title="python爬虫爬取评论代码"></a>python爬虫爬取评论代码</h2><pre><code>import requests
from bs4 import BeautifulSoup
import codecs
import time

absolute_url = &apos;https://movie.douban.com/subject/26363254/comments&apos;
url = &apos;https://movie.douban.com/subject/26363254/comments?start={}&amp;limit=20&amp;sort=new_score&amp;status=P&apos;
header={&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0&apos;,&apos;Connection&apos;:&apos;keep-alive&apos;}




def html_prase(html, struct):
    soup=BeautifulSoup(html,&apos;lxml&apos;)
    comment_nodes = []
    comment_nodes = soup.select(struct)
    xiangdui_link_nodes= soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;)
    return comment_nodes,xiangdui_link_nodes

if __name__ == &apos;__main__&apos;:
    #读取cookie数据
    f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
    cookies = {}
    for line in f_cookies.read().split(&apos;;&apos;):
        name, value = line.strip().split(&apos;=&apos;, 1)
        cookies[name] = value
    f = codecs.open(&quot;comments.txt&quot;, &apos;a&apos;, encoding=&apos;utf-8&apos;)
    html = requests.get(url, cookies=cookies, headers=header).content
    comment_nodes=[]
    xiangdui_links=[]
    #获取评论
    comment_nodes,xiangdui_link_nodes = html_prase(html , &apos;.comment &gt; p&apos;)
    soup = BeautifulSoup(html, &apos;lxml&apos;)
    comment_list = []
    for node in comment_nodes:
        comment_list.append(node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;)
    while(xiangdui_link_nodes!=[]):#每次查看是否有后页，即不断往深处挖掘，获取数据
        xiangdui_link = soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;) #取出后页的相对链接
        xiangdui_links.append(xiangdui_link)
        time.sleep(1)
        html = requests.get(absolute_url+xiangdui_link_nodes, cookies=cookies, headers=header).content
        soup = BeautifulSoup(html, &apos;lxml&apos;)
        comment_nodes, xiangdui_link_nodes = html_prase(html, &apos;.comment &gt; p&apos;)
        for node in comment_nodes:
            comment = node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;
            comment_list.append(comment)
            f.writelines(comment)    
</code></pre><p>在抓取豆瓣影评的时候，一开始我是直接对URL爬虫，仅仅是加了一个header，抓取一段时间，豆瓣的反爬虫策略就将我的IP封掉了，所以我又加入了cookie字段。cookie是一个字典类型的数据，可以以比较简单的方式获取。打开要浏览的豆瓣页面，点击登陆页面后，打开Chrome的开发者模式，开始监听登陆时候的http请求和响应。<br><img src="http://i.imgur.com/AK1Gm88.png" alt=""></p>
<p>这个时候，在Cookie字段可以找到cookie数据，复制后存为txt文件，然后写代码读取txt文件，并存为字典格式数据。</p>
<pre><code>f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
cookies = {}
for line in f_cookies.read().split(&apos;;&apos;):
    name, value = line.strip().split(&apos;=&apos;, 1)
    cookies[name] = value
</code></pre><p>用requests对网页进行爬虫抓取之后，此后就是利用Beautifulsoup对获取的html进行解析，获取豆瓣用户评论，以及后页的链接。<br><img src="http://i.imgur.com/8gnYM4R.png" alt=""></p>
<p>对《战狼2》的豆瓣影评链接进行分析，发现每一页链接都是如上图的组成，网页解析可以获取后面的红色字段，实现不断向后页爬虫。最后爬虫结果得到18M左右的数据。</p>
<h2 id="jieba模块提取评论内容关键词"><a href="#jieba模块提取评论内容关键词" class="headerlink" title="jieba模块提取评论内容关键词"></a>jieba模块提取评论内容关键词</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Wed Aug  9 09:51:51 2017

@author: lkj
&quot;&quot;&quot;
import codecs
import jieba
import matplotlib.pyplot as plt  
import matplotlib as mpl 
import numpy as np 
from collections import Counter

zhfont1 = mpl.font_manager.FontProperties(fname=&apos;C:\Windows\Fonts\simsun.ttc&apos;)

def draw_bar(labels,quants):  
    width = 0.4  
    ind = np.linspace(0.5,9.5,10)  
    # make a square figure  
    fig = plt.figure(1)  
    ax  = fig.add_subplot(111)  
    # Bar Plot  
    ax.bar(ind-width/2,quants,width,color=&apos;green&apos;)  
    # Set the ticks on x-axis  
    ax.set_xticks(ind)  
    ax.set_xticklabels(labels,fontproperties=zhfont1)  
    # labels  
    ax.set_xlabel(u&apos;关键词&apos;,fontproperties=zhfont1)  
    ax.set_ylabel(u&apos;评论数量&apos;,fontproperties=zhfont1)  
    # title  
    ax.set_title(u&apos;筛选后的TOP10关键词&apos;, bbox={&apos;facecolor&apos;:&apos;0.8&apos;, &apos;pad&apos;:5},fontproperties=zhfont1)  
    #plt.legend(prop=zhfont1)
    plt.grid(True)  
    plt.show()   

word_lists = []  # 关键词列表
with codecs.open(&apos;comments.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;) as f:
    Lists = f.readlines()  # 文本列表
    for List in Lists:
        cut_list = list(jieba.cut(List))
        for word in cut_list:
            word_lists.append(word)
word_lists_set = set(word_lists)  # 去除重复元素
sort_count = []
word_lists_set = list(word_lists_set)
length = len(word_lists_set)
print(u&quot;共有%d个关键词&quot; %length)
k = 1
for w in word_lists_set:
    sort_count.append(w + u&apos;:&apos; + str(word_lists.count(w)) + u&quot;次\n&quot;)
    print (u&quot;%d---&quot; % k + w + u&quot;:&quot; + str(word_lists.count(w)) + u&quot;次&quot;)
    k += 1
with codecs.open(&apos;count_word.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:
    f.writelines(sort_count)
#先取出前100关键词，再进行人为筛选
key_words_TOP100=[]
key_words_TOP100=Counter(word_lists).most_common(100)
key_words_shaixuan=[key_words_TOP100[6],key_words_TOP100[24],key_words_TOP100[25],
                    key_words_TOP100[30],key_words_TOP100[39],key_words_TOP100[52],
                    key_words_TOP100[60],key_words_TOP100[77],key_words_TOP100[78],
                    key_words_TOP100[94]]
labels = []
quants = []
for i in range(10):
    labels.append(key_words_shaixuan[i][0])
    quants.append(key_words_shaixuan[i][1])
draw_bar(labels,quants)
</code></pre><p>绘制柱形图的时候需要指定字体，不然会出现中文乱码。对关键词TOP100需要进行人为筛选，因为jieba分词会出现很多诸如“我们”之类的在这里无意义的词汇，人为筛选出TOP10关键词如下：</p>
<p><img src="http://i.imgur.com/M2v23dA.png" alt=""></p>
<p>从关键词来看，大多数网友还是看好这部电影的，认为这是大场面的动作戏，达到了好莱坞大片水平，当然也不乏网友认为这是满足吴京个人英雄主义的意淫。</p>
<h2 id="绘制词云"><a href="#绘制词云" class="headerlink" title="绘制词云"></a>绘制词云</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Tue Aug  8 21:46:04 2017

@author: lkj
&quot;&quot;&quot;

# -*- coding:utf-8 -*-
import codecs

import jieba
from scipy.misc import imread
from wordcloud import WordCloud


# 绘制词云
def save_jieba_result():
    # 设置多线程切割
    #jieba.enable_parallel(4)
    with codecs.open(&apos;comments.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()
    cut_text = &quot; &quot;.join(jieba.cut(comment_text))  # 将jieba分词得到的关键词用空格连接成为字符串
    with codecs.open(&apos;jieba.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f:
        f.write(cut_text)


def draw_wordcloud2():
    with codecs.open(&apos;jieba.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()

    color_mask = imread(&quot;zhanlang2.jpg&quot;)  # 读取背景图片

    stopwords = [u&apos;就是&apos;, u&apos;电影&apos;, u&apos;你们&apos;, u&apos;这么&apos;, u&apos;不过&apos;, u&apos;但是&apos;, u&apos;什么&apos;, u&apos;没有&apos;, u&apos;这个&apos;, u&apos;那个&apos;, u&apos;大家&apos;, u&apos;比较&apos;, u&apos;看到&apos;, u&apos;真是&apos;,
                 u&apos;除了&apos;, u&apos;时候&apos;, u&apos;已经&apos;, u&apos;可以&apos;,u&apos;湄公河&apos;]
    cloud = WordCloud(font_path=&quot;MSYH.TTF&quot;, background_color=&apos;white&apos;,
                      max_words=2000, max_font_size=200, min_font_size=4, mask=color_mask,stopwords=stopwords)
    word_cloud = cloud.generate(comment_text)  # 产生词云
    word_cloud.to_file(&quot;zhanlang2_cloud.jpg&quot;)

save_jieba_result()
draw_wordcloud2()
</code></pre><p>词云的绘制需要需要指定font_path，不然会出现中文乱码，我在网上下好微软雅黑的字体（.TTF文件）一并放在目录下调用。</p>
<h2 id="也想说两句"><a href="#也想说两句" class="headerlink" title="也想说两句"></a>也想说两句</h2><p>电影的后面，红旗飘扬，进过敌战区的吴京身披五星红旗大摇大摆经过，那一刻真的为作为一个中国人而感到自豪。<br><img src="http://i.imgur.com/bUYDPL5.jpg" alt=""><br>有人觉得看的剧情尴尬，完全是吴京个人英雄主义的表现，但是在《看速度与激情》一个人干翻整个俄罗斯核基地为什么不觉得尴尬呢？我们接受了太多的美国大片以及美国的个人英雄主义的意识形态的输出，有美国队长能够拯救世界，为什么中国队长不行呢？现在我们的国家也是越来越强大，很欣慰能有《湄公河行动》、《战狼2》这样的主旋律大片，虽然有很多不好的声音，也有越来越多的人被感染，而认同，《战狼2》上映两周就登顶国内票房冠军就印证了这一切。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&id=491295324&auto=0&height=66"></iframe>

<p>不可否认，在调动自然流露爱国情愫上，《战狼2》是成功的，它在我不知不觉的情感代入里推揉了我的泪腺。当舰长青筋怒暴将憋在心中已久的爱国情感汇成一句“开火”时，男主冷锋独自潜入暴乱的非洲国家拼尽全力解救侨胞、身处险境孤立无援的那种英雄悲凉绝望感，瞬间倾倒而出。祖国，在这一刻，有了最真切感受。</p>
<p>当然，《战狼2》还是存在很多瑕疵，但是也为国产电影树立了一个新的标杆，相信国产大片也会越来越好，期待有更多像《战狼2》这样优秀的电影。</p>
<p>最后，很喜欢这部电影的结尾，中华人民共和国的护照。<br><img src="http://i.imgur.com/DG2aone.jpg" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> python </tag>
            
            <tag> 电影 </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hexo-github搭建个人博客]]></title>
      <url>/2017/08/06/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>搭好个人网站之后纠结于第一篇博客的内容，想想还是首先分享一下自己搭建这个网站的过程，毕竟还是碰到了很多坑，解决了一些问题才成功的。<br><a id="more"></a></p>
<h2 id="配置准备"><a href="#配置准备" class="headerlink" title="配置准备"></a>配置准备</h2><p>配置环境是win10，其实在linux和mac环境下的搭建过程也是类似。</p>
<ol>
<li>安装node。一种静态博客框架，到<a href="https://nodejs.org/en/" title="Node.js官网" target="_blank" rel="external">Node.js官网</a>上下载，根据自己电脑版本下载对应文件，按照安装提示一路安装即可。</li>
<li>GitHub账号申请，已有可以跳过。Github用来做博客的远程仓库，后期需要配置域名，以及和本地的hexo创建连接。没有账号需要在GitHub官网进行申请，以及SSH key的配置，网上可以搜到很多教程。</li>
<li>git的安装。把本地的hexo上传到GitHub上去，安装过程可以参考<a href="https://git-scm.com/book/zh/v1/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git" target="_blank" rel="external">官方教程</a>。</li>
</ol>
<h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><p>Node.js和Git安装好之后就可以正式安装了，首先创建一个空文件夹比如hexo，在cmd窗口下进入到这个文件夹，此后的命令都是在这个文件夹下进行。</p>
<p>执行如下命令安装hexo：</p>
<pre><code>sudo npm install -g hexo
</code></pre><p>而后初始化hexo：</p>
<pre><code>hexo init
</code></pre><p>生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>这时候在启动本地服务，就能在本地浏览器进行预览：<br>    hexo server</p>
<p>浏览器输入<a href="http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。" target="_blank" rel="external">http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。</a></p>
<h2 id="配置GitHub"><a href="#配置GitHub" class="headerlink" title="配置GitHub"></a>配置GitHub</h2><p>首选需要建立仓库（repository),仓库的名字格式必须为：你的GitHub账户名+.github.io,比如说你的github账户名为abc，那么你的仓库名为abc.github.io</p>
<p>同时，本地的hexo文件夹可以看到以下内容：<br><img src="http://i.imgur.com/NL4v9X7.png" alt=""><br>打开_config.yml文件，下滑到最后进行编辑，修改成下面的样子：</p>
<pre><code>deploy:
  type: git
  repo: https://github.com/xxx/xxx.github.io.git
  branch: master
</code></pre><p><strong>注意</strong>：xxx需要改为你自己的github账户名，而且格外注意的是，所有的冒号后面必须加一个空格，否则hexo指令会执行不成功。</p>
<p>执行hexo generate命令生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>执行命令安装hexo的git上传工具：</p>
<pre><code>npm install hexo-deployer-git --save
</code></pre><p>再执行配置命令：</p>
<pre><code>hexo deploy
</code></pre><p>命令时终端会提示你输入Github的用户名和密码，输入配置好后，本地文件就会上传至你的github仓库。</p>
<h2 id="域名绑定"><a href="#域名绑定" class="headerlink" title="域名绑定"></a>域名绑定</h2><p>笔者是在阿里云上后买的<a href="lkj666.top">lkj666.top</a>域名，然后需要与github page进行绑定。<br>在域名解析管理的页面下，添加以下几条：<br><img src="http://i.imgur.com/609hZBT.png" alt=""><br>其中：</p>
<ol>
<li>CNAME记录类型会将你的域名进行别名指向。可以为一个主机设置别名在这里可以设置自己的域名指向xxx.github.io，以后就可以用自己的域名来代替访问xxx.github.io.</li>
<li>此外还增加了两条A类型，来指定github.com的IP地址，以及xxx.github.io的IP地址，可以用ping指令的方式来获取。</li>
</ol>
<p>此外，还需要修改本地以及仓库的CNAME文件：<br>在本地hexo的source文件夹下新建一个CNAME文件，将自己的域名写入，比如我将自己的域名lkj666.top写入CNAME文件一定要记住，是所有文件格式，不是txt文件格式。<br>在对应的xxx.github.io仓库根目录下新建一个文件CNAME，同样将自己的域名写入。</p>
<p>做好上述过程后再执行下列命令:</p>
<pre><code>hexo clean //每次执行提交之前最好执行这个clean命令来清除缓存
hexo generate
hexo deploy
</code></pre><p>此后就可以通过你的域名访问你自己的博客了。</p>
]]></content>
      
        
    </entry>
    
  
  
</search>
