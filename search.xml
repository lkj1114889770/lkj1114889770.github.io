<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[自组织特征映射神经网络（SOM）]]></title>
      <url>/2017/09/30/%E8%87%AA%E7%BB%84%E7%BB%87%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88SOM%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>自组织特征映射神经网络（SOM）设计思想基于人体大脑，外界信息输入时，大脑皮层对应区域的部分神经元会产生兴奋，位置临近的神经云也会有相近的刺激。大脑神经元的这种特点，不是先天安排好的，而是通过后天的自学习组织形成的。芬兰Helsinki大学的Kohonen教授提出了一种成为自组织特征映射的神经网络模型。<br><a id="more"></a></p>
<h2 id="SOM介绍"><a href="#SOM介绍" class="headerlink" title="SOM介绍"></a>SOM介绍</h2><p>SOM与kmeans算法有点相似，其基本思想也是，将距离小的个体集合划分为同一类别，而将距离大的个体划分为不同的类别。</p>
<p><img src="http://appwk.baidu.com/naapi/doc/view?ih=584&amp;o=png_6_0_0_436_680_396_210_876_1252.5&amp;iw=1100&amp;ix=0&amp;iy=414&amp;aimw=1100&amp;rn=1&amp;doc_id=d6e8e02c647d27284b735162&amp;pn=1&amp;sign=b919aed799fa017c1e7f322b49ebb944&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="SOM结构图"></p>
<p>从结构上看，SOM比较简单，只有两层，输入层和竞争层，只有输入层到竞争层的权重向量需要训练，不同于其他神经网络，竞争层同层之间的神经元还有侧向连接，在学习的过程中还会相互影响。竞争层神经元的竞争通过神经元对应的权重向量和输入样本的距离比较，距离最近的神经元成为获胜节点。</p>
<p>常见的相互连接的调整方式有以下几种</p>
<ol>
<li>墨西哥草帽函数：获胜节点，有最大的权值调整量，离获胜节点越远，调整量越小，甚至达到某一距离，调整量还会变为负值。如图a所示。</li>
<li>大礼帽函数：墨西哥草帽函数的简化，如图b所示。</li>
<li>厨师帽函数：大礼帽函数的简化，如图c所示。</li>
</ol>
<p><img src="https://i.imgur.com/JtGnkfW.png" alt=""></p>
<h2 id="SOM算法学习过程"><a href="#SOM算法学习过程" class="headerlink" title="SOM算法学习过程"></a>SOM算法学习过程</h2><h3 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h3><p>输入层网络节点数与输入样本维度（列数）相同，通常需要进行数据归一化，常见的方法是标准归一化。<br>竞争层网络根据数据维度以及分类类别数来确定，二维数据和4种分类的话<br>，竞争层含4个节点，但是权重矩阵为4X2，权重矩阵的初始化一般·随机给一个0-1之间的随机值。对于分类类别数目不清楚的情况，可以定义竞争层多于可能的实际分类的节点，这样最后训练结果中不对应分类结果的节点始终不会收到刺激而兴奋，即抑制。</p>
<p>学习率会影响收敛速度，一般定义一个动态的学习率，随迭代次数增加而递减。优胜邻域半径也定义为一个动态收缩的，随着迭代次数增加而递减。</p>
<pre><code># 学习率和学习半径函数    
def ratecalc(self,indx):
    lrate = self.lratemax-(float(indx)+1.0)/float(self.Steps)*(self.lratemax-self.lratemin) 
    r = self.rmax-(float(indx)+1.0)/float(self.Steps)*(self.rmax-self.rmin)
    return lrate,r
</code></pre><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>训练过程如下：</p>
<ol>
<li>随机抽取一个数据样本，计算竞争层中神经元对应的权重矩阵与数据样本的距离，找到距离最近的为获胜节点。</li>
<li>根据优胜邻域半径，找出此邻域内的所有节点。</li>
<li>根据学习率调整优胜邻域半径内的所有节点，然后回到步骤1进行迭代，直到到达相应的迭代次数</li>
<li><p>根据最终的迭代结果，为分类结果分配标签。</p>
<h1 id="主算法"><a href="#主算法" class="headerlink" title="主算法"></a>主算法</h1><p> def train(self):</p>
<pre><code>#1 构建输入层网络
dm,dn = shape(self.dataMat) 
normDataset = self.normalize(self.dataMat) # 归一化数据x
#2 构建分类网格
grid = self.init_grid() # 初始化第二层分类网格 
#3 构建两层之间的权重向量
self.w = random.rand(dn,self.M*self.N); #随机初始化权值 w
distM = self.distEclud   # 确定距离公式
#4 迭代求解
if self.Steps &lt; 10*dm:    self.Steps = 10*dm   # 设定最小迭代次数
for i in range(self.Steps):     
    lrate,r = self.ratecalc(i) # 计算学习率和分类半径
    self.lratelist.append(lrate);self.rlist.append(r)
    # 随机生成样本索引，并抽取一个样本
    k = random.randint(0,dm) 
    mySample = normDataset[k,:]     

    # 计算最优节点：返回最小距离的索引值
    minIndx= (distM(mySample,self.w)).argmin()
    d1 = int(round(minIndx - r));
    d2 = int(round(minIndx + r));

    if(d1&lt;0): 
        d1=0
    if(d2&gt;(shape(self.w)[1]-1)):
        d2= shape(self.w)[1]-1
    di=d1;
    #print(d1,d2)
    while(di&lt;=d2):
        self.w[:,di] = self.w[:,di]+lrate*(mySample[0]-self.w[:,di])
        di=di+1

# 分配类别标签
for i in range(dm):
    self.classLabel.append(distM(normDataset[i,:],self.w).argmin())
self.classLabel = mat(self.classLabel)        
</code></pre></li>
</ol>
<p>具体实现代码可见：<a href="https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/SOM" target="_blank" rel="external">https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/SOM</a></p>
<p>针对数据集的分类结果：</p>
<p><img src="https://i.imgur.com/KSlpNem.png" alt=""></p>
<p><strong><br>参考文献：</strong></p>
<ol>
<li>《机器学习算法与编程实践》 郑捷著；</li>
</ol>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[浓烟下的诗歌电台]]></title>
      <url>/2017/09/22/%E6%B5%93%E7%83%9F%E4%B8%8B%E7%9A%84%E8%AF%97%E6%AD%8C%E7%94%B5%E5%8F%B0/</url>
      <content type="html"><![CDATA[<p><img src="http://clubimg.dbankcdn.com/data/attachment/forum/201612/08/192259beyumvep5wmavc1e.jpg" alt=""></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="530" height="86" src="//music.163.com/outchain/player?type=2&id=31445772&auto=0&height=66"></iframe>

<p>诗化的语言<br>堆砌的意向<br>磁性的烟嗓<br>浓烟下的诗歌电台</p>
<p>有颗远方的心，依然想去追寻。</p>
]]></content>
      
        
        <tags>
            
            <tag> 音乐 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[K-means Clustering]]></title>
      <url>/2017/09/22/K_means%20Clustering/</url>
      <content type="html"><![CDATA[<p>分类通常是一种监督式学习算法，事先都知道标签信息，但是实际上很多情况下都不知道标签信息，这个时候就经常用到聚类算法（Clustering），属于无监督学习的一种，本文介绍无监督学习中典型的一种k-means聚类算法。<br><a id="more"></a></p>
<h2 id="conventional-k-means"><a href="#conventional-k-means" class="headerlink" title="conventional k-means"></a>conventional k-means</h2><p>k-means聚类算法的思想很简单，就是将数据相似度最大的聚集在一起为一类，如何衡量数据之间的相似度，通常用欧几里得距离来表示：</p>
<p><img src="https://i.imgur.com/lhYz2GX.png" alt=""></p>
<p>有时候也用余弦向量来度量：</p>
<p><img src="https://i.imgur.com/6DqsyIq.png" alt=""></p>
<p>算法过程也比较简单：</p>
<ol>
<li>从数据集D中随机取k个元素，作为初始k个簇的中心</li>
<li>计算剩下的元素到k个中心的距离，并将其归类到离自己最近的簇中心点对应的簇</li>
<li>重新计算每个簇的中心，采用簇中所有元素各个维度的算数平均数</li>
<li>若新的簇的中心不变，或者在变化阈值内，则聚类结束，否则重新回到第2步。</li>
</ol>
<h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means++"></a>k-means++</h2><p>k-means算法的一个弊端就是，初始选取的随机k个中心会对最终实际聚类效果影响很大，基于此对于初始k个点选取进行了改进，即k-measn++算法。<br>基本思想就是，选取的初始k个点的距离尽可能远。</p>
<ol>
<li>首先随机选择一个点作为第一个簇中心点</li>
<li>计算其余点与最近的一个簇中心点的距离D(x)保存在一个数组，并累加得到和sum（D(X))。</li>
<li>再在（0，1）取一个随机值Random，sum*Random对应的D(x)区间即为选中的下一个聚类中心（因为D(X)越大，被选中的概率越大）。</li>
<li>重复2 3步骤知直到k个初始聚类中心都被找出来</li>
<li>再进行上面的k-means聚类算法。</li>
</ol>
<h2 id="Kernel-k-means"><a href="#Kernel-k-means" class="headerlink" title="Kernel k-means"></a>Kernel k-means</h2><p>当数据无法线性可分的时候，k-means算法也无法进行分类，类似于SVM，将分类空间推广到更广义的度量空间，即为kernel k-means.</p>
<p><img src="https://i.imgur.com/FkIdHw1.jpg" alt=""></p>
<p>将点从原来的空间映射到更高维度的特征空间，则距离公式变成：</p>
<p><img src="https://i.imgur.com/QSX5K1g.png" alt=""></p>
<p>常见的核函数有：<br>Linear Kernel:</p>
<p><img src="https://i.imgur.com/Pamhtvp.png" alt=""></p>
<p>Polynomial Kernel:</p>
<p><img src="https://i.imgur.com/8c4UlBi.png" alt=""></p>
<p>Gaussian Kernel：</p>
<p><img src="https://i.imgur.com/Q0fK08M.png" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[朴素贝叶斯分类]]></title>
      <url>/2017/09/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/</url>
      <content type="html"><![CDATA[<p>贝叶斯算法是分类算法的一种，算法的核心是概率论中的贝叶斯定理，而朴素贝叶斯则是贝叶斯分类算法中最简单的一种。<br><a id="more"></a></p>
<h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>贝叶斯算法的核心就是贝叶斯定理：</p>
<p><img src="https://i.imgur.com/Mc59jtP.png" alt=""></p>
<p>而在分类算法中，B为某个类别，A为特征，P(B|A)为某个待分类个体的特征取不同类别的概率，显然是计算取不同类别的概率，取最大概率作为该个体的分类类别；上式右边的三个式子的取值通常可以根据已知数据集即训练集求取。</p>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯的“朴素”体现在样本的各个特征都是条件独立的，这样就极大地简化了算法，当然这是一个比较强的条件，在实际应用中不一定符合。算法流程如下：</p>
<ol>
<li>样本x的特征集合{a1,a2,…am}</li>
<li>所有分类类别取值{y1,y2,…yn}</li>
<li>计算所有P(y1|x),P(y2|x)….,P(yn|x)</li>
<li>取3中最大概率对应的分类为x的分类</li>
</ol>
<p>根据贝叶斯定理，可以转换成求解：</p>
<p><img src="https://i.imgur.com/rRISY1C.png" alt=""></p>
<p>因为特征之间的独立性，那么有：</p>
<p><img src="https://i.imgur.com/4NRfZsa.png" alt=""></p>
<p>而对于P(yi)可以从样本中得到，分母特征的取值概率通常对于一组特定的特征组合为常数，可以不用求解，至此完成了朴素贝叶斯算法的介绍。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[kaggle入门：Predict survival on the Titanic]]></title>
      <url>/2017/09/12/kaggle%E5%85%A5%E9%97%A8%EF%BC%9APredict-survival-on-the-Titanic/</url>
      <content type="html"><![CDATA[<p><a href="https://www.kaggle.com/" target="_blank" rel="external">Kaggle</a>是一个数据分析建模的应用竞赛平台，学习了机器学习的算法，这是个很好的应用平台。Predict survival on the Titanic作为kaggle入门级别的比赛，也是我接触kaggle的第一个实践项目，最后的结果虽然不够优秀，仅Top20%左右，但是还是将第一次的实践过程mark一下。<br><img src="https://i.imgur.com/FEDyFse.jpg" alt=""></p>
<a id="more"></a>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>从kaggle的网站上下载好比赛数据（train.csv和test.csv），泰坦尼克号问题就是根据乘客的个人信息，分析是否能够活下来，训练集提供了乘客信息以及存活状况，测试集仅提供信息，需要预测能否存活，其实就是一个二分类问题。</p>
<p>下面读入训练集数据，开始初步分析。</p>
<pre><code>import pandas as pd
import numpy as np
from pandas import Series,DataFrame
data_train = pd.read_csv(&quot;train.csv&quot;)
data_train.info()
</code></pre><p>可以看到数据信息：</p>
<pre><code>RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
</code></pre><p>数据中存在缺项，其中不同的字段表示的信息为</p>
<p>PassengerId    乘客ID<br>Survived       是否存活<br>Pclass         船舱等级<br>Name           姓名<br>Sex            性别<br>Age            年龄<br>SibSp          兄弟姐妹个数<br>Parch          父母小孩个数<br>Ticket         船票信息<br>Fare           票价<br>Cabin          船舱<br>Embarked       登船港口</p>
<pre><code>import matplotlib.pyplot as plt
fig = plt.figure()
fig.set(alpha=0.2)  # 设定图表颜色alpha参数
data_train.Age[data_train.Survived == 0].plot(kind=&apos;kde&apos;)   
data_train.Age[data_train.Survived == 1].plot(kind=&apos;kde&apos;)
plt.xlabel(u&quot;年龄&quot;)
plt.ylabel(u&quot;密度&quot;)
plt.title(u&quot;从年龄看获救情况&quot;)
plt.legend((u&quot;未获救&quot;,u&quot;获救&quot;),loc=&apos;best&apos;)
plt.show()
</code></pre><p><img src="https://i.imgur.com/Dt77VZG.png" alt=""></p>
<p>年龄越小，获救的概率还是越高的，小孩还是要优先嘛。</p>
<pre><code>#看看各乘客等级的获救情况
fig = plt.figure()
fig.set(alpha=0.2)

Survived_0 = data_train.Pclass[data_train.Survived == 0].value_counts()
Survived_1 = data_train.Pclass[data_train.Survived == 1].value_counts()
df=pd.DataFrame({u&apos;获救&apos;:Survived_1, u&apos;未获救&apos;:Survived_0})
df.plot(kind=&apos;bar&apos;, stacked=True)
plt.title(u&quot;各乘客等级的获救情况&quot;)
plt.xlabel(u&quot;乘客等级&quot;) 
plt.ylabel(u&quot;人数&quot;) 
plt.show()
</code></pre><p><img src="https://i.imgur.com/CAp7eKj.png" alt=""></p>
<p>船舱等级越高，获救概率也是越高，还是上层社会的人容易获救啊。</p>
<pre><code>#看看各性别的获救情况
fig = plt.figure()
fig.set(alpha=0.2)  

Survived_m = data_train.Survived[data_train.Sex == &apos;male&apos;].value_counts()
Survived_f = data_train.Survived[data_train.Sex == &apos;female&apos;].value_counts()
df=pd.DataFrame({u&apos;男性&apos;:Survived_m, u&apos;女性&apos;:Survived_f})
df.plot(kind=&apos;bar&apos;, stacked=True)
plt.title(u&quot;按性别看获救情况&quot;)
plt.xlabel(u&quot;性别&quot;) 
plt.ylabel(u&quot;人数&quot;)
plt.show()
</code></pre><p><img src="https://i.imgur.com/1PG3g2x.png" alt=""></p>
<p>女性获救概率更高，嗯，Lady first.</p>
<p>姓名中含有一些称谓信息，也代表着乘客的身份以及社会地位。</p>
<pre><code>data_train.groupby(data_train[&apos;Name&apos;].apply(lambda x: x.split(&apos;, &apos;)[1]).apply(lambda x: x.split(&apos;.&apos;)[0]))[&apos;Survived&apos;].mean()
</code></pre><p>得到的结果如下：</p>
<pre><code>Name
Capt            0.000000
Col             0.500000
Don             0.000000
Dr              0.428571
Jonkheer        0.000000
Lady            1.000000
Major           0.500000
Master          0.575000
Miss            0.697802
Mlle            1.000000
Mme             1.000000
Mr              0.156673
Mrs             0.792000
Ms              1.000000
Rev             0.000000
Sir             1.000000
the Countess    1.000000
Name: Survived, dtype: float64
</code></pre><p>还是要看身份的，有身份的人容易获救啊。</p>
<pre><code>#信息中船舱有无对于获救情况影响
fig = plt.figure()
fig.set(alpha=0.2) 

Cabin_Has= data_train.Survived[data_train.Cabin.notnull()].value_counts()
Cabin_None = data_train.Survived[data_train.Cabin.isnull()].value_counts()
df=pd.DataFrame({u&apos;有船舱号&apos;:Cabin_Has, u&apos;无船舱号&apos;:Cabin_None})
df.plot(kind=&apos;bar&apos;, stacked=True)
plt.title(u&quot;有无船舱号的获救情况&quot;)
plt.xlabel(u&quot;船舱号有无&quot;) 
plt.ylabel(u&quot;人数&quot;) 
plt.show()
</code></pre><p><img src="https://i.imgur.com/C9tiryu.png" alt=""></p>
<p>看起来有船舱号的人获救概率高一点。</p>
<pre><code>#信息中Embarked登船港口的影响
fig = plt.figure()
fig.set(alpha=0.2) 

Survived_0 = data_train.Embarked[data_train.Survived == 0].value_counts()
Survived_1 = data_train.Embarked[data_train.Survived == 1].value_counts()
df=pd.DataFrame({u&apos;获救&apos;:Survived_1, u&apos;未获救&apos;:Survived_0})
df.plot(kind=&apos;bar&apos;, stacked=True)
plt.title(u&quot;不同登船港口的获救情况&quot;)
plt.xlabel(u&quot;乘客等级&quot;) 
plt.ylabel(u&quot;人数&quot;) 
plt.show()
</code></pre><p><img src="https://i.imgur.com/8CZL0cP.png" alt=""></p>
<p>C港口获救概率高一点。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>下面就需要对训练集以及测试集的数据进行处理，以便算法的处理。</p>
<pre><code>data_test = pd.read_csv(&apos;test.csv&apos;)
data_test[&apos;Survived&apos;]=3 #为了能够合并在一起，test数据添加Suvrvived一列，不过数值为不存在的3
data_combine=pd.concat([data_train,data_test])
data_combine.info()
</code></pre><p>可以看到合并数据信息：</p>
<pre><code>Int64Index: 1309 entries, 0 to 417
Data columns (total 12 columns):
Age            1046 non-null float64
Cabin          295 non-null object
Embarked       1307 non-null object
Fare           1308 non-null float64
Name           1309 non-null object
Parch          1309 non-null int64
PassengerId    1309 non-null int64
Pclass         1309 non-null int64
Sex            1309 non-null object
SibSp          1309 non-null int64
Survived       1309 non-null int64
Ticket         1309 non-null object
dtypes: float64(2), int64(5), object(5)
</code></pre><h3 id="缺省数据的处理"><a href="#缺省数据的处理" class="headerlink" title="缺省数据的处理"></a>缺省数据的处理</h3><p>上面信息中可以看到Age、Cabin、Embarked、Fare有数据缺失，下面需要进行处理。</p>
<p>Cabin缺失数据很多，可以将有无Cabin记录为特征；Embarked仅缺失2个，可以取取值最多的来填补。</p>
<pre><code>#对船舱号缺值进行处理，有船舱号标识为1，无船舱号标识为0
data_combine.loc[(data_combine.Cabin.notnull()), &apos;Cabin&apos; ] = 1
data_combine.loc[(data_combine.Cabin.isnull()), &apos;Cabin&apos; ] = 0
#有两行数据缺失Embarked值，补全为Embarked的最多取值S
data_combine.loc[(data_combine.Embarked.isnull()),&apos;Embarked&apos;] = &apos;S&apos;
</code></pre><p>Fare有一个缺值，取所有Fare的平均值来填补。</p>
<pre><code>data_combine.Fare.fillna(data_combine.Fare.mean(), inplace=True) #Fare有一个缺值，用均值填充
</code></pre><p>Age缺省数值也是不多，但是不像Fare和Embarked那么少，考虑使用算法来拟合，采用随机森林的回归进行预测拟合填补。</p>
<pre><code>from sklearn.ensemble import RandomForestRegressor

### 使用 RandomForestClassifier 填补缺失的年龄属性
def set_missing_ages(df):
    # 把已有的数值型特征取出来丢进Random Forest Regressor中
    age_df = df[[&apos;Age&apos;,&apos;Fare&apos;, &apos;Parch&apos;, &apos;SibSp&apos;, &apos;Pclass&apos;]]
    # 乘客分成已知年龄和未知年龄两部分
    known_age = age_df[age_df.Age.notnull()].as_matrix()
    unknown_age = age_df[age_df.Age.isnull()].as_matrix()
    # y即目标年龄
    y = known_age[:, 0]
    # X即特征属性值
    X = known_age[:, 1:]
    # fit到RandomForestRegressor之中
    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)
    rfr.fit(X, y)
    # 用得到的模型进行未知年龄结果预测
    predictedAges = rfr.predict(unknown_age[:, 1::])
    # 用得到的预测结果填补原缺失数据
    df.loc[ (df.Age.isnull()), &apos;Age&apos; ] = predictedAges 
    return df, rfr

data_combine, rfr = set_missing_ages(data_combine)
</code></pre><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>对名字可以提取到称谓作为一个特征，根据SibSp和Parch可以知道家庭情况，再添加一个FamilySize特征。</p>
<pre><code>data_combine[&apos;title&apos;]=data_combine[&apos;Name&apos;].apply(lambda x: x.split(&apos;, &apos;)[1]).apply(lambda x: x.split(&apos;.&apos;)[0])
data_combine[&apos;FamilySize&apos;]=data_combine[&apos;SibSp&apos;] + data_combine[&apos;Parch&apos;]
</code></pre><p>下面将一些特征取值为文本的转化成数值取值。</p>
<pre><code>#将所有特征转换成数值型编码
# Sex
df = pd.get_dummies(data_combine[&apos;Sex&apos;],prefix=&apos;Sex&apos;)
data_combine = pd.concat([data_combine,df],axis=1).drop(&apos;Sex&apos;,axis=1)

# Embarked
df = pd.get_dummies(data_combine[&apos;Embarked&apos;],prefix=&apos;Embarked&apos;)
data_combine = pd.concat([data_combine,df],axis=1).drop(&apos;Embarked&apos;,axis=1)

# title
data_combine[&apos;title&apos;]=data_combine[&apos;title&apos;].astype(&apos;category&apos;)
data_combine[&apos;title&apos;]=data_combine[&apos;title&apos;].cat.codes

# Pclass
df = pd.get_dummies(data_combine[&apos;Pclass&apos;],prefix=&apos;Pclass&apos;)
data_combine = pd.concat([data_combine,df],axis=1).drop(&apos;Pclass&apos;,axis=1)


data_combine.drop([&apos;Name&apos;,&apos;SibSp&apos;,&apos;Parch&apos;,&apos;Ticket&apos;],axis=1,inplace=True)
</code></pre><h3 id="算法预测"><a href="#算法预测" class="headerlink" title="算法预测"></a>算法预测</h3><p>预测算法采用集成学习中的随机森林，作一个典型的二分类。    </p>
<pre><code>from sklearn.ensemble import RandomForestClassifier
X_train = data_combine.iloc[:891,:].drop([&quot;PassengerId&quot;,&quot;Survived&quot;], axis=1)
Y_train = data_combine.iloc[:891,:][&quot;Survived&quot;]
X_test = data_combine.iloc[891:,:].drop([&quot;PassengerId&quot;,&quot;Survived&quot;], axis=1)
clf = RandomForestClassifier(n_estimators=300,min_samples_leaf=4)
clf.fit(X_train, Y_train)
Y_test = clf.predict(X_test)
gender_submission = pd.DataFrame({&apos;PassengerId&apos;:data_test.iloc[:,0],&apos;Survived&apos;:Y_test})
gender_submission.to_csv(&apos;gender_submission.csv&apos;, index=None)
</code></pre><h2 id="submission"><a href="#submission" class="headerlink" title="submission"></a>submission</h2><p>将保存的gender_submission.csv文件提交到kaggle，得到的0.79904，毕竟第一次实践，进入了Top20%，虽然不够优秀，也还可以，算法继续学习之后还待改进。<br><img src="https://i.imgur.com/jOHX4b4.png" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据处理—Encoding Categorical Value]]></title>
      <url>/2017/08/30/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E2%80%94Encoding-Categorical-Value/</url>
      <content type="html"><![CDATA[<p>在机器学习的数据处理中，常常有些特征的值为类型变量（categorical variable），即这些特征对应的值是一些文本，为了便于后期的模型建立，常常将这些文本属性的值转换成数值，即对categorical variable的编码处理。<br><a id="more"></a></p>
<h2 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h2><p>数据集假设为：</p>
<pre><code>import pandas as pd  
df = pd.DataFrame([  
            [&apos;green&apos;, &apos;M&apos;, 11],   
            [&apos;red&apos;, &apos;L&apos;, 22],   
            [&apos;blue&apos;, &apos;XL&apos;, 33]])  

df.columns = [&apos;color&apos;, &apos;size&apos;, &apos;prize&apos;] 
</code></pre><blockquote>
<pre><code>  color size  prize 
0  green    M   11             
1    red    L   22  
2   blue   XL   33
</code></pre></blockquote>
<h2 id="Replace"><a href="#Replace" class="headerlink" title="Replace"></a>Replace</h2><p>一种方法是使用DataFrame的自带函数功能，替换函数replace。</p>
<pre><code>encoding_num={&quot;color&quot;:{&quot;green&quot;:0,&quot;red&quot;:1,&quot;blue&quot;:2},&quot;size&quot;:{&quot;M&quot;:0,&quot;L&quot;:1,&quot;XL&quot;:2}}
df.replace(encoding_num,inplace=True)

df
Out: 
   color  size  prize
0      0     0   10.1
1      1     1   13.5
2      2     2   15.3
</code></pre><h2 id="One-Hot-Encoding"><a href="#One-Hot-Encoding" class="headerlink" title="One Hot Encoding"></a>One Hot Encoding</h2><p>One-hot Encoding，又称为一位有效编码，即对每个状态采取一位进行编码。假设某个特征有N个特征值，则需要N位进行编码，且任意时候只有一位有效。</p>
<pre><code>pd.get_dummies(df)
Out[28]: 
   prize  color_blue  color_green  color_red  size_L  size_M  size_XL
0     11           0            1          0       0       1        0
1     22           0            0          1       1       0        0
2     33           1            0          0       0       0        1
</code></pre><p>这种编码方式弊端就是，当某个特征对应特征值很多时，就需要很多位进行编码，使得数据列数过大。</p>
<h2 id="Label-Encoding"><a href="#Label-Encoding" class="headerlink" title="Label Encoding"></a>Label Encoding</h2><p>这种编码方式，主要是基于pandas的Categorical模块，将某一列转换成category类型，然后使用category value来编码。</p>
<pre><code>df[&quot;color&quot;]=df[&quot;color&quot;].astype(&apos;category&apos;)
df[&quot;size&quot;]=df[&quot;size&quot;].astype(&apos;category&apos;)
df[&quot;color&quot;]=df[&quot;color&quot;].cat.codes
df[&quot;size&quot;]=df[&quot;size&quot;].cat.codes

df
Out[37]: 
   color  size  prize
0      1     1     11
1      2     0     22
2      0     2     33
</code></pre>]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从决策树到随机森林]]></title>
      <url>/2017/08/24/%E4%BB%8E%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%B0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
      <content type="html"><![CDATA[<p>学习了决策树算法之后又接触到了随机森林，随机森林可以说是决策树算法的集成加强版。从分类上来说，随机森林属于机器学习中的集成学习，所谓集成学习，顾名思义就是集成一些方法一起来学习，随机森林就是集成很多决策树来实现其分类或者回归的功能。</p>
<a id="more"></a>
<h2 id="随机森林之“森林”"><a href="#随机森林之“森林”" class="headerlink" title="随机森林之“森林”"></a>随机森林之“森林”</h2><p>“森林”表示有很多决策树，每棵决策树都是一个分类器，对于一个输入测试样本，经过森林中的每棵树都会有相应的预测值或者标签，而最终的结果就取决于这些树之间的投票结果作为最终预测结果。每棵随机树都是一个弱分类器，但是通过投票选择，最终组成一个强分类器。<br><img src="http://i.imgur.com/Q9qFsle.png" alt=""></p>
<h2 id="随机森林之“随机”"><a href="#随机森林之“随机”" class="headerlink" title="随机森林之“随机”"></a>随机森林之“随机”</h2><p>随机森林的随机体现在两个地方：一个是构建单棵决策树时的样本选择随机，一个是决策树分裂的时候选择的特征集随机。</p>
<ol>
<li>假设有N个样本，那么构建某一棵随机数时，放回抽样选择N个样本，称为bootstrap，这样每棵树的训练集都是不同的，当然也会包括重复样本。</li>
<li>在决策树分类的时候，假设每个样本含有M个特征，对于每一棵树，随机抽取m&lt;&lt;M(有不同的取法，常见的有log2（M)，sqrt（M）等）个特征，在每一次进行树枝分裂的时候都从这m个特征中选取最优分裂点。</li>
</ol>
<h2 id="随机森林经典python实现及API总结"><a href="#随机森林经典python实现及API总结" class="headerlink" title="随机森林经典python实现及API总结"></a>随机森林经典python实现及API总结</h2><p>python的sklearn模块集成了众多的机器学习算法，其中也包括随机森林（RandomTree），再结合pandas模块，就可以实现随机森林算法的分类或者回归。</p>
<pre><code>from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np

iris = load_iris() #导入鸢尾植物数据集
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df[&apos;is_train&apos;] = np.random.uniform(0, 1, len(df)) &lt;= .75  #随机选取训练节，大概取3/4
df[&apos;species&apos;] = pd.Categorical.from_codes(iris.target, iris.target_names) #添加从分类变量解码的分类值

train, test = df[df[&apos;is_train&apos;]==True], df[df[&apos;is_train&apos;]==False]

features = df.columns[:4]
clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train[&apos;species&apos;]) #将分类结果编码成数值
clf.fit(train[features], y)

preds = iris.target_names[clf.predict(test[features])]
 #将实际结果与预测结果合并成交叉列表输出
print(pd.crosstab(test[&apos;species&apos;], preds, rownames=[&apos;actual&apos;], colnames=[&apos;preds&apos;])) 
</code></pre><p>最终的预测结果如下图：</p>
<table>
<thead>
<tr>
<th style="text-align:left">preds</th>
<th style="text-align:left">setosa</th>
<th style="text-align:left">versicolor</th>
<th style="text-align:left">virginica</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">actual</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">setosa</td>
<td style="text-align:left">16</td>
<td style="text-align:left">0</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">versicolor</td>
<td style="text-align:left">0</td>
<td style="text-align:left">15</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">virginca</td>
<td style="text-align:left">0</td>
<td style="text-align:left">3</td>
<td style="text-align:left">16</td>
</tr>
</tbody>
</table>
<p>当然，每次运行最终预测结果是不一样的，因为测试集和训练集都是每次随机选取的。</p>
<pre><code>iris = load_iris() #导入鸢尾植物数据集
</code></pre><p>sklearn模块含有一些机器学习经典的数据集，这里导入了鸢尾植物数据集，导入后iris为字典数据类型，存储了其萼片和花瓣的长宽，一共4个属性，鸢尾植物又分三类。与之相对，iris里有两个属性iris.data，iris.target，data里是一个矩阵，每一列代表了萼片或花瓣的长宽，一共4列，每一列代表某个被测量的鸢尾植物，一共采样了150条记录。</p>
<p>然后又碰到了pandas的一个模块，pandas.Categorical，将一些label转变成categorical variable（分类变量）</p>
<pre><code>Categorical.from_codes(codes, categories, ordered=False)
</code></pre><p>这个函数产生一个categorical type根据codes和categories。</p>
<p>pandas.factorizes 则与之相反，Encode input values as an enumerated type or categorical variable。</p>
<p>返回的结果中：<br>labels : the indexer to the original array<br>uniques : ndarray (1-d) or Index</p>
<p>python机器学习模块sklearn中的RandomForestClassifier</p>
<p><strong>sklearn.ensemble</strong>.<strong>RandomForestClassifier</strong>(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)</p>
<p><strong>n_estimators</strong> : integer, optional (default=10)<br>&emsp;&emsp;森林中树木数目</p>
<p><strong>criterion</strong> : string, optional (default=”gini”)<br>&emsp;&emsp;树枝分裂算法，gini和entropy</p>
<p><strong>max_features</strong> : int, float, string or None, optional (default=”auto”)<br>&emsp;&emsp;寻找最优分裂点计算采用的特征数目<br>If int, then consider max_features features at each split.<br>If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.<br>If “auto”, then max_features=sqrt(n_features).<br>If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).<br>If “log2”, then max_features=log2(n_features).<br>If None, then max_features=n_features.<br>Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.</p>
<p><strong>max_depth</strong> : integer or None, optional (default=None)<br>&emsp;&emsp;树的最大深度<br>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>
<p><strong>min_samples_split</strong> : int, float, optional (default=2)<br>The minimum number of samples required to split an internal node:<br>If int, then consider min_samples_split as the minimum number.<br>If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.<br>Changed in version 0.18: Added float values for percentages.</p>
<p><strong>min_samples_leaf</strong> : int, float, optional (default=1)<br>The minimum number of samples required to be at a leaf node:<br>If int, then consider min_samples_leaf as the minimum number.<br>If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.<br>Changed in version 0.18: Added float values for percentages.</p>
<p><strong>min_weight_fraction_leaf</strong> : float, optional (default=0.)<br>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</p>
<p><strong>max_leaf_nodes</strong> : int or None, optional (default=None)<br>Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>
<p><strong>min_impurity_split</strong> : float,<br>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.<br>Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19 and will be removed in 0.21. Use min_impurity_decrease instead. </p>
<p><strong>min_impurity_decrease</strong> : float, optional (default=0.)<br>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.<br>The weighted impurity decrease equation is the following:</p>
<blockquote>
<p>N_t / N <em> (impurity - N_t_R / N_t </em> right_impurity</p>
<pre><code>- N_t_L / N_t * left_impurity)
</code></pre></blockquote>
<p>where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.<br>N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.<br>New in version 0.19.</p>
<p><strong>bootstrap</strong> : boolean, optional (default=True)<br>Whether bootstrap samples are used when building trees.<br>oob_score : bool (default=False)<br>Whether to use out-of-bag samples to estimate the generalization accuracy.</p>
<p><strong>n_jobs</strong> : integer, optional (default=1)<br>The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.</p>
<p><strong>random_state</strong> : int, RandomState instance or None, optional (default=None)<br>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.</p>
<p><strong>verbose</strong> : int, optional (default=0)<br>Controls the verbosity of the tree building process.<br>warm_start : bool, optional (default=False)<br>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.</p>
<p><strong>class_weight</strong> : dict, list of dicts, “balanced”,<br>“balanced_subsample” or None, optional (default=None) Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.<br>Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].<br>The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))<br>The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.<br>For multi-output, the weights of each column of y will be multiplied.<br>Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.</p>
<p>其余函数：</p>
<table>
<thead>
<tr>
<th style="text-align:left">function</th>
<th style="text-align:left">description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">apply(X)</td>
<td style="text-align:left">Apply trees in the forest to X, return leaf indices.</td>
</tr>
<tr>
<td style="text-align:left">decision_path(X)</td>
<td style="text-align:left">Return the decision path in the forest</td>
</tr>
<tr>
<td style="text-align:left">fit(X, y[, sample_weight])</td>
<td style="text-align:left">Build a forest of trees from the training set (X, y).</td>
</tr>
<tr>
<td style="text-align:left">get_params([deep])</td>
<td style="text-align:left">Get parameters for this estimator.</td>
</tr>
<tr>
<td style="text-align:left">predict(X)</td>
<td style="text-align:left">Predict class for X.</td>
</tr>
<tr>
<td style="text-align:left">predict_log_proba(X)</td>
<td style="text-align:left">Predict class log-probabilities for X.</td>
</tr>
<tr>
<td style="text-align:left">predict_proba(X)</td>
<td style="text-align:left">Predict class probabilities for X.</td>
</tr>
<tr>
<td style="text-align:left">score(X, y[, sample_weight])</td>
<td style="text-align:left">Returns the mean accuracy on the given test data and labels.</td>
</tr>
<tr>
<td style="text-align:left">set_params(**params)</td>
<td style="text-align:left">Set the parameters of this estimator.</td>
</tr>
</tbody>
</table>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树算法介绍及python实现]]></title>
      <url>/2017/08/22/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p>决策树算法是机器学习中经典的算法之一，既可以作为分类算法，也可以作为回归算法。在做开始入门机器学习这方面内容时，自然就接触到了这方面的知识。因此，本文对决策树算法进行了一些整理，首先对决策树算法的原理进行介绍，并且用python对决策树算法进行代码实现。<br><a id="more"></a></p>
<h2 id="决策树算法的概述"><a href="#决策树算法的概述" class="headerlink" title="决策树算法的概述"></a>决策树算法的概述</h2><p>决策树算法的思想很类似于我们写代码经常用到的if，else if，else，用什么特征来判断if，这就是决策树算法的精髓所在。</p>
<div align="center"><br>    <img src="http://i.imgur.com/5xkMuwo.jpg" width="400" height="400" alt="决策树示例"><br></div><br>上图是一个结构简单的决策树，用于判断贷款用户是否具备偿款能力，根据不同的条件，从根节点拥有房产开始，根据特征不断判断不断分裂到子节点，叶子节点代表最终的分类结果，树枝分裂的地方就是要根据特征来判断。由此可知，决策树最关键的地方在于树枝分叉的地方，即合理选择特征，从根节点开始，经历子节点的分支后，最终到达叶子。<br><br>从上面来说，决策树算法的基本步骤为：<br>1.从根节点开始<br>2.遍历所有特征作为一种分裂方式，找到最好的分裂特征<br>3.分类成两个或者多个节点<br>4.对子节点同样进行2-3的操作，其实就是第一个递归建树的操作，直到到达叶子节点，即得到分类结果。<br>常见的决策树算法有ID3、C4.5、CART等，下面将分别进行介绍。<br><br>## ID3算法 ##<br>理论上来说，其实可以有很多种树能够完成这个分类问题，但是如何将这个分类做的比较优呢？有个叫昆兰的大牛将信息论中的熵引入到决策树算法，解决了这个问题。<br><br>首先，什么是熵？熵的概念源于在物理的热力学，主要是用于描述一个热力学系统的无序程度。信息论的创始人香农将熵引入信息论中，表示不确定性的度量，直观理解为信息含量越多，熵越大，也就是分布越无序。熵的数值定义为：<br><br><img src="http://i.imgur.com/CPbE2UB.gif" alt=""><br><br>X为样例的集合，P(xi)为样例xi的出现概率。<br>分类特征的选择应该使得分类后更加有序，熵减小，在这里引入了熵的增益这个概念。<br><br><img src="http://i.imgur.com/z1tT5ej.gif" alt=""><br><br>其中，Sv为采用特征A分类后的，某一个类别样例数占原有的样例数的比例，H(Sv)该分类后类别的熵，用原来的上减去特征A分类后的每个类别的熵乘于比例权重之后的和。这个时候便得到了选取分类特征的方法，即选取使得熵的增益为最大的特征A作为树枝分裂的特征。ID3就是基于熵增益最大的决策树算法。<br><br>### ID3算法计算实例 ###<br>下面以一个经典的打网球的例子说明如何构建决策树。例子中打网球（play）主要由天气（outlook)、温度（temperature）、湿度（humidity)、风（windy）来确定，样本数据如下：<br><div align="center"><br>    <img src="http://i.imgur.com/i4FloIa.png" width="400" height="270" alt="样本数据"><br></div><br>在本例中S有14个样例，目标变量是是否打球，play=yes or no,yes有9个样例，no有5个样例，首先计算从根节点的信息熵：<br>H(S)=-(9/14<em>log(9/14))-(5/14</em>log(5/14))=0.28305<br>从根节点开始，有四个特征（outlook temperature humidity windy)可以用来进行树枝分裂，首先需要计算这四个特征的信息熵增益，以outlook为例进行计算。<br><br>特征A（outlook）有三个不同的取值{sunny,overcast,rainy}，这样将原先的数据集S分为3类，其中<br>sunnny有5个样本，2个play=yes，3个play=no<br>overcast有4个样本，4个play=yes<br>rainy有5个样本，3个play=yes，2个play=no<br>根据上面的公式，按outlook分类后的熵为：<br>H（S,A)=5/14sunny熵+4/14overcast熵+5/14rainy熵<br>&emsp;&emsp;&emsp;&emsp;=5/14<em>（-2/5</em>log(2/5)-3/5<em>log(3/5))+4/14</em>(-1<em>log1)+5/14</em>(-3/5<em>log(3/5)-2/5</em>log(2/5))=0.20878<br><br>所以对于outlook的信息熵增益为G(S,outlook)=H(S)-H(S,A)=0.07427<br>用类似的方法，算出，A取其他三个特征时候的信息熵增益分别为：<br>G(S,temperature)=0.00879<br>G(S,humidity)=0.04567<br>G(S,windy)=0.01448<br>显然，用outlook作为特征A时候，熵增益最大，因此作为第一个节点，即为根节点。这样，S就换分为3个子集，sunny，overcast，rainy，其中overcast熵为0，已经分好类，直接作为叶子节点，而sunny，rainy熵都大于0，采取类似于上述的过程继续选择特征，最后可得决策树为：<br><div align="center"><br>    <img src="http://i.imgur.com/ewGSoY2.png" width="400" height="270" alt="最终决策树"><br></div>

<h3 id="ID3算法的python代码实现"><a href="#ID3算法的python代码实现" class="headerlink" title="ID3算法的python代码实现"></a>ID3算法的python代码实现</h3><pre><code>import pandas as pd
import math

def Tree_building(dataSet):
    tree = []
    if(Calculate_Entropy(dataSet) == 0): #熵为0说明分类已经到达叶子节点
        if(dataSet[&apos;play&apos;].sum()==0):  #根据play的值到达0或者1叶子节点
            tree.append(0)
        else:
            tree.append(1)
        return tree
    numSamples=len(dataSet) #样例数
    Feature_Entropy={} #记录按特征A分类后的熵值的字典
    for i in range(1,len(dataSet.columns)-1):
        Set=dict(list(dataSet.groupby(dataSet.columns[i]))) #取出不同的特征
        Entropy=0.0
        for key,subSet in Set.items():
            Entropy+=(len(subSet)/numSamples)*Calculate_Entropy(subSet) #计算熵
        Feature_Entropy[dataSet.columns[i]]=Entropy

    #选最小熵值的特征分类点，这样熵值增益最大    
    Feature = min(zip(Feature_Entropy.values(),Feature_Entropy.keys()))[1] 
    Set=dict(list(dataSet.groupby(Feature)))
    for key,value in Set.items():
        subTree=[]
        subTree.append(Feature)
        subTree.append(key)
        subTree.append(Tree_building(value)) #树枝扩展函数的迭代
        tree.append(subTree)

    return tree

def Calculate_Entropy(data):
    numSamples=len(data)  #样本总数
    P=data.sum()[&apos;play&apos;]  #正例数量
    N=numSamples-P   #反例数量
    if((N==0)or(P==0)):  
        Entropy=0
        return Entropy
    Entropy = -P/numSamples*math.log(P/numSamples)-N/numSamples*math.log(N/numSamples)
    return Entropy

if __name__ == &apos;__main__&apos;:
    data=pd.read_csv(&apos;tennis.csv&apos;)
    tree=Tree_building(data)
    print(tree)
</code></pre><h2 id="C4-5-和CART算法"><a href="#C4-5-和CART算法" class="headerlink" title="C4.5 和CART算法"></a>C4.5 和CART算法</h2><h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>ID3算法有一个弊端，因为是选择信息增益最大的特征来分裂，所以更偏向于具有大量属性的特征进行分裂，这样做其实有时候是没有意义的，针对此，有了C4.5算法，对ID3进行了改进。C4.5采用信息增益率来选择分裂特征即：<br>gr(S,A)=gain(S,A)/H(S,A),<br>其中，gain(S,A)为ID3算法的熵的增益，H(S,A)为取特征为A进行分类的的信息熵<br>取A为outlook即为H（S,outlook），H(S,A)=H(S,outlook)= -(5/14)<em>log(5/14) - (5/14)</em>log(5/14) - (4/14)*log(4/14)</p>
<p>C4.5算法采用的熵信息增益率，因为分母采用了基于属性A分裂后的信息熵，从而抵消了如果信息A属性取值数目过大带来的影响。</p>
<p>C4.5算法还可以应用于处理连续性的属性则按属性A的取值递增排序，将每对相邻值的中点看作可能的分裂点，对每个可能的分裂点，计算：<br><img src="http://i.imgur.com/DWuCTu8.png" alt=""></p>
<p>SL和SR分别对应A属性分类点划分出的两个子集，取使得划分后信息熵最小的取值作为A属性的最佳分裂点，参与后面的运算，不够感觉这样计算量有点多的orz</p>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>CART算法的划分基于递归建立二叉树，对于一个变量属性来说，它的划分点是一对连续变量属性值的中点。假设m个样本的集合一个属性有m个连续的值，那么则会有m-1个分裂点，每个分裂点为相邻两个连续值的均值。每个属性的划分按照能减少的杂质的量来进行排序，而杂质的减少量定义为划分前的杂质减去划分后的每个节点的杂质量划分所占比率之和。而杂质度量方法常用Gini指标，假设一个样本共有C类，那么一个节点的Gini不纯度可定义为</p>
<p><img src="http://i.imgur.com/s2AqUO0.png" alt=""></p>
<p>那么，按属性A的某个属性值t分裂最后的Gini值为：</p>
<p><img src="https://i.imgur.com/x3qv9ea.png" alt=""></p>
<p>分别计算属性A不同属性值的Gini值，取最小的作为A的最佳分类点，然后对于S集此时所有的属性进行上述运算之后，取具有最小的Gini作为分裂属性，其最小的Gini值的属性值作为分裂点。</p>
<p>CART还可以用于作为回归树，但是此时Gini值的算法就不一样，采用的是总方差：</p>
<p><img src="http://i.imgur.com/FS7qiGE.png" alt=""></p>
<p>回归树的叶节点所含样本中，其输出变量的平均值就是预测结果。</p>
]]></content>
      
        
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[也看战狼2：爬取豆瓣影评做词云]]></title>
      <url>/2017/08/08/%E4%B9%9F%E7%9C%8B%E6%88%98%E7%8B%BC2%EF%BC%9A%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84%E5%81%9A%E8%AF%8D%E4%BA%91/</url>
      <content type="html"><![CDATA[<p><img src="http://i.imgur.com/x7xGFON.jpg" alt=""></p>
<p>《战狼2》上映的第二天就去看了，当时觉得还不错，不管是打斗场景还是故事情节，看的都很过瘾，个人觉得可以给4星半。但是这段时间一直看到晚上对《战狼2》各种各样的不同的评论，因此闲暇之余，用爬虫获取了截止于2017.8.8号的豆瓣用户的近14万的评论，对其中的关键词做成了词云。<br><a id="more"></a></p>
<h2 id="python爬虫爬取评论代码"><a href="#python爬虫爬取评论代码" class="headerlink" title="python爬虫爬取评论代码"></a>python爬虫爬取评论代码</h2><pre><code>import requests
from bs4 import BeautifulSoup
import codecs
import time

absolute_url = &apos;https://movie.douban.com/subject/26363254/comments&apos;
url = &apos;https://movie.douban.com/subject/26363254/comments?start={}&amp;limit=20&amp;sort=new_score&amp;status=P&apos;
header={&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0&apos;,&apos;Connection&apos;:&apos;keep-alive&apos;}




def html_prase(html, struct):
    soup=BeautifulSoup(html,&apos;lxml&apos;)
    comment_nodes = []
    comment_nodes = soup.select(struct)
    xiangdui_link_nodes= soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;)
    return comment_nodes,xiangdui_link_nodes

if __name__ == &apos;__main__&apos;:
    #读取cookie数据
    f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
    cookies = {}
    for line in f_cookies.read().split(&apos;;&apos;):
        name, value = line.strip().split(&apos;=&apos;, 1)
        cookies[name] = value
    f = codecs.open(&quot;comments.txt&quot;, &apos;a&apos;, encoding=&apos;utf-8&apos;)
    html = requests.get(url, cookies=cookies, headers=header).content
    comment_nodes=[]
    xiangdui_links=[]
    #获取评论
    comment_nodes,xiangdui_link_nodes = html_prase(html , &apos;.comment &gt; p&apos;)
    soup = BeautifulSoup(html, &apos;lxml&apos;)
    comment_list = []
    for node in comment_nodes:
        comment_list.append(node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;)
    while(xiangdui_link_nodes!=[]):#每次查看是否有后页，即不断往深处挖掘，获取数据
        xiangdui_link = soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;) #取出后页的相对链接
        xiangdui_links.append(xiangdui_link)
        time.sleep(1)
        html = requests.get(absolute_url+xiangdui_link_nodes, cookies=cookies, headers=header).content
        soup = BeautifulSoup(html, &apos;lxml&apos;)
        comment_nodes, xiangdui_link_nodes = html_prase(html, &apos;.comment &gt; p&apos;)
        for node in comment_nodes:
            comment = node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;
            comment_list.append(comment)
            f.writelines(comment)    
</code></pre><p>在抓取豆瓣影评的时候，一开始我是直接对URL爬虫，仅仅是加了一个header，抓取一段时间，豆瓣的反爬虫策略就将我的IP封掉了，所以我又加入了cookie字段。cookie是一个字典类型的数据，可以以比较简单的方式获取。打开要浏览的豆瓣页面，点击登陆页面后，打开Chrome的开发者模式，开始监听登陆时候的http请求和响应。<br><img src="http://i.imgur.com/AK1Gm88.png" alt=""></p>
<p>这个时候，在Cookie字段可以找到cookie数据，复制后存为txt文件，然后写代码读取txt文件，并存为字典格式数据。</p>
<pre><code>f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
cookies = {}
for line in f_cookies.read().split(&apos;;&apos;):
    name, value = line.strip().split(&apos;=&apos;, 1)
    cookies[name] = value
</code></pre><p>用requests对网页进行爬虫抓取之后，此后就是利用Beautifulsoup对获取的html进行解析，获取豆瓣用户评论，以及后页的链接。<br><img src="http://i.imgur.com/8gnYM4R.png" alt=""></p>
<p>对《战狼2》的豆瓣影评链接进行分析，发现每一页链接都是如上图的组成，网页解析可以获取后面的红色字段，实现不断向后页爬虫。最后爬虫结果得到18M左右的数据。</p>
<h2 id="jieba模块提取评论内容关键词"><a href="#jieba模块提取评论内容关键词" class="headerlink" title="jieba模块提取评论内容关键词"></a>jieba模块提取评论内容关键词</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Wed Aug  9 09:51:51 2017

@author: lkj
&quot;&quot;&quot;
import codecs
import jieba
import matplotlib.pyplot as plt  
import matplotlib as mpl 
import numpy as np 
from collections import Counter

zhfont1 = mpl.font_manager.FontProperties(fname=&apos;C:\Windows\Fonts\simsun.ttc&apos;)

def draw_bar(labels,quants):  
    width = 0.4  
    ind = np.linspace(0.5,9.5,10)  
    # make a square figure  
    fig = plt.figure(1)  
    ax  = fig.add_subplot(111)  
    # Bar Plot  
    ax.bar(ind-width/2,quants,width,color=&apos;green&apos;)  
    # Set the ticks on x-axis  
    ax.set_xticks(ind)  
    ax.set_xticklabels(labels,fontproperties=zhfont1)  
    # labels  
    ax.set_xlabel(u&apos;关键词&apos;,fontproperties=zhfont1)  
    ax.set_ylabel(u&apos;评论数量&apos;,fontproperties=zhfont1)  
    # title  
    ax.set_title(u&apos;筛选后的TOP10关键词&apos;, bbox={&apos;facecolor&apos;:&apos;0.8&apos;, &apos;pad&apos;:5},fontproperties=zhfont1)  
    #plt.legend(prop=zhfont1)
    plt.grid(True)  
    plt.show()   

word_lists = []  # 关键词列表
with codecs.open(&apos;comments.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;) as f:
    Lists = f.readlines()  # 文本列表
    for List in Lists:
        cut_list = list(jieba.cut(List))
        for word in cut_list:
            word_lists.append(word)
word_lists_set = set(word_lists)  # 去除重复元素
sort_count = []
word_lists_set = list(word_lists_set)
length = len(word_lists_set)
print(u&quot;共有%d个关键词&quot; %length)
k = 1
for w in word_lists_set:
    sort_count.append(w + u&apos;:&apos; + str(word_lists.count(w)) + u&quot;次\n&quot;)
    print (u&quot;%d---&quot; % k + w + u&quot;:&quot; + str(word_lists.count(w)) + u&quot;次&quot;)
    k += 1
with codecs.open(&apos;count_word.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:
    f.writelines(sort_count)
#先取出前100关键词，再进行人为筛选
key_words_TOP100=[]
key_words_TOP100=Counter(word_lists).most_common(100)
key_words_shaixuan=[key_words_TOP100[6],key_words_TOP100[24],key_words_TOP100[25],
                    key_words_TOP100[30],key_words_TOP100[39],key_words_TOP100[52],
                    key_words_TOP100[60],key_words_TOP100[77],key_words_TOP100[78],
                    key_words_TOP100[94]]
labels = []
quants = []
for i in range(10):
    labels.append(key_words_shaixuan[i][0])
    quants.append(key_words_shaixuan[i][1])
draw_bar(labels,quants)
</code></pre><p>绘制柱形图的时候需要指定字体，不然会出现中文乱码。对关键词TOP100需要进行人为筛选，因为jieba分词会出现很多诸如“我们”之类的在这里无意义的词汇，人为筛选出TOP10关键词如下：</p>
<p><img src="http://i.imgur.com/M2v23dA.png" alt=""></p>
<p>从关键词来看，大多数网友还是看好这部电影的，认为这是大场面的动作戏，达到了好莱坞大片水平，当然也不乏网友认为这是满足吴京个人英雄主义的意淫。</p>
<h2 id="绘制词云"><a href="#绘制词云" class="headerlink" title="绘制词云"></a>绘制词云</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Tue Aug  8 21:46:04 2017

@author: lkj
&quot;&quot;&quot;

# -*- coding:utf-8 -*-
import codecs

import jieba
from scipy.misc import imread
from wordcloud import WordCloud


# 绘制词云
def save_jieba_result():
    # 设置多线程切割
    #jieba.enable_parallel(4)
    with codecs.open(&apos;comments.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()
    cut_text = &quot; &quot;.join(jieba.cut(comment_text))  # 将jieba分词得到的关键词用空格连接成为字符串
    with codecs.open(&apos;jieba.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f:
        f.write(cut_text)


def draw_wordcloud2():
    with codecs.open(&apos;jieba.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()

    color_mask = imread(&quot;zhanlang2.jpg&quot;)  # 读取背景图片

    stopwords = [u&apos;就是&apos;, u&apos;电影&apos;, u&apos;你们&apos;, u&apos;这么&apos;, u&apos;不过&apos;, u&apos;但是&apos;, u&apos;什么&apos;, u&apos;没有&apos;, u&apos;这个&apos;, u&apos;那个&apos;, u&apos;大家&apos;, u&apos;比较&apos;, u&apos;看到&apos;, u&apos;真是&apos;,
                 u&apos;除了&apos;, u&apos;时候&apos;, u&apos;已经&apos;, u&apos;可以&apos;,u&apos;湄公河&apos;]
    cloud = WordCloud(font_path=&quot;MSYH.TTF&quot;, background_color=&apos;white&apos;,
                      max_words=2000, max_font_size=200, min_font_size=4, mask=color_mask,stopwords=stopwords)
    word_cloud = cloud.generate(comment_text)  # 产生词云
    word_cloud.to_file(&quot;zhanlang2_cloud.jpg&quot;)

save_jieba_result()
draw_wordcloud2()
</code></pre><p>词云的绘制需要需要指定font_path，不然会出现中文乱码，我在网上下好微软雅黑的字体（.TTF文件）一并放在目录下调用。</p>
<h2 id="也想说两句"><a href="#也想说两句" class="headerlink" title="也想说两句"></a>也想说两句</h2><p>电影的后面，红旗飘扬，进过敌战区的吴京身披五星红旗大摇大摆经过，那一刻真的为作为一个中国人而感到自豪。<br><img src="http://i.imgur.com/bUYDPL5.jpg" alt=""><br>有人觉得看的剧情尴尬，完全是吴京个人英雄主义的表现，但是在《看速度与激情》一个人干翻整个俄罗斯核基地为什么不觉得尴尬呢？我们接受了太多的美国大片以及美国的个人英雄主义的意识形态的输出，有美国队长能够拯救世界，为什么中国队长不行呢？现在我们的国家也是越来越强大，很欣慰能有《湄公河行动》、《战狼2》这样的主旋律大片，虽然有很多不好的声音，也有越来越多的人被感染，而认同，《战狼2》上映两周就登顶国内票房冠军就印证了这一切。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&id=491295324&auto=0&height=66"></iframe>

<p>不可否认，在调动自然流露爱国情愫上，《战狼2》是成功的，它在我不知不觉的情感代入里推揉了我的泪腺。当舰长青筋怒暴将憋在心中已久的爱国情感汇成一句“开火”时，男主冷锋独自潜入暴乱的非洲国家拼尽全力解救侨胞、身处险境孤立无援的那种英雄悲凉绝望感，瞬间倾倒而出。祖国，在这一刻，有了最真切感受。</p>
<p>当然，《战狼2》还是存在很多瑕疵，但是也为国产电影树立了一个新的标杆，相信国产大片也会越来越好，期待有更多像《战狼2》这样优秀的电影。</p>
<p>最后，很喜欢这部电影的结尾，中华人民共和国的护照。<br><img src="http://i.imgur.com/DG2aone.jpg" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> 电影 </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hexo-github搭建个人博客]]></title>
      <url>/2017/08/06/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>搭好个人网站之后纠结于第一篇博客的内容，想想还是首先分享一下自己搭建这个网站的过程，毕竟还是碰到了很多坑，解决了一些问题才成功的。<br><a id="more"></a></p>
<h2 id="配置准备"><a href="#配置准备" class="headerlink" title="配置准备"></a>配置准备</h2><p>配置环境是win10，其实在linux和mac环境下的搭建过程也是类似。</p>
<ol>
<li>安装node。一种静态博客框架，到<a href="https://nodejs.org/en/" title="Node.js官网" target="_blank" rel="external">Node.js官网</a>上下载，根据自己电脑版本下载对应文件，按照安装提示一路安装即可。</li>
<li>GitHub账号申请，已有可以跳过。Github用来做博客的远程仓库，后期需要配置域名，以及和本地的hexo创建连接。没有账号需要在GitHub官网进行申请，以及SSH key的配置，网上可以搜到很多教程。</li>
<li>git的安装。把本地的hexo上传到GitHub上去，安装过程可以参考<a href="https://git-scm.com/book/zh/v1/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git" target="_blank" rel="external">官方教程</a>。</li>
</ol>
<h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><p>Node.js和Git安装好之后就可以正式安装了，首先创建一个空文件夹比如hexo，在cmd窗口下进入到这个文件夹，此后的命令都是在这个文件夹下进行。</p>
<p>执行如下命令安装hexo：</p>
<pre><code>sudo npm install -g hexo
</code></pre><p>而后初始化hexo：</p>
<pre><code>hexo init
</code></pre><p>生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>这时候在启动本地服务，就能在本地浏览器进行预览：<br>    hexo server</p>
<p>浏览器输入<a href="http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。" target="_blank" rel="external">http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。</a></p>
<h2 id="配置GitHub"><a href="#配置GitHub" class="headerlink" title="配置GitHub"></a>配置GitHub</h2><p>首选需要建立仓库（repository),仓库的名字格式必须为：你的GitHub账户名+.github.io,比如说你的github账户名为abc，那么你的仓库名为abc.github.io</p>
<p>同时，本地的hexo文件夹可以看到以下内容：<br><img src="http://i.imgur.com/NL4v9X7.png" alt=""><br>打开_config.yml文件，下滑到最后进行编辑，修改成下面的样子：</p>
<pre><code>deploy:
  type: git
  repo: https://github.com/xxx/xxx.github.io.git
  branch: master
</code></pre><p><strong>注意</strong>：xxx需要改为你自己的github账户名，而且格外注意的是，所有的冒号后面必须加一个空格，否则hexo指令会执行不成功。</p>
<p>执行hexo generate命令生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>执行命令安装hexo的git上传工具：</p>
<pre><code>npm install hexo-deployer-git --save
</code></pre><p>再执行配置命令：</p>
<pre><code>hexo deploy
</code></pre><p>命令时终端会提示你输入Github的用户名和密码，输入配置好后，本地文件就会上传至你的github仓库。</p>
<h2 id="域名绑定"><a href="#域名绑定" class="headerlink" title="域名绑定"></a>域名绑定</h2><p>笔者是在阿里云上后买的<a href="lkj666.top">lkj666.top</a>域名，然后需要与github page进行绑定。<br>在域名解析管理的页面下，添加以下几条：<br><img src="http://i.imgur.com/609hZBT.png" alt=""><br>其中：</p>
<ol>
<li>CNAME记录类型会将你的域名进行别名指向。可以为一个主机设置别名在这里可以设置自己的域名指向xxx.github.io，以后就可以用自己的域名来代替访问xxx.github.io.</li>
<li>此外还增加了两条A类型，来指定github.com的IP地址，以及xxx.github.io的IP地址，可以用ping指令的方式来获取。</li>
</ol>
<p>此外，还需要修改本地以及仓库的CNAME文件：<br>在本地hexo的source文件夹下新建一个CNAME文件，将自己的域名写入，比如我将自己的域名lkj666.top写入CNAME文件一定要记住，是所有文件格式，不是txt文件格式。<br>在对应的xxx.github.io仓库根目录下新建一个文件CNAME，同样将自己的域名写入。</p>
<p>做好上述过程后再执行下列命令:</p>
<pre><code>hexo clean //每次执行提交之前最好执行这个clean命令来清除缓存
hexo generate
hexo deploy
</code></pre><p>此后就可以通过你的域名访问你自己的博客了。</p>
]]></content>
      
        
    </entry>
    
  
  
</search>
