<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[也看战狼2：爬取豆瓣影评做词云]]></title>
      <url>/2017/08/08/%E4%B9%9F%E7%9C%8B%E6%88%98%E7%8B%BC2%EF%BC%9A%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E5%BD%B1%E8%AF%84%E5%81%9A%E8%AF%8D%E4%BA%91/</url>
      <content type="html"><![CDATA[<p><img src="http://i.imgur.com/x7xGFON.jpg" alt=""></p>
<p>《战狼2》上映的第二天就去看了，当时觉得还不错，不管是打斗场景还是故事情节，看的都很过瘾，个人觉得可以给4星半。但是这段时间一直看到晚上对《战狼2》各种各样的不同的评论，因此闲暇之余，用爬虫获取了截止于2017.8.8号的豆瓣用户的近14万的评论，对其中的关键词做成了词云。<br><a id="more"></a></p>
<h2 id="python爬虫爬取评论代码"><a href="#python爬虫爬取评论代码" class="headerlink" title="python爬虫爬取评论代码"></a>python爬虫爬取评论代码</h2><pre><code>import requests
from bs4 import BeautifulSoup
import codecs
import time

absolute_url = &apos;https://movie.douban.com/subject/26363254/comments&apos;
url = &apos;https://movie.douban.com/subject/26363254/comments?start={}&amp;limit=20&amp;sort=new_score&amp;status=P&apos;
header={&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0&apos;,&apos;Connection&apos;:&apos;keep-alive&apos;}




def html_prase(html, struct):
    soup=BeautifulSoup(html,&apos;lxml&apos;)
    comment_nodes = []
    comment_nodes = soup.select(struct)
    xiangdui_link_nodes= soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;)
    return comment_nodes,xiangdui_link_nodes

if __name__ == &apos;__main__&apos;:
    #读取cookie数据
    f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
    cookies = {}
    for line in f_cookies.read().split(&apos;;&apos;):
        name, value = line.strip().split(&apos;=&apos;, 1)
        cookies[name] = value
    f = codecs.open(&quot;comments.txt&quot;, &apos;a&apos;, encoding=&apos;utf-8&apos;)
    html = requests.get(url, cookies=cookies, headers=header).content
    comment_nodes=[]
    xiangdui_links=[]
    #获取评论
    comment_nodes,xiangdui_link_nodes = html_prase(html , &apos;.comment &gt; p&apos;)
    soup = BeautifulSoup(html, &apos;lxml&apos;)
    comment_list = []
    for node in comment_nodes:
        comment_list.append(node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;)
    while(xiangdui_link_nodes!=[]):#每次查看是否有后页，即不断往深处挖掘，获取数据
        xiangdui_link = soup.select(&apos;#paginator &gt; a&apos;)[0].get(&apos;href&apos;) #取出后页的相对链接
        xiangdui_links.append(xiangdui_link)
        time.sleep(1)
        html = requests.get(absolute_url+xiangdui_link_nodes, cookies=cookies, headers=header).content
        soup = BeautifulSoup(html, &apos;lxml&apos;)
        comment_nodes, xiangdui_link_nodes = html_prase(html, &apos;.comment &gt; p&apos;)
        for node in comment_nodes:
            comment = node.get_text().strip().replace(&quot;\n&quot;, &quot;&quot;) + u&apos;\n&apos;
            comment_list.append(comment)
            f.writelines(comment)    
</code></pre><p>在抓取豆瓣影评的时候，一开始我是直接对URL爬虫，仅仅是加了一个header，抓取一段时间，豆瓣的反爬虫策略就将我的IP封掉了，所以我又加入了cookie字段。cookie是一个字典类型的数据，可以以比较简单的方式获取。打开要浏览的豆瓣页面，点击登陆页面后，打开Chrome的开发者模式，开始监听登陆时候的http请求和响应。<br><img src="http://i.imgur.com/AK1Gm88.png" alt=""></p>
<p>这个时候，在Cookie字段可以找到cookie数据，复制后存为txt文件，然后写代码读取txt文件，并存为字典格式数据。</p>
<pre><code>f_cookies = open(&apos;cookie.txt&apos;, &apos;r&apos;)
cookies = {}
for line in f_cookies.read().split(&apos;;&apos;):
    name, value = line.strip().split(&apos;=&apos;, 1)
    cookies[name] = value
</code></pre><p>用requests对网页进行爬虫抓取之后，此后就是利用Beautifulsoup对获取的html进行解析，获取豆瓣用户评论，以及后页的链接。<br><img src="http://i.imgur.com/8gnYM4R.png" alt=""></p>
<p>对《战狼2》的豆瓣影评链接进行分析，发现每一页链接都是如上图的组成，网页解析可以获取后面的红色字段，实现不断向后页爬虫。最后爬虫结果得到18M左右的数据。</p>
<h2 id="jieba模块提取评论内容关键词"><a href="#jieba模块提取评论内容关键词" class="headerlink" title="jieba模块提取评论内容关键词"></a>jieba模块提取评论内容关键词</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Wed Aug  9 09:51:51 2017

@author: lkj
&quot;&quot;&quot;
import codecs
import jieba
import matplotlib.pyplot as plt  
import matplotlib as mpl 
import numpy as np 
from collections import Counter

zhfont1 = mpl.font_manager.FontProperties(fname=&apos;C:\Windows\Fonts\simsun.ttc&apos;)

def draw_bar(labels,quants):  
    width = 0.4  
    ind = np.linspace(0.5,9.5,10)  
    # make a square figure  
    fig = plt.figure(1)  
    ax  = fig.add_subplot(111)  
    # Bar Plot  
    ax.bar(ind-width/2,quants,width,color=&apos;green&apos;)  
    # Set the ticks on x-axis  
    ax.set_xticks(ind)  
    ax.set_xticklabels(labels,fontproperties=zhfont1)  
    # labels  
    ax.set_xlabel(u&apos;关键词&apos;,fontproperties=zhfont1)  
    ax.set_ylabel(u&apos;评论数量&apos;,fontproperties=zhfont1)  
    # title  
    ax.set_title(u&apos;筛选后的TOP10关键词&apos;, bbox={&apos;facecolor&apos;:&apos;0.8&apos;, &apos;pad&apos;:5},fontproperties=zhfont1)  
    #plt.legend(prop=zhfont1)
    plt.grid(True)  
    plt.show()   

word_lists = []  # 关键词列表
with codecs.open(&apos;comments.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;) as f:
    Lists = f.readlines()  # 文本列表
    for List in Lists:
        cut_list = list(jieba.cut(List))
        for word in cut_list:
            word_lists.append(word)
word_lists_set = set(word_lists)  # 去除重复元素
sort_count = []
word_lists_set = list(word_lists_set)
length = len(word_lists_set)
print(u&quot;共有%d个关键词&quot; %length)
k = 1
for w in word_lists_set:
    sort_count.append(w + u&apos;:&apos; + str(word_lists.count(w)) + u&quot;次\n&quot;)
    print (u&quot;%d---&quot; % k + w + u&quot;:&quot; + str(word_lists.count(w)) + u&quot;次&quot;)
    k += 1
with codecs.open(&apos;count_word.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as f:
    f.writelines(sort_count)
#先取出前100关键词，再进行人为筛选
key_words_TOP100=[]
key_words_TOP100=Counter(word_lists).most_common(100)
key_words_shaixuan=[key_words_TOP100[6],key_words_TOP100[24],key_words_TOP100[25],
                    key_words_TOP100[30],key_words_TOP100[39],key_words_TOP100[52],
                    key_words_TOP100[60],key_words_TOP100[77],key_words_TOP100[78],
                    key_words_TOP100[94]]
labels = []
quants = []
for i in range(10):
    labels.append(key_words_shaixuan[i][0])
    quants.append(key_words_shaixuan[i][1])
draw_bar(labels,quants)
</code></pre><p>绘制柱形图的时候需要指定字体，不然会出现中文乱码。对关键词TOP100需要进行人为筛选，因为jieba分词会出现很多诸如“我们”之类的在这里无意义的词汇，人为筛选出TOP10关键词如下：</p>
<p><img src="http://i.imgur.com/M2v23dA.png" alt=""></p>
<p>从关键词来看，大多数网友还是看好这部电影的，认为这是大场面的动作戏，达到了好莱坞大片水平，当然也不乏网友认为这是满足吴京个人英雄主义的意淫。</p>
<h2 id="绘制词云"><a href="#绘制词云" class="headerlink" title="绘制词云"></a>绘制词云</h2><pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Tue Aug  8 21:46:04 2017

@author: lkj
&quot;&quot;&quot;

# -*- coding:utf-8 -*-
import codecs

import jieba
from scipy.misc import imread
from wordcloud import WordCloud


# 绘制词云
def save_jieba_result():
    # 设置多线程切割
    #jieba.enable_parallel(4)
    with codecs.open(&apos;comments.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()
    cut_text = &quot; &quot;.join(jieba.cut(comment_text))  # 将jieba分词得到的关键词用空格连接成为字符串
    with codecs.open(&apos;jieba.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f:
        f.write(cut_text)


def draw_wordcloud2():
    with codecs.open(&apos;jieba.txt&apos;, encoding=&apos;utf-8&apos;) as f:
        comment_text = f.read()

    color_mask = imread(&quot;zhanlang2.jpg&quot;)  # 读取背景图片

    stopwords = [u&apos;就是&apos;, u&apos;电影&apos;, u&apos;你们&apos;, u&apos;这么&apos;, u&apos;不过&apos;, u&apos;但是&apos;, u&apos;什么&apos;, u&apos;没有&apos;, u&apos;这个&apos;, u&apos;那个&apos;, u&apos;大家&apos;, u&apos;比较&apos;, u&apos;看到&apos;, u&apos;真是&apos;,
                 u&apos;除了&apos;, u&apos;时候&apos;, u&apos;已经&apos;, u&apos;可以&apos;,u&apos;湄公河&apos;]
    cloud = WordCloud(font_path=&quot;MSYH.TTF&quot;, background_color=&apos;white&apos;,
                      max_words=2000, max_font_size=200, min_font_size=4, mask=color_mask,stopwords=stopwords)
    word_cloud = cloud.generate(comment_text)  # 产生词云
    word_cloud.to_file(&quot;zhanlang2_cloud.jpg&quot;)

save_jieba_result()
draw_wordcloud2()
</code></pre><p>词云的绘制需要需要指定font_path，不然会出现中文乱码，我在网上下好微软雅黑的字体（.TTF文件）一并放在目录下调用。</p>
<h2 id="也想说两句"><a href="#也想说两句" class="headerlink" title="也想说两句"></a>也想说两句</h2><p>电影的后面，红旗飘扬，进过敌战区的吴京身披五星红旗大摇大摆经过，那一刻真的为作为一个中国人而感到自豪。<br><img src="http://i.imgur.com/bUYDPL5.jpg" alt=""><br>有人觉得看的剧情尴尬，完全是吴京个人英雄主义的表现，但是在《看速度与激情》一个人干翻整个俄罗斯核基地为什么不觉得尴尬呢？我们接受了太多的美国大片以及美国的个人英雄主义的意识形态的输出，有美国队长能够拯救世界，为什么中国队长不行呢？现在我们的国家也是越来越强大，很欣慰能有《湄公河行动》、《战狼2》这样的主旋律大片，虽然有很多不好的声音，也有越来越多的人被感染，而认同，《战狼2》上映两周就登顶国内票房冠军就印证了这一切。</p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&id=491295324&auto=0&height=66"></iframe>

<p>不可否认，在调动自然流露爱国情愫上，《战狼2》是成功的，它在我不知不觉的情感代入里推揉了我的泪腺。当舰长青筋怒暴将憋在心中已久的爱国情感汇成一句“开火”时，男主冷锋独自潜入暴乱的非洲国家拼尽全力解救侨胞、身处险境孤立无援的那种英雄悲凉绝望感，瞬间倾倒而出。祖国，在这一刻，有了最真切感受。</p>
<p>当然，《战狼2》还是存在很多瑕疵，但是也为国产电影树立了一个新的标杆，相信国产大片也会越来越好，期待有更多像《战狼2》这样优秀的电影。</p>
<p>最后，很喜欢这部电影的结尾，中华人民共和国的护照。<br><img src="http://i.imgur.com/DG2aone.jpg" alt=""></p>
]]></content>
      
        
        <tags>
            
            <tag> 电影 </tag>
            
            <tag> 爬虫 </tag>
            
            <tag> python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[hexo-github搭建个人博客]]></title>
      <url>/2017/08/06/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<p>搭好个人网站之后纠结于第一篇博客的内容，想想还是首先分享一下自己搭建这个网站的过程，毕竟还是碰到了很多坑，解决了一些问题才成功的。<br><a id="more"></a></p>
<h2 id="配置准备"><a href="#配置准备" class="headerlink" title="配置准备"></a>配置准备</h2><p>配置环境是win10，其实在linux和mac环境下的搭建过程也是类似。</p>
<ol>
<li>安装node。一种静态博客框架，到<a href="https://nodejs.org/en/" title="Node.js官网" target="_blank" rel="external">Node.js官网</a>上下载，根据自己电脑版本下载对应文件，按照安装提示一路安装即可。</li>
<li>GitHub账号申请，已有可以跳过。Github用来做博客的远程仓库，后期需要配置域名，以及和本地的hexo创建连接。没有账号需要在GitHub官网进行申请，以及SSH key的配置，网上可以搜到很多教程。</li>
<li>git的安装。把本地的hexo上传到GitHub上去，安装过程可以参考<a href="https://git-scm.com/book/zh/v1/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git" target="_blank" rel="external">官方教程</a>。</li>
</ol>
<h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><p>Node.js和Git安装好之后就可以正式安装了，首先创建一个空文件夹比如hexo，在cmd窗口下进入到这个文件夹，此后的命令都是在这个文件夹下进行。</p>
<p>执行如下命令安装hexo：</p>
<pre><code>sudo npm install -g hexo
</code></pre><p>而后初始化hexo：</p>
<pre><code>hexo init
</code></pre><p>生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>这时候在启动本地服务，就能在本地浏览器进行预览：<br>    hexo server</p>
<p>浏览器输入<a href="http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。" target="_blank" rel="external">http://localhost:4000,这个时候就能在浏览器看到，但是并没有与域名绑定，除了自己的本地电脑，在网络上无法访问。</a></p>
<h2 id="配置GitHub"><a href="#配置GitHub" class="headerlink" title="配置GitHub"></a>配置GitHub</h2><p>首选需要建立仓库（repository),仓库的名字格式必须为：你的GitHub账户名+.github.io,比如说你的github账户名为abc，那么你的仓库名为abc.github.io</p>
<p>同时，本地的hexo文件夹可以看到以下内容：<br><img src="http://i.imgur.com/NL4v9X7.png" alt=""><br>打开_config.yml文件，下滑到最后进行编辑，修改成下面的样子：</p>
<pre><code>deploy:
  type: git
  repo: https://github.com/xxx/xxx.github.io.git
  branch: master
</code></pre><p><strong>注意</strong>：xxx需要改为你自己的github账户名，而且格外注意的是，所有的冒号后面必须加一个空格，否则hexo指令会执行不成功。</p>
<p>执行hexo generate命令生成静态页面：</p>
<pre><code>hexo generate
</code></pre><p>执行命令安装hexo的git上传工具：</p>
<pre><code>npm install hexo-deployer-git --save
</code></pre><p>再执行配置命令：</p>
<pre><code>hexo deploy
</code></pre><p>命令时终端会提示你输入Github的用户名和密码，输入配置好后，本地文件就会上传至你的github仓库。</p>
<h2 id="域名绑定"><a href="#域名绑定" class="headerlink" title="域名绑定"></a>域名绑定</h2><p>笔者是在阿里云上后买的<a href="lkj666.top">lkj666.top</a>域名，然后需要与github page进行绑定。<br>在域名解析管理的页面下，添加以下几条：<br><img src="http://i.imgur.com/609hZBT.png" alt=""><br>其中：</p>
<ol>
<li>CNAME记录类型会将你的域名进行别名指向。可以为一个主机设置别名在这里可以设置自己的域名指向xxx.github.io，以后就可以用自己的域名来代替访问xxx.github.io.</li>
<li>此外还增加了两条A类型，来指定github.com的IP地址，以及xxx.github.io的IP地址，可以用ping指令的方式来获取。</li>
</ol>
<p>此外，还需要修改本地以及仓库的CNAME文件：<br>在本地hexo的source文件夹下新建一个CNAME文件，将自己的域名写入，比如我将自己的域名lkj666.top写入CNAME文件一定要记住，是所有文件格式，不是txt文件格式。<br>在对应的xxx.github.io仓库根目录下新建一个文件CNAME，同样将自己的域名写入。</p>
<p>做好上述过程后再执行下列命令:</p>
<pre><code>hexo clean //每次执行提交之前最好执行这个clean命令来清除缓存
hexo generate
hexo deploy
</code></pre><p>此后就可以通过你的域名访问你自己的博客了。</p>
]]></content>
      
        
    </entry>
    
  
  
</search>
