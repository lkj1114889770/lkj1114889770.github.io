<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>听歌的小孩</title>
  <subtitle>好好学习，好好科研</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-09T10:47:16.947Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>听歌的小孩</name>
    <email>kaijianliu@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>计算机视觉中的attention机制</title>
    <link href="http://yoursite.com/2019/10/09/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84attention%E6%9C%BA%E5%88%B6/"/>
    <id>http://yoursite.com/2019/10/09/计算机视觉中的attention机制/</id>
    <published>2019-10-09T07:26:18.000Z</published>
    <updated>2019-10-09T10:47:16.947Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>所谓attention机制，即是聚焦于某个局部信息的机制，这样比较符合人脑和人眼的感知机制。这篇文章对计算机视觉领域一些常见的attention机制进行了归纳总结。</p>
<a id="more"></a>
<h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><p><a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="external">Squeeze-and-Excitation Networks</a>（简称 SENet）是一种通道上的attention机制，通过自学习的方式，显式地建模每个特征图通道的重要程度，从而提升有用特征，并且抑制对当前任务用处不大的特征。</p>
<p><img src="/img/计算机视觉中的attention机制/1570606969643.png" alt="1570606969643"></p>
<p>SE block基本结构如上图，$F_{sq}$操作为全局平均池化(global average pool)，这样每个通道最终得到一个实数，即为$1 \times 1 \times C$的向量，经过两次全连接，输出的依然是$1 \times 1 \times C $的向量，然后用这个向量和原先的特征图通道相乘，从而对每个通道重新标定。</p>
<h2 id="可变形卷积"><a href="#可变形卷积" class="headerlink" title="可变形卷积"></a>可变形卷积</h2><h3 id="Deformable-Convolution-v1"><a href="#Deformable-Convolution-v1" class="headerlink" title="Deformable Convolution v1"></a>Deformable Convolution v1</h3><p>可变形卷积在卷积采样的时候引入了attention，即加入了offset，能够适应不同尺度、长宽比和角度。</p>
<p><img src="/img/计算机视觉中的attention机制/1570608172366.png" alt="1570608172366"></p>
<p>可变形卷积加入了额外的卷积层，预测得到$2N$维度且和原始feature map同样大小的offset，然后根据这个offset在feature map上进行采样卷积操作。</p>
<p><img src="/img/计算机视觉中的attention机制/1570608747406.png" alt="1570608747406"></p>
<p>deformable RoI pooling 同样也需要offsets参数，不过这里是利用标准的RoI Pooling之后得到的$k \times k$，通过全连接的到$k \times k$的offsets，由于RoI区域大小不同，所以还需要进一步处理：</p>
<script type="math/tex; mode=display">
\Delta p_{ij}  = \gamma \Delta \mathop p ^ -_{ij}  \circ (w,h)</script><p>这里，$\gamma$为预定义的系数，取0.1，$\mathop p\limits^ -  _{ij}$为输出的归一化的offset。</p>
<h3 id="Deformable-Convolution-v2"><a href="#Deformable-Convolution-v2" class="headerlink" title="Deformable Convolution v2"></a>Deformable Convolution v2</h3><p>Deformable Convolution v2进一步增加了可变形卷积的自由度，即不仅仅预测offsets，采样点的权重也同时重新标定，为可调节的可变形卷积：</p>
<script type="math/tex; mode=display">
y(p) = \sum\limits_{k = 1}^K {w_k  \cdot x(p + p_k  + \Delta p_k ) \cdot \Delta m_k }</script><p>这里$\Delta m_k$为可学习的权重参数。</p>
<h2 id="Non-local-Neural-Networks"><a href="#Non-local-Neural-Networks" class="headerlink" title="Non-local Neural Networks"></a>Non-local Neural Networks</h2><p>Non-local 机制在提取某处特征时利用其周围点的信息，其实本质上是一种scaled dot-product attention，这里就先介绍一下scaled dot-product attention，后面会发现这和non-local是一样的。</p>
<p><img src="/img/计算机视觉中的attention机制/1570610071889.png" alt="1570610071889"></p>
<p>这里有三个名词query Q, key K, value V，本质上，attention可以描述为一个query到一系列key-value的映射。其计算主要分三步：</p>
<ul>
<li>第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；</li>
<li>第二步一般是使用一个softmax函数对这些权重进行归一化；</li>
<li>最后将权重和相应的键值value进行加权求和得到最后的attention。</li>
</ul>
<p>scaled dot-product attention是一种self-attention机制：</p>
<script type="math/tex; mode=display">
Attention(Q_i ,K,V) = soft\max (v_i  * [v_1^T ,v_2^T ,...,v_n^T ]) * \left[ \begin{array}{l}
 v_1  \\ 
 v_2  \\ 
 ... \\ 
 v_n  \\ 
 \end{array} \right] = soft\max (Q_i K^T )V</script><p>在nlp中，这里的$v_i$对应于word embedding，在CV中，可以对应于特征图某一点的特征向量。了解了上述知识，再来介绍non-local。non-local操作用公式可以表示为：</p>
<script type="math/tex; mode=display">
y_i  = \frac{1}{C(x)}\sum\limits_{\forall j} {f(x_i ,x_j )g(x_j )}</script><p>其中，某一点$i$的响应值$y_i$，与$g(x_j)$和$x_i,x_j$之间的信息得到的权重有关，$g(x_j)$可对应卷积操作，$f(x_i,x_j)$显式地建模空间域上计算$x_j$对$y_i$贡献的权重，这个的计算通常是Gaussian，Embedded Gaussian，Dot product，Concatenation等，这里介绍所使用的的Embedded Gaussian，即：</p>
<script type="math/tex; mode=display">
f(x_i ,x_j ) = e^{\theta (x_i )^T \varphi (x_j )}</script><p>这里的$\theta()$和$\varphi()$  non-local使用$1 \times 1$的卷积进行了实现，采用softmax操作就完成了f()函数的幂计算和除以C(x)计算）。将non local包装成一个block，这个block就类似ResNet网络中的block，这样non local操作就可以很方便地插入到现有的网络结构中:</p>
<script type="math/tex; mode=display">
z_i = W_z y_i+ x_i</script><p>这样，最终non local模块为：</p>
<p><img src="/img/计算机视觉中的attention机制/1570612231627.png" alt="1570612231627"></p>
<p>为了简化计算，θ、φ和g操作的卷积核数量设定为输入feature map通道数的一半（Figure2中512对1024）。</p>
<h2 id="Asymmetric-Non-local-Neural-Networks"><a href="#Asymmetric-Non-local-Neural-Networks" class="headerlink" title="Asymmetric Non-local Neural Networks"></a>Asymmetric Non-local Neural Networks</h2><p><a href="https://arxiv.org/pdf/1908.07678.pdf" target="_blank" rel="external">Asymmetric Non-local Neural Networks for Semantic Segmentation</a>，这篇19年的ICCV的文章对non-local进行了改进。</p>
<p><img src="/img/计算机视觉中的attention机制/1570612447568.png" alt="1570612447568"></p>
<p>non-local的一个缺点就是，计算量太大，计算复杂度为$O(CH^2W^2)$. non-local为对称的，Query和Key向量大小相等，Asymmetric Non-local Neural Networks采用了非对称式的，Key对应于$N \times S$大小，$S \ll N$，因此可以极大地减小计算量。</p>
<p>具体来说，应用于语义分割的时候，整体框架如下图：</p>
<p><img src="/img/计算机视觉中的attention机制/1570612715221.png" alt="1570612715221"></p>
<p>网络含有两个主要的模块，其中一个是Asymmetric Pyramid Non-local Block(APNB)和Asymmetric Fusion Non-local Block(AFNB)，本质上均为上述的非对称non-local，其中AFNB将stage4作为key和value，将stage5作为Query. 为了节省计算，APNB的key和value来源于同一个。</p>
<h2 id="Criss-Cross-Attention"><a href="#Criss-Cross-Attention" class="headerlink" title="Criss-Cross Attention"></a>Criss-Cross Attention</h2><p><a href="https://arxiv.org/pdf/1811.11721.pdf" target="_blank" rel="external">CCNet: Criss-Cross Attention for Semantic Segmentation</a>，这篇文章采用了一个两个连续的十字交叉的attention方式代替了non-local，也是可以大大简化计算，同时也取得了不错的效果。</p>
<p><img src="/img/计算机视觉中的attention机制/1570613225250.png" alt="1570613225250"></p>
<p>图中的绿色部分，即为计算feature map中一个点的操作，为一个十字形，non-local计算一个点的时候，需要计算feature map中所有点，即$H^2W^2$次，而十字形仅计算$(HW*(H+W-1))$次，而且如下图所示，通过连续两次，即可覆盖所有的点：</p>
<p><img src="/img/计算机视觉中的attention机制/1570613545669.png" alt="1570613545669"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;所谓attention机制，即是聚焦于某个局部信息的机制，这样比较符合人脑和人眼的感知机制。这篇文章对计算机视觉领域一些常见的attention机制进行了归纳总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>小目标检测总结</title>
    <link href="http://yoursite.com/2019/10/08/%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/10/08/小目标检测总结/</id>
    <published>2019-10-08T11:11:40.000Z</published>
    <updated>2019-10-08T12:12:41.231Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>目标检测最常见的faster rcnn以及yolo在大、中尺寸的目标上能取得不错的效果，但是在小目标上性能不太好，这篇文章针对小目标检测问题的方法进行了归纳总结。</p>
<a id="more"></a>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>FPN是最常用的多尺度检测的方法，通过将小目标分配给高分辨率的尺度进行检测，可以提高小目标的准确率。</p>
<p><img src="/img/小目标检测总结/timg.jpg" alt="点击查看源网页"></p>
<h2 id="SNIPER"><a href="#SNIPER" class="headerlink" title="SNIPER"></a>SNIPER</h2><p>一种多尺度训练的方法，根据原始标注，在原图上裁剪出包含目标的chips，然后调整至相同大小的尺寸给网络进行训练，相当于对大目标进行了缩小，对小目标进行了放大。</p>
<p><img src="/img/小目标检测总结/1570533764133.png" alt="1570533764133"></p>
<h2 id="YOLT"><a href="#YOLT" class="headerlink" title="YOLT"></a>YOLT</h2><p><a href="https://arxiv.org/abs/1805.09512" target="_blank" rel="external">YOLT (you only look twice)</a>, 这个方法是针对大图像（比如卫星图像）中的小目标进行检测的方法，主要的方法是：1、将原图进行切块，然后分别输入模型进行检测；2、将图像放大至不同尺度，训练几个不同尺度的模型，将检测结果进行融合。</p>
<p><img src="/img/小目标检测总结/1570535124525.png" alt="1570535124525"></p>
<h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data <strong>Augmentation</strong></h2><p>CVPR2019论文: Augmentation for small object detection</p>
<ol>
<li>对包含小目标的图片进行过采样</li>
<li>对小目标在图片上复制粘贴多次</li>
</ol>
<p><img src="/img/小目标检测总结/1570535461483.png" alt="1570535461483"></p>
<h2 id="Proposal-区域进行超分辨率"><a href="#Proposal-区域进行超分辨率" class="headerlink" title="Proposal 区域进行超分辨率"></a>Proposal 区域进行超分辨率</h2><p><a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2017/Improving-SmallObject-Detection.pdf" target="_blank" rel="external">Improving Small Object Detection</a>: 对proposal区域采用Convolutional-deconvolutional network进行超分辨率。该网络的结构是对称的，每个卷积层都有对应的反卷积层。卷积层用来获取图像的抽象内容，反卷积层用来放大特征尺寸并且恢复图像细节。</p>
<p><img src="/img/小目标检测总结/1570535852438.png" alt="1570535852438"></p>
<h2 id="contextual-information"><a href="#contextual-information" class="headerlink" title="contextual information"></a>contextual information</h2><p><a href="https://arxiv.org/ftp/arxiv/papers/1709/1709.05054.pdf" target="_blank" rel="external">Feature-Fused SSD: Fast Detection for Small Objects</a>，这篇文章将上下文信息引入，以提高小目标的检测识别准确率。figure1中，海边这个信息对于船的检测识别，人对于瓶子，自行车对于人，这些上下文信息的引入，都可以提高检测结果。</p>
<p><img src="/img/小目标检测总结/1570535491620.png" alt="1570535491620"></p>
<p>论文整体思路还是比较简单，使用高层的特征图来加强低层特征图的语义信息，以增加小物体检测精度。</p>
<p><img src="/img/小目标检测总结/1570535682913.png" alt="1570535682913"></p>
<h2 id="Anchor设置"><a href="#Anchor设置" class="headerlink" title="Anchor设置"></a>Anchor设置</h2><p>根据小目标的尺寸，设置合适的anchor boxes</p>
<h2 id="DetNet"><a href="#DetNet" class="headerlink" title="DetNet"></a>DetNet</h2><p>针对目标检测，对特征提取网络进行了优化，主要包括：</p>
<ol>
<li>增加网络高层输出特征的分辨率，将stride等于32修改为stride等于16。</li>
<li>引入dilated卷积层增加网络高层的感受野，这是因为第一个改进点引起的感受野减小。</li>
<li>减小网络高层的宽度，减少因增大分辨率带来的计算量。</li>
</ol>
<h2 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h2><p>HRNet通过加入高分辨率特征图和低分辨率特征图的连接，在特征提取的同时保持高分辨率表示，可以有益于小目标检测。</p>
<p><a href="https://github.com/HRNet/HRNet-Object-Detection/blob/master/images/hrnetv2p.png" target="_blank" rel="external"><img src="/img/小目标检测总结/hrnetv2p.png" alt="img"></a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目标检测最常见的faster rcnn以及yolo在大、中尺寸的目标上能取得不错的效果，但是在小目标上性能不太好，这篇文章针对小目标检测问题的方法进行了归纳总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>RepPoints:用代表性点集进行目标检测</title>
    <link href="http://yoursite.com/2019/09/26/RepPoints%EF%BC%9A%E7%94%A8%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%82%B9%E9%9B%86%E8%BF%9B%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/09/26/RepPoints：用代表性点集进行目标检测/</id>
    <published>2019-09-26T14:09:30.000Z</published>
    <updated>2019-10-11T14:22:49.442Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://arxiv.org/abs/1904.11490" target="_blank" rel="external">RepPoints</a>提出了用代表点进行更细粒度的定位和更方便的分类，并且在这个基础上，实现了anchior-free方法的multi-stage级联，达到了和anchor-based方法相媲美的精度。</p>
<a id="more"></a>
<h2 id="目标的表示"><a href="#目标的表示" class="headerlink" title="目标的表示"></a>目标的表示</h2><p>基于bounding box的方法通常是采用$(x_p,y_p,w_p,h_p)$来表示一个目标，通过预测一个4维向量$(\Delta x_p,\Delta y_p,\Delta w_p,\Delta h_p)$,来回归得到最终的目标框：</p>
<script type="math/tex; mode=display">
B_r = (x_p+w_p\Delta x_p, y_p + h\Delta y_p,w_pe^{\Delta w_p},h_pe^{\Delta h_p})</script><p>这样基于multi_stage的方式，bbox的生成如下图，从默认的anchor第一次得到bbox proposal，然后作为下一阶段的输入，进一步refine，最终得到最后的结果。</p>
<p><img src="/img/用代表性点集进行目标检测/1569508562946.png" alt="1569508562946"></p>
<p>RepPoints的方法不同于bbox采用四元组进行回归，而是预测一组关键特征点：</p>
<script type="math/tex; mode=display">
R=\{x_k,y_k\}_k^{n},</script><p>论文中，默认$k=9$;这样，RepPoints的refine可以表示为：</p>
<script type="math/tex; mode=display">
R_r = \{(x_k+\Delta x_k,y_k + \Delta y_k) \}_k^n,</script><p>这样如果是multi_stage的refine，可以相应表示为：</p>
<p><img src="/img/用代表性点集进行目标检测/1569508966664.png" alt="1569508966664"></p>
<p>由于得到是分布于目标上的关键点，还是需要转换成bbox来表示最终的物体位置，同时RepPoints也是利用bbox计算定位损失函数的。转换成bbox的方法，论文中提到了三种方法：1）直接对x,y坐标取最大值最小值；2）取部分点对x，y坐标取最大值，最小值；3）基于RepPoints的均值和标准差进行计算。</p>
<p>得到的bbox，论文中称之为伪框（pseudo box）。通过伪框来计算定位损失，计算伪框的左上角和右下角和gt的距离损失。</p>
<p><img src="/img/RepPoints：用代表性点集进行目标检测/1569509738415.png" alt="1569509738415"></p>
<h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><p><img src="/img/RepPoints：用代表性点集进行目标检测/1569509867224.png" alt="1569509867224"></p>
<p>总体架构如上图所示，RPDet采用FPN结构，上图展示了其中一个尺度。这里有两个阶段，初始的第一阶段，将feature map上的点作为初始点（图中的绿色点），预测9个$(\Delta x,\Delta y)$参数（图中的蓝色点）；然后进入第二阶段，在这9个点的基础上进行refine，通过conv预测offset，得到最终的RepPoints. 检测头的框架如下图：</p>
<p><img src="/img/RepPoints：用代表性点集进行目标检测/1569547217902.png" alt="1569547217902"><br>分类分支和定位分支作为两个并行的分支，分类的值利用locate subset<strong>第一阶段</strong>预测的offset1进行可变形卷积，然后通过$1\times 1$卷积输出class预测结果。locate subsets第一阶段通过卷积直接预测offset1，然后采用可变形卷积核标准conv $1\times 1$卷积得到offset2，两次refine得到最终坐标。</p>
<h2 id="正负样本分配"><a href="#正负样本分配" class="headerlink" title="正负样本分配"></a>正负样本分配</h2><p>对于第一阶段的定位，正样本分配原则为：1）首先根据目标大小，分配到对应的feature map尺度，尺度计算为$s(B)=log_2(\sqrt {w_Bh_B/4} )$;2）目标的gt中心落在对应的feature map bin。第二阶段的真样本为：伪框和gt的IoU大于0.5的。</p>
<p>分类分支，将和伪框的IoU大于0.5的认为是正样本，小于0.4为负样本，在此之间的忽略不计。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.11490&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;RepPoints&lt;/a&gt;提出了用代表点进行更细粒度的定位和更方便的分类，并且在这个基础上，实现了anchior-free方法的multi-stage级联，达到了和anchor-based方法相媲美的精度。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Anchor-free 目标检测新算法盘点</title>
    <link href="http://yoursite.com/2019/09/02/Anchor-free%20%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B0%E7%AE%97%E6%B3%95%E7%9B%98%E7%82%B9/"/>
    <id>http://yoursite.com/2019/09/02/Anchor-free 目标检测新算法盘点/</id>
    <published>2019-09-02T11:56:13.000Z</published>
    <updated>2019-09-26T07:13:16.118Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对2019年出现的几个新的Anchor-free目标检测算法进行盘点：<a href="https://arxiv.org/abs/1904.08900" target="_blank" rel="external">CornerNet-Lite</a>，<a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="external">FCOS</a>，<a href="https://arxiv.org/abs/1904.08189" target="_blank" rel="external">CenterNe</a>t</p>
<a id="more"></a>
<h2 id="CornerNet-Lite"><a href="#CornerNet-Lite" class="headerlink" title="CornerNet-Lite"></a>CornerNet-Lite</h2><p>CornetNet-Lite是CornerNet的优化版，提出了两种算法：</p>
<ul>
<li>CornerNet-Saccade，提高速度</li>
<li>CornerNet-Squeeze，提高准确率</li>
</ul>
<h3 id="CornerNet-Saccade"><a href="#CornerNet-Saccade" class="headerlink" title="CornerNet-Saccade"></a>CornerNet-Saccade</h3><p>Saccades 是 <strong><em>single type and multi-object</em></strong>。CornerNet-Saccade 检测图像中可能的目标位置周围的小区域内的目标。它使用缩小后的完整图像来预测注意力图和粗边界框；两者都提出可能的对象位置，然后，CornerNet-Saccade通过检查以高分辨率为中心的区域来检测目标。</p>
<p><img src="/img/One-stage目标检测新算法盘点/1567426867914.png" alt="1567426867914"></p>
<p>其实思路很简单，首先在低分辨率的图上进行粗检测，然后在高分辨比率图片上检测粗检测出来的区域，进一步检测目标物，有一种级联的感觉。这里Saccade就是先扫一下全图，粗检测确定一些目标区域，然后在高分辨率图上进行细检测。</p>
<h3 id="CornerNet-Squeeze"><a href="#CornerNet-Squeeze" class="headerlink" title="CornerNet-Squeeze"></a><strong>CornerNet-Squeeze</strong></h3><p>CornerNet中，大部分计算资源都花在了Hourglass-104上。Hourglass-104 由残差块构成，其由两个3×3卷积层和跳连接（skip connection）组成。CornerNet-Squeeze借鉴了SqueezeNet和mobileNet来设计了一个轻量级的Hourglass结构。主要就是本文将CornerNet-Squeeze中的残差模块替换为fire module。将其中第二层的3x3的标准卷积替换为深度可分离卷积。</p>
<h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p><strong>CornerNet-Squeeze为34ms / 34.4 mAP，而YOLOv3为39ms / 33mAP</strong>，<strong>CornerNet-Saccade精度可达42.6mAP。</strong></p>
<h2 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h2><p>FCOS(FCOS: Fully Convolutional One-Stage Object Detection)，该算法是一种基于FCN的逐像素目标检测算法。</p>
<p><img src="/img/One-stage目标检测新算法盘点/1567427820413.png" alt="1567427820413"></p>
<p>FCOS直接对feature map中每个位置对应原图的边框都进行回归，换句话说FCOS直接把每个位置都作为训练样本，这一点和FCN用于语义分割相同。在FCOS中，如果位置 $(x,y)$落入任何真实边框，就认为它是一个正样本，它的类别标记为这个真实边框的类别。同时采用了FPN进行多级检测。</p>
<h3 id="Center-ness-For-FCOS"><a href="#Center-ness-For-FCOS" class="headerlink" title="Center-ness  For FCOS"></a>Center-ness  For FCOS</h3><p><img src="/img/One-stage目标检测新算法盘点/1567427959778.png" alt="1567427959778"></p>
<p>采用Center-ness抑制离目标中心位置较远的与侧边框，添加了Center-ness分支，预测Center-ness。</p>
<p><img src="/img/One-stage目标检测新算法盘点/1567428160976.png" alt="1567428160976"></p>
<p>可以看到其取值为0-1，采用交叉熵进行训练；预测的时候可以将Center-ness与分类分数相乘，降低远离中心的bbox权重。</p>
<h3 id="performance"><a href="#performance" class="headerlink" title="performance"></a>performance</h3><p><img src="/img/One-stage目标检测新算法盘点/1567428272744.png" alt="1567428272744"></p>
<h2 id="CenterNet-Keypoint-Triplets-for-Object-Detection"><a href="#CenterNet-Keypoint-Triplets-for-Object-Detection" class="headerlink" title="CenterNet: Keypoint Triplets for Object Detection"></a>CenterNet: Keypoint Triplets for Object Detection</h2><p>相比于CornerNet，只预测左上角和右下角，这样无法利用物体内部特征，产生了很多误检测，CenterNet还预测一个中心点，这样就是预测左上角、右下角、中心点三元组。</p>
<p><img src="/img/One-stage目标检测新算法盘点/1567428836242.png" alt="1567428836242"></p>
<h3 id="Keypoint-Triplets"><a href="#Keypoint-Triplets" class="headerlink" title="Keypoint Triplets"></a>Keypoint Triplets</h3><p>对每个预测框定义一个中心区域，判断每个中心区域是否有中心点，如有则保留，并且此时框的 confidence 为中心点，左上角点和右下角点的confidence的平均；否则去除这个bbox，这样就是的网络具备了目标内部信息感知的能力。采用一种尺度可调节的区域中心定义方法，该方法可以在预测框的尺度较大时定义一个相对较小的中心区域，在预测框的尺度较小时预测一个相对较大的中心区域</p>
<p><img src="/img/One-stage目标检测新算法盘点/1567429034183.png" alt="1567429034183"></p>
<h3 id="Center-Pooling-amp-amp-Cascade-corner-pooling"><a href="#Center-Pooling-amp-amp-Cascade-corner-pooling" class="headerlink" title="Center Pooling &amp;&amp; Cascade corner pooling"></a>Center Pooling &amp;&amp; Cascade corner pooling</h3><p><img src="/img/One-stage目标检测新算法盘点/1567429189510.png" alt="1567429189510"></p>
<p>Center Pooling提取中心点水平方向和垂直方向的最大值相加，作为中心点特征。传统做法的 corner pooling。它提取物体边界最大值并相加，该方法只能提供关联物体边缘语义信息，对于更加丰富的物体内部语义信息则很难提取到。图4(c)为cascade corner pooling 原理，它首先提取物体边界最大值，然后在边界最大值处继续向内部(图中沿虚线方向)提取提最大值，并与边界最大值相加，以此给角点特征提供更加丰富的关联物体语义信息。Cascade corner pooling 也可通过不同方向上的 corner pooling 的组合实现.</p>
<h3 id="performance-1"><a href="#performance-1" class="headerlink" title="performance"></a>performance</h3><p>输入511X511,Hourglass-104作为backbone,multi-scale可达<strong>47.0 mAP</strong>, single-scale 44.9 mAP。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对2019年出现的几个新的Anchor-free目标检测算法进行盘点：&lt;a href=&quot;https://arxiv.org/abs/1904.08900&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CornerNet-Lite&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/pdf/1904.01355.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FCOS&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/1904.08189&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CenterNe&lt;/a&gt;t&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>实例分割之PANet-Aggregation Network for Instance Segmentation</title>
    <link href="http://yoursite.com/2019/05/09/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E4%B9%8BPANet-Aggregation-Network-for-Instance-Segmentation/"/>
    <id>http://yoursite.com/2019/05/09/实例分割之PANet-Aggregation-Network-for-Instance-Segmentation/</id>
    <published>2019-05-09T11:43:45.000Z</published>
    <updated>2019-06-05T14:34:19.245Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这篇文章提出了一种Path Aggregation Network(PANet)，针对常规的多特征图从上到下（top-down）的信息传播，添加了自下向上（bottom-up）的路径，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。</p>
<p><img src="/img/PANet/1.png" alt="1"></p>
<a id="more"></a>
<p>主要的工作如下：</p>
<ul>
<li><p>创建自下而上（bottom-up）的增强路径（图b绿色虚线），可以缩短信息传播路径，在FPN中红色的车行路径可以超过100层，而这种绿色虚线的短路径10层都不到，这样增强了low-level信息传播，新的路径增强可以让底层信息流通更快。</p>
</li>
<li><p>提出了自适应池化层，即上图b部分，详细示意图如下图：</p>
</li>
</ul>
<p><img src="/img/PANet/2.png" alt="2"></p>
<p>  对每个候选区域，将其映射至不同特征层次，如上图的灰色部分，然后采用RoIAlign池化不同层次的特征网络，再采用融合操作，让网络适应特征。</p>
<ul>
<li>采用了FC层补充mask预测。如figure1(e)显示，详细如下图所示：</li>
</ul>
<p><img src="/img/PANet/3.png" alt="3"></p>
<p>  主分支采用和连续卷积核decconv，同时采用了一个段路径，从conv3到conv4_fc，conv5_fc，再到fc层。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章提出了一种Path Aggregation Network(PANet)，针对常规的多特征图从上到下（top-down）的信息传播，添加了自下向上（bottom-up）的路径，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/PANet/1.png&quot; alt=&quot;1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="实例分割" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>maskrcnn-benchmark 安装踩坑记</title>
    <link href="http://yoursite.com/2019/05/06/maskrcnn-benchmark%E5%AE%89%E8%A3%85%E8%B8%A9%E5%9D%91%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/05/06/maskrcnn-benchmark安装踩坑记/</id>
    <published>2019-05-06T10:52:50.000Z</published>
    <updated>2019-06-05T14:18:55.590Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>想用maskrcnn框架做一个检测任务，虽然根据官网指示，倒是过程中还是碰到好几个坑，从上午折腾到傍晚才搞定，暂且记录一下安装过程。</p>
<a id="more"></a>
<h3 id="gcc降级"><a href="#gcc降级" class="headerlink" title="gcc降级"></a>gcc降级</h3><p>我是用的是cuda9.0,需要的是5.3及以下的GCC，而maskrcnn benhmark要求GCC &gt;= 4.9，Ubuntu16.04自带的是GCC 5.4，所以需要进行降级，安装5.3版本：</p>
<ul>
<li>首选需要下载GCC源码： <a href="https://ftp.gnu.org/gnu/gcc/gcc-5.3.0/gcc-5.3.0.tar.gz" title="https://ftp.gnu.org/gnu/gcc/gcc-5.3.0/gcc-5.3.0.tar.gz" target="_blank" rel="external">https://ftp.gnu.org/gnu/gcc/gcc-5.3.0/gcc-5.3.0.tar.gz</a></li>
<li><p>解压<br><code>sudo tar -zxvf gcc-5.3.0.tar.gz （解压）</code></p>
</li>
<li><p>下载编译依赖项：</p>
<p>  cd gcc-5.3.0<br>  sudo ./contrib/download_prerequisites   </p>
</li>
<li><p>编译,，时间略长，20多分钟</p>
<p>  mkdir build<br>  cd build<br>  sudo  ../gcc-5.3.0/configure —enable-checking=release —enable-languages=c,c++ —disable-multilib<br>  sudo make -j4</p>
</li>
<li><p>安装</p>
</li>
<li>  sudo make install</li>
</ul>
<p>这个时候可以检查版本，gcc -v &amp;&amp; g++ -v,若显示版本不对，重启即可。</p>
<h3 id="官方step"><a href="#官方step" class="headerlink" title="官方step"></a>官方step</h3><pre><code>conda create --name maskrcnn_benchmark
conda activate maskrcnn_benchmark

# this installs the right pip and dependencies for the fresh python
conda install ipython

# maskrcnn_benchmark and coco api dependencies
pip install ninja yacs cython matplotlib tqdm opencv-python

# follow PyTorch installation in https://pytorch.org/get-started/locally/
# we give the instructions for CUDA 9.0
conda install -c pytorch pytorch-nightly torchvision cudatoolkit=9.0

export INSTALL_DIR=$PWD

# install pycocotools
cd $INSTALL_DIR
git clone https://github.com/cocodataset/cocoapi.git
cd cocoapi/PythonAPI
python setup.py build_ext install

# install apex
cd $INSTALL_DIR
git clone https://github.com/NVIDIA/apex.git
cd apex
python setup.py install --cuda_ext --cpp_ext

# install PyTorch Detection
cd $INSTALL_DIR
git clone https://github.com/facebookresearch/maskrcnn-benchmark.git
cd maskrcnn-benchmark

# the following will install the lib with
# symbolic links, so that you can modify
# the files if you want and won&#39;t need to
# re-build it
python setup.py build develop


unset INSTALL_DIR
</code></pre><p>这里出现了一个问题就是，耽误了很久：</p>
<pre><code> 1 error detected in the compilation of &quot;/tmp/tmpxft_0000279e_00000000-7_multi_tensor_l2norm_kernel.cpp1.ii&quot;.
error: command &#39;/usr/local/cuda/bin/nvcc&#39; failed with exit status 2
error
  Rolling back uninstall of apex
</code></pre><p>在apex编译安装的时候，应该是由于我的电脑有多个cuda(cuda8 cuda9)默认账户下cuda软连接到cuda8,但是我的账户.bashrc下制定了cuda9，所以nvcc -v看到的虽然还是cuda9,但是编译apex时候找的是默认路径下的cuda对应的cuda8,因此出现错误，这里找到github上的解决方法如下：</p>
<pre><code>CUDA_HOME=/usr/local/cuda-9.0 python setup.py install --cuda_ext --cpp_ext
</code></pre><p>即，编译的时候指定cuda9路径。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;想用maskrcnn框架做一个检测任务，虽然根据官网指示，倒是过程中还是碰到好几个坑，从上午折腾到傍晚才搞定，暂且记录一下安装过程。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文笔记-Region Proposal by Guided Anchoring(GA-RPN)</title>
    <link href="http://yoursite.com/2019/03/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Region-Proposal-by-Guided-Anchoring-GA-RPN/"/>
    <id>http://yoursite.com/2019/03/20/论文笔记-Region-Proposal-by-Guided-Anchoring-GA-RPN/</id>
    <published>2019-03-20T06:13:40.000Z</published>
    <updated>2019-03-25T03:11:39.260Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/pdf/1901.03278.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1901.03278.pdf</a></p>
<p>Anchor是目标检测领域的基石，目前比较先进的目标检测算法都是基于密集Anchor机制，通过预定的尺度和宽高比在feature map空间域上进行采样。这篇CVPR2019的文章提出了一种Guided Anchoring，根据语义信息来生成Anchor，主要是思想是定位目标的中心点以及相应的尺寸和宽高比，而且采用了feature adaption模块来缓解特征的不一致性。将GA-RPN应用到Fast R-CNN, Faster R-CNN和RetinaNet, 这篇论文在mAP上分别提高了 2.2%, 2.7% and 1.2%. </p>
<a id="more"></a>
<h2 id="Guided-Anchoring"><a href="#Guided-Anchoring" class="headerlink" title="Guided Anchoring"></a>Guided Anchoring</h2><p>从一个给定图片I，目标的位置和形状可以用4元组(x,y,w,h)来表示，并且满足：</p>
<p><img src="/img/GA-RPN/2.png" alt=""></p>
<p>这表明，目标是否存在依赖于在图中位置，而且目标的尺寸大小也依赖于位置。所以主要流程如下图所示，在其中一个特征图F1，location prediction分支产生概率图来表明目标的存在位置；shape prediction分支预测location-dependent（位置依赖)的形状。根据设定阈值选择位置以及对应位置的最可能的形状生成anchor。考虑到anchor形状不固定，采用一个Feature Adaption模块进行调整。</p>
<p><img src="/img/GA-RPN/1.png" alt=""></p>
<h2 id="Anchor-Location-Prediction"><a href="#Anchor-Location-Prediction" class="headerlink" title="Anchor Location Prediction"></a>Anchor Location Prediction</h2><p>Anchor位置预测分支输出和特征图F1一样大小的概率图，对F1中每一个位置p(i,j|F1)对应图片I中位置为((i+0.5)s,(j+0.5)s)，s为特征图F1的步长。具体来说，采用1x1的卷积对特征图进行卷积，然后采用Sigmoid转化成概率值，根据阈值再进行筛选。</p>
<h2 id="Anchor-Shape-Prediction"><a href="#Anchor-Shape-Prediction" class="headerlink" title="Anchor Shape Prediction"></a>Anchor Shape Prediction</h2><p>考虑到形状w，h变化范围较大，转换成：</p>
<p><img src="/img/GA-RPN/3.png" alt=""></p>
<p>其中s是步长，σ是经验尺度因子（文中取8）。可以将[0, 1000]压缩至[-1, 1]。该分支输出dw和dh。首先通过1*1卷积层产生两个通道的map（包括dw和dh的值），然后经过逐元素转换层实现w和h的转化。</p>
<h2 id="Anchor-Guided-Feature-Adaptation"><a href="#Anchor-Guided-Feature-Adaptation" class="headerlink" title="Anchor-Guided Feature Adaptation"></a>Anchor-Guided Feature Adaptation</h2><p>由于每个位置的形状不同，大的anchor对应较大感受野，小的anchor对应小的感受野。所以不能像之前基于anchor的方法那样直接对feature map进行卷积来预测，而是要对feature map进行feature adaptation。作者利用变形卷积的思想，根据形状对各个位置单独进行转换，其实就是把 anchor 的形状信息直接融入到特征图中，这样新得到的特征图就可以去适应每个位置 anchor 的形状。：</p>
<p><img src="/img/GA-RPN/4.png" alt=""></p>
<p>NT由3x3的可行变卷积层实现，如图1所示，首先再shape prediction分支预测offset field，然后对带偏移的原始feature map做变形卷积获得adapted features，然后再次基础上可以进一步完成分类以及bbx回归。</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Joint-objective"><a href="#Joint-objective" class="headerlink" title="Joint objective"></a>Joint objective</h3><p>多任务损失函数：</p>
<p><img src="/img/GA-RPN/5.png" alt=""></p>
<h3 id="Anchor-location-targets"><a href="#Anchor-location-targets" class="headerlink" title="Anchor location targets"></a>Anchor location targets</h3><p>为了训练anchor localization分支，采用ground truth bbx来指导label生成，1代表有效位置，0代表无效位置。考虑大中心附近的anchor应该较多，而远离中心的anchor数目应该少一些，假定R(x, y, w, h)表示以(x, y)为中心，w和h分别为宽高的矩形区域，首先将groundtruth的bbox(xg, yg, wg, hg)映射到feature map的尺度得到(x’g, y’g, w’g, h’g），Anchor最好是在gt object的中心附近才能有大的Iou，因此对每个box定义3种类型区域：</p>
<p><img src="/img/GA-RPN/6.png" alt=""></p>
<ol>
<li>定义中心区域CR=R(x’g, y’g, σ1w’, σ1h’)，CR区域内的像素标记为正样本；</li>
<li>定义ignore区域IR=R(x’g, y’g, σ2w’, σ2h’)\CR，该区域的像素标记为ignore；</li>
<li>其余区域标记为外部区域OR，该区域所有像素标记为负样本。<br>考虑到基于FPN利用了多层feature，所以只有当feature map与目标的尺度范围匹配时才标记为CR，而临近层相同区域标记为IR，如上图2所示。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/1901.03278.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1901.03278.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anchor是目标检测领域的基石，目前比较先进的目标检测算法都是基于密集Anchor机制，通过预定的尺度和宽高比在feature map空间域上进行采样。这篇CVPR2019的文章提出了一种Guided Anchoring，根据语义信息来生成Anchor，主要是思想是定位目标的中心点以及相应的尺寸和宽高比，而且采用了feature adaption模块来缓解特征的不一致性。将GA-RPN应用到Fast R-CNN, Faster R-CNN和RetinaNet, 这篇论文在mAP上分别提高了 2.2%, 2.7% and 1.2%. &lt;/p&gt;
    
    </summary>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之排序</title>
    <link href="http://yoursite.com/2019/03/01/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2019/03/01/数据结构与算法之排序/</id>
    <published>2019-03-01T03:09:50.000Z</published>
    <updated>2019-03-01T07:40:52.038Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>排序就是按照某种逻辑顺序将一组对象重新排列的过程，这篇博客就对常见的排序算法进行总结，包括：冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序。</p>
<a id="more"></a>
<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p>冒泡排序通过比较相邻的元素大小来完成排序。这里定义比较边界，也就是进行大小比较的边界。对于长度为n的数组，第一趟的比较边界为[0,n-1]，也就是说从a[0]开始，相邻元素两两比较大小，如果满足条件就进行交换，否则继续比较，一直到最后一个比较的元素为a[n-1]为止，此时第一趟排序完成；每一次排序完后最大元素沉入底部，比较边界变成[0,n-2]。对于长度为n的序列，最多需要n趟完成排序，所以冒泡排序就由两层循环构成，最外层循环用于控制排序的趟数，最内层循环用于比较相邻数字的大小并在本趟排序完成时更新比较边界。</p>
<p><img src="/img/sort/maopao.gif" alt=""></p>
<p>在排序后期可能数组已经有序了而算法却还在一趟趟的比较数组元素大小，可以引入一个标记，如果在一趟排序中，数组元素没有发生过交换说明数组已经有序，跳出循环即可。</p>
<p><strong>冒泡排序的时间复杂度为O(n^2),辅助空间为O(1),属于原地排序。</strong></p>
<h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><p>基本思想很简单：首先找到数组中最小的那个元素，然后和数组中第一个元素交换位置；然后在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置；如重复直到整个数组排序好。</p>
<p><strong>对于长度为N的数组，选择排序需要大约N^2/2次比较和N次交换，辅助空间也是O(1),属于原地排序。</strong></p>
<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><p>插入排序将数组分为有序部分和无序部分，与选择排序一样索引左边的都是有序的，为了给更小的元素腾出空间，他们都可能会被移动，当索引到达数组右端的时候，数组就排序完成了。</p>
<p><img src="/img/sort/charu.gif" alt=""></p>
<p>插入排序对一个很大且其中元素已经有序（或接近有序）的数组进行排序会比对随机顺序的数组或是逆序数组进行排序要快得多。</p>
<p>插入排序平均需要~N^2/4次比较和~N^2/4次交换，最坏情况下需要~N^2/2次比较和~N^2/2次交换，最好情况下需要N-1次比较和0次交换。</p>
<h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><p>对于大规模乱序数组，插入排序很慢，因为它只会交换相邻的元素，因此元素需要一点一点地从一端移动到另一端。<strong>希尔排序改进了插入排序，交换不相邻的元素以对数组局部进行排序，并最终用插入排序将局部有序数组排序。</strong></p>
<p>本质上来说，希尔排序就是把数列进行分组(不停使用插入排序)，直至从宏观上看起来有序，最后插入排序起来就容易了(无须多次移位或交换）。<br>下图为来自网上的一个图解例子：</p>
<p><img src="/img/sort/xier.png" alt=""></p>
<p>希尔排序适合于中等大小的数组且不需要额外的内存空间，因为采取了分治策略，<strong>平均时间复杂度为O(NlgN)</strong>。</p>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p>归并排序将两个有序的数组归并成一个更大的有序数组，所以在排序的时候将一个数组地跪地分成两半进行分别排序，然后将结果归并起来。<strong>归并排序由两个过程完成：有序表的合并和排序的递归实现</strong>。</p>
<p><img src="/img/sort/guibing.gif" alt=""></p>
<p>将待排序序列分为A和B两部分，如果A和B都是有序的，只需要调用有序序列的合并算法mergeArray就完成了排序，可是A和B不是有序的，再分别将A和B一分为二，直至最终的序列只有一个元素，我们认为只有一个元素的序列是有序的，合并这些序列，就得到了新的有序序列，然后返回给上层调用者，上上层调用这再合并这些序列，得到更长的有序序列，这就是递归形式的归并排序，示意图如下图所示：</p>
<p><img src="/img/sort/guibing2.png" alt=""></p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>快速排序是冒泡排序的改进版，也是最好的一种内排序。快速排序也是一种分治的排序算法，它将一个数组分成两个子数组，将两部分独立地进行排序。与归并排序不同的是：归并排序将数组分成两个数组进行排序后归并成一个有序数组；快速排序则是当两个子数组都有序之后整个数组也就有序了。<br>其主要步骤以下面一个例子为例：</p>
<ol>
<li>设置两个变量i、j，排序开始的时候：i=0，j=n-1；</li>
<li>第一个数组值作为比较值，首先保存到temp中，即temp=A[0]；</li>
<li>然后j— ,向前搜索,找到小于temp后,因为s[i]的值保存在temp中,所以直接赋值,s[i]=s[j]</li>
<li>然后i++,向后搜索,找到大于temp后,因为s[j]的值保存在第2步的s[i]中,所以直接赋值,s[j]=s[i],然后j—,避免死循环</li>
<li>重复第3、4步，直到i=j,最后将temp值返回s[i]中</li>
<li>然后采用“二分”的思想,以i为分界线,拆分成两个数组 s[0,i-1]、s[i+1,n-1]又开始排序</li>
</ol>
<p><img src="/img/sort/kuaisu.png" alt=""></p>
<p>快速排序空间复杂度为O(NlgN)。</p>
<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>堆排序基于优先队列数据结构，高效地实现删除最大元素和元素插入操作。</p>
<h3 id="堆的定义"><a href="#堆的定义" class="headerlink" title="堆的定义"></a>堆的定义</h3><p>当一棵二叉树的没个节点都大于其子节点的时候，称之为堆有序，其根节点即是最大节点。具体实现就是将二叉树的节点按照层级顺序放入数组，根节点在1，它的子节点在2,3，而子节点的子节点在4,5,6,7，以此类推。</p>
<p><img src="/img/sort/figure2.4.2.png" alt=""></p>
<p>堆的上浮和下沉操作都可以使得堆有序化。</p>
<p><img src="/img/sort/figure2.4.3.png" alt=""></p>
<p><img src="/img/sort/figure2.4.5.png" alt=""></p>
<h3 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h3><p>堆排序主要为两个阶段：构造一个堆有序的数组使得最大元素位于开头，构建堆的目的就是使以每个节点作为根节点的树都满足堆的定义，因此从堆（完全二叉树）的<strong>最下侧非叶子节点开始构建初始堆</strong>，根据堆的性质，这个节点的索引是⌊n/2⌋。从下向上，一直到堆顶节点也满足堆的定义，表示完成堆的初始化；堆的下沉排序，将堆中最大元素删除，然后放入堆缩小后数组中空出的位置。</p>
<p><img src="/img/sort/figure2.4.7.png" alt=""></p>
<p>堆排序是现有的唯一同时最优利用时间和空间的排序算法，在最坏的情况下也能保证~2NlgN次比较和恒定的额外空间。</p>
<h3 id="各排序算法比较"><a href="#各排序算法比较" class="headerlink" title="各排序算法比较"></a>各排序算法比较</h3><p><img src="/img/sort/bijiao.jpg" alt=""></p>
<p><font face="微软雅黑" color="black" size="4"><strong>参考</strong></font><br>《算法（第4版》</p>
<p><a href="https://www.cnblogs.com/lifexy/p/7597276.html" target="_blank" rel="external">https://www.cnblogs.com/lifexy/p/7597276.html</a></p>
<p><a href="https://www.cnblogs.com/lz3018/p/5742255.html" target="_blank" rel="external">https://www.cnblogs.com/lz3018/p/5742255.html</a></p>
<p><a href="https://www.cnblogs.com/beli/p/6297741.html" target="_blank" rel="external">https://www.cnblogs.com/beli/p/6297741.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排序就是按照某种逻辑顺序将一组对象重新排列的过程，这篇博客就对常见的排序算法进行总结，包括：冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之查找</title>
    <link href="http://yoursite.com/2019/02/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E6%9F%A5%E6%89%BE/"/>
    <id>http://yoursite.com/2019/02/26/数据结构与算法之查找/</id>
    <published>2019-02-26T07:42:27.000Z</published>
    <updated>2019-03-02T07:17:12.286Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>查找是针对符号表（有时也称之为字典或者索引）存储的键(key)值(value)信息，按照指定的键来查找信息，在这里就对常见的查找算法以及实现高效查找的符号表数据结构进行介绍。</p>
<a id="more"></a>
<h2 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h2><p>顺序查找是在无序队列中找出与关键字相同的数的具体位置，原理是让关键字与队列中的数逐个比较，知道找出与给定关键字相同的数为止。</p>
<h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><p>二分查找针对的是有序的符号表，如果是无序的首先就要进行排序操作。其基本思想很简单，首先将key和中间键比较，如果相等则返回其索引；如果小于中间键则在左半部分同样取中间键进行比较；大于则在右半部分取中间键进行比较。</p>
<pre><code>**在N个键的有序符号表中进行二分查找最多需要（logN+1）次比较），算法复杂度为O(logN).**
</code></pre><h2 id="分块查找"><a href="#分块查找" class="headerlink" title="分块查找"></a>分块查找</h2><p>分块查找是二分查找和顺序查找的一种改进，分块查找要求索引表是有序的，对于块内节点没有排序要求（块内无序，块间有序），因此特别适合节点动态变换的情况。这种带索引表的分块有序表查找性能取决于两步查找时间之和：第一步可以采用简单书序查找或者是二分查找查找索引表；第二部对于块之内只能顺序查找。</p>
<p>假设索引表有n个元素，每块含有s个元素，平均查找长度为(n/s+s)/2+1,时间复杂度为O(n)~O(lgn).</p>
<h2 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h2><p>二叉查找树（Binary Search Tree）每个节点都含有一个可比较大小的键，每个节点都含有两个链接（有点类似于链表的扩展），基本定义为：</p>
<pre><code>class Node{
    Key key;  // 键
    Value val;  // 值
    Node left,right;  // 指向子树的链接
    int N;  // 以该节点为根的子数节点总数
}
</code></pre><p><img src="/img/search/figure3.2.2.png" alt=""></p>
<p>具有的一条最重要的性质为<strong>：每个节点的键都大于左子树的任意节点键值，并且小于右子树任意节点键值。</strong></p>
<h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><p><img src="/img/search/figure3.2.4.png" alt=""></p>
<p>在二叉查找树中查找，可以采用递归的方法：如果树是空的，则查找未命中；如果被查找键和根节点的键相等，则查找命中；否则就递归地再适当的子树中查找，如果被查找的键较小就选择左子树，较大就选择右子树。</p>
<h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>插入操作和查找类似：如果树是空的，就返回含有该键值对的新节点；如果被查找的键小于根节点的键，就继续在左子树插入该键，否则在右子树插入该键。</p>
<p><img src="/img/search/figure3.2.6.png" alt=""></p>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>从二叉树中删除一个元素如图3.2.13，总结来说，就是在被删除元素的右子树中不断检索左子树，作为后继节点替换被删除元素，从而达到删除对应元素的目的。</p>
<p><img src="/img/search/figure3.2.13.png" alt=""></p>
<h3 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h3><p>二叉查找树实现良好的性能依赖于其中键的分布足够随机以能够消除长路径。最坏的情况下，N个节点都存在于一个查找路径上，查找增长数量级为N，平均查找2lnN=1.39lgN.</p>
<h2 id="平衡查找树"><a href="#平衡查找树" class="headerlink" title="平衡查找树"></a>平衡查找树</h2><p>平衡查找树含有N个节点，且能保持树高为~lgN,这样就能保证所有查找在lgN次内比较结束。</p>
<h3 id="2-3查找树"><a href="#2-3查找树" class="headerlink" title="2-3查找树"></a>2-3查找树</h3><p>一棵2-3查找树由以下节点组成：</p>
<ol>
<li>2-节点，含有一个键和两个链接，左链接的子树小于该节点，有链接的子树大于该节点。</li>
<li>3-节点，含有两个个键和三个链接，除左右链接外，还有一个中链接，指向键都位于两个键之间的树。</li>
</ol>
<p><img src="/img/search/figure3.3.1.png" alt=""></p>
<p><strong>因为包括3节点，所以可以保持平衡性，即任意一条路径长度都相同，在这种情况下，插入算法都是局部变换，除了相关的节点和链接之外，不必修改其他节点。在一棵大小为N的2-3树种，查找和插入操作访问的节点不超过lgN个。</strong></p>
<p><img src="/img/search/figure3.3.10.png" alt=""></p>
<p><img src="/img/search/figure3.3.11.png" alt=""></p>
<h3 id="红黑二叉查找树"><a href="#红黑二叉查找树" class="headerlink" title="红黑二叉查找树"></a>红黑二叉查找树</h3><p>红黑二叉查找树的思想是用标准的二叉查找树（完全由2-节点构成）和一些额外的信息（替换3-节点）。树种包括两种链接：红链接将两个2-节点链接构成一个3-节点，黑链接即是2-3树中的普通链接。</p>
<p><img src="/img/search/figure3.3.12.png" alt=""></p>
<p>红黑树是满足下列条件的二叉查找树“</p>
<ul>
<li>红链接均为左链接；</li>
<li>没有任何节点同时和两条红链接相接；</li>
<li>树是完美黑色平衡的，任意空链接到根节点的路径上的黑链接数量相同。</li>
</ul>
<p><img src="/img/search/figure3.3.15.png" alt=""></p>
<p><strong>某些插入操作之后可能出现右红链接或者两条连续的红链接，这时候就需要进行旋转，以保证红黑树的有序性和完美平衡性。</strong></p>
<p><img src="/img/search/figure3.3.17.png" alt=""></p>
<p>如果左右节点均为红色，需要进行颜色转换：</p>
<p><img src="/img/search/figure3.3.21.png" alt=""></p>
<p>一个完整的红黑树构造轨迹如图3.3.24：</p>
<p><img src="/img/search/figure3.3.24.png" alt=""></p>
<p><img src="/img/search/table3.3.1.png" alt=""></p>
<h2 id="散列表"><a href="#散列表" class="headerlink" title="散列表"></a>散列表</h2><p>散列表（也称之为哈希表）将键作为数组索引，从而可以快速访问任意键的值。散列表的算法主要分为两步：用<strong>散列函数（哈希函数）</strong>将键转化为数组的索引；针对多个键散列到同一个索引值的情况，<strong>处理碰撞冲突</strong>。<br><strong>直接定址</strong>与<strong>解决冲突</strong>是哈希表的两大特点，其思想很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。散列表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整散列函数函数算法即可在时间和空间上做出取舍。</p>
<h3 id="基于拉链法的散列表"><a href="#基于拉链法的散列表" class="headerlink" title="基于拉链法的散列表"></a>基于拉链法的散列表</h3><p>基于拉链法的散列表采用链表的数据结构，数组中元素为链表，将散列值相同的键值保存在一个链表中。</p>
<p><img src="/img/search/figure3.4.3.png" alt=""></p>
<h3 id="基于线性探测法的散列表"><a href="#基于线性探测法的散列表" class="headerlink" title="基于线性探测法的散列表"></a>基于线性探测法的散列表</h3><p>这种方法会用一个比较大一点的并行数组来保存键值对，一个保存键，一个保存值，解决碰撞的策略也是极其简单粗暴：当碰撞发生时（一个散列表的值已经被另一个不同的键占用），直接检查散列表的下一位置（索引值加1），不同则继续查找（到数组结尾时折回开头），直到找到该键或者遇到一个空元素，这时保存值。</p>
<p><img src="/img/search/1.png" alt=""></p>
<p>Hash（key） = key % 10（表长）；<br>89放入9的位置；18放入8的位置；49与89冲突，往后加到尾了就再回到头，0的位置为空放入；58与18冲突，往后加有89，再加有49，再加就放入1的位置；9与89冲突一直加到2的位置放入。<br>Hash（key） + 0，Hash（key）+1，……</p>
<p><font face="微软雅黑" color="black" size="4"><strong>参考</strong></font><br>《算法（第4版》，本篇博客大部分图的出于此书。</p>
<p><a href="https://blog.csdn.net/sayhello_world/article/details/77200009" target="_blank" rel="external">https://blog.csdn.net/sayhello_world/article/details/77200009</a></p>
<p><a href="https://blog.csdn.net/simplehap/article/details/70577454" target="_blank" rel="external">https://blog.csdn.net/simplehap/article/details/70577454</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;查找是针对符号表（有时也称之为字典或者索引）存储的键(key)值(value)信息，按照指定的键来查找信息，在这里就对常见的查找算法以及实现高效查找的符号表数据结构进行介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之链表</title>
    <link href="http://yoursite.com/2019/02/21/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2019/02/21/数据结构之链表/</id>
    <published>2019-02-21T07:37:15.000Z</published>
    <updated>2019-03-04T05:41:17.765Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>链表由一系列的不必在内存中连续的结构组成，每一个结构包括节点（数据域）和指向该节点之后节点的指针（指针域）构成。</p>
<pre><code>struct Node{
    int value;
    Node *next;
}
</code></pre><p>上面即是一种典型的单向链表，还有一种双向链表，除了数据域之外，还包括分别指向上一个节点和下一个节点的指针；如果让链表最后一个节点指向第一个节点，就构成了循环链表。</p>
<h2 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h2><a id="more"></a>
<h3 id="插入节点"><a href="#插入节点" class="headerlink" title="插入节点"></a>插入节点</h3><p>在p之后插入值为i的节点</p>
<pre><code>void insert(Node *p, int i){
    Node* node = new Node;
    node-&gt;value = i;
    node-&gt;next = p-&gt;next;
    p-&gt;next = node;
}
</code></pre><h3 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h3><p>删除某个节点p时，只需要用p后一个节点覆盖p当前节点：</p>
<pre><code>void delete(NOde *p){
    p-&gt;value = p-&gt;next-&gt;value;
    p-&gt;next = p-&gt;next-&gt;next;
}
</code></pre><h3 id="找出倒数第k个节点"><a href="#找出倒数第k个节点" class="headerlink" title="找出倒数第k个节点"></a>找出倒数第k个节点</h3><p>采用两个指针p1,p2，p1先走k步，然后p1和p2一起走，当p1到达节点终点时，p2即指向倒数第k个节点。</p>
<pre><code>Node* findk(Node* head, int k){
    Node *p1 = head;
    Node *p2 = head;
    for(int i=0;i&lt;k;i++){
        if(p1==NULL) return NULL;
        p1=p1-&gt;next;
    }
    while(p1-&gt;next !=NULL &amp;&amp; p2-&gt;next != NULL){
        p1=p1-&gt;next;
        p2=p2-&gt;next;
    }
    return p2;
}
</code></pre><p>同样的道理，如果要找出链表的中间节点，可以令p1和p2分别以前进2步、前进1步的速度遍历，当p1到达节点终点时，p2即指向终点节点。</p>
<h3 id="判断是否有环"><a href="#判断是否有环" class="headerlink" title="判断是否有环"></a>判断是否有环</h3><p>直观的判断是，存在环路的时候，一直遍历都不会遇到NULL节点，但是由于不知道链表的长度吗，所以采用这种判断不知道多久才能终止。转化成一个追及问题，如果有环，那么两个遍历速度不一致的指针一定会相遇，采用类似上面，有两个指针pslow，pfast,pslow每次走一步，pfast每次走两步，若是有环，pfast就能追上pslow，否则pfast会碰到NULL。</p>
<pre><code>bool isLoop(Node* head){
    if(head==NULL) return false;
    Node* pfast = head;
    Node* pslow = head;
    while(pfast!=NULL){
        if(pfast-&gt;next==NULL) return false;
        pslow = pslow-&gt;next;
        pfast = pfast-&gt;next-&gt;next;
        if(pfast==pslow) return true;
    }
    return false;
}
</code></pre><h3 id="反向遍历"><a href="#反向遍历" class="headerlink" title="反向遍历"></a>反向遍历</h3><p>可以采用先将链表存在栈中，然后先进后出即为反向遍历：</p>
<pre><code>void reverse(Node* head){
    stack&lt;Node*&gt; nodestack;
    Node* p = head;
    while(p!=NULL){
        nodestack.push(p);
        p=p-&gt;next;
    }
    while(!nodestack.empty()){
        cout&lt;&lt;nodestack.top().value&lt;&lt;endl;
        nodestack.pop();
    }
}
</code></pre><p>或者是采用递归的方法：</p>
<pre><code>void reverse(Node* head){
    if(head!=NULL){
        if(head-&gt;next!=NULL){
            reverse(head-&gt;next);
        }
        cout&lt;&lt;head-&gt;value&lt;&lt;endl;
    }
}
</code></pre><h3 id="链表反转"><a href="#链表反转" class="headerlink" title="链表反转"></a>链表反转</h3><p>采用就地反转法，有一篇博客讲的很清楚<a href="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" title="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" target="_blank" rel="external">https://www.cnblogs.com/byrhuangqiang/p/4311336.html</a>，这里就转载一下人家的方法，主要思路就是把当前链表的下一个节点PCur插入到头结点dummy的下一个节点中，就地反转。<br>dummy-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5的就地反转过程：<br>dummy-&gt;2-&gt;1-&gt;3-&gt;4-&gt;5<br>dummy-&gt;3-&gt;2-&gt;1-&gt;4-&gt;5<br>dummy-&gt;4&gt;-3-&gt;2-&gt;1-&gt;5<br>dummy-&gt;5-&gt;4-&gt;3-&gt;2-&gt;1</p>
<p>初始状态为：</p>
<p><img src="/img/LinkedList/reverseList1.png" width="300" hegiht="150" align="center"></p>
<p>pCur指向每一次需要反转的节点，将prev对应的节点连接到下一个需要反转的节点，将pCur连接的当前需要反转的节点作为插入到dummy之后，即移到了头部，调整pCur指向下一个需要反转的节点，这样循环直到下一个需要反转的节点为NULL，其动态过程如下图：</p>
<p><img src="/img/LinkedList/reverseList2.png" width="300" hegiht="450" align="center"></p>
<pre><code>Node* reverseList(Node* head){
    if(head==NULL) return head;
    Node* dummy = new Node;
    dummy-&gt;next = head;
    Node* prev = dummy-&gt;next;
    Node* pCur = prev-&gt;next;
    while(pCur!=NULL){
        prev-&gt;next = pCur-&gt;next;
        pCur-&gt;next = dummy-&gt;next;
        dummy-&gt;next = pCur;
        pCur = prev-&gt;next;
    }
    return dummy-&gt;next;
}
</code></pre><h3 id="链表相交"><a href="#链表相交" class="headerlink" title="链表相交"></a>链表相交</h3><p>两个链表相交，则相交之后必重合，从交点到末尾节点均相同。那么如果两个链表p1，p2相交的话，首先假设获得链表的长度分别为m，n，那么让较长的先走|m-n|步，再同步走的话，如果两个链表相交，那么p1，p2指针必定会相撞。</p>
<font face="微软雅黑" color="black" size="4">**参考**</font>

<p><a href="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" title="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" target="_blank" rel="external">https://www.cnblogs.com/byrhuangqiang/p/4311336.html</a><br><a href="https://www.cnblogs.com/byonecry/p/4458821.html" title="https://www.cnblogs.com/byonecry/p/4458821.html" target="_blank" rel="external">https://www.cnblogs.com/byonecry/p/4458821.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h2&gt;&lt;p&gt;链表由一系列的不必在内存中连续的结构组成，每一个结构包括节点（数据域）和指向该节点之后节点的指针（指针域）构成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct Node{
    int value;
    Node *next;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面即是一种典型的单向链表，还有一种双向链表，除了数据域之外，还包括分别指向上一个节点和下一个节点的指针；如果让链表最后一个节点指向第一个节点，就构成了循环链表。&lt;/p&gt;
&lt;h2 id=&quot;一些例子&quot;&gt;&lt;a href=&quot;#一些例子&quot; class=&quot;headerlink&quot; title=&quot;一些例子&quot;&gt;&lt;/a&gt;一些例子&lt;/h2&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>CornerNet:将目标检测转为关键点预测</title>
    <link href="http://yoursite.com/2018/09/26/CornerNet-%E5%B0%86%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%BD%AC%E4%B8%BA%E5%85%B3%E9%94%AE%E7%82%B9%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/09/26/CornerNet-将目标检测转为关键点预测/</id>
    <published>2018-09-26T06:22:54.000Z</published>
    <updated>2018-09-26T08:28:31.846Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="external">https://arxiv.org/abs/1808.01244</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="external">https://github.com/umich-vl/CornerNet
</a></p>
<p>CornerNet是ECCV2018上的一篇文章，与以往的Anchor机制的目标检测方法不同，这篇文章借鉴了人体关键点检测的思路，将目标检测转为关键点检测（Dectecting Objects as Paired Keypoints），是一种不一样的新思路，阅读了这篇文章，做个笔记。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>目前目标检测算法的主要思路还是设置大量的Anchor作为预选框，通过训练的方式获取最后的bounding box，这样就带来两个问题：</p>
<ol>
<li>大量的Anchor只有少部分和gt有比较大的overlap，从而带来正负样本巨大的不均衡的问题，减慢训练过程</li>
<li>Anchor的设置本身也是需要超参数的(形状、个数怎么设置)，在multi-scale的时候会更加明显。</li>
</ol>
<p>作者因此提出了一种新的one-stage解决方法，将目标检测转为一堆关键点检测，the top-left corner 和bottom-right corner，使用卷积神经网络来为一个类别预测heatmap获取top-left corners,同样预测另一个heatmap获取bottom-right corners,还预测embedding vector对顶点进行分组，确定是否属于同一个目标，如下图所示。</p>
<p><img src="/img/CornerNet/figure_1.png" alt=""></p>
<p>另一个创新点是提出了corner pooling，一种为了更好地获取corner的新的pooling layer。以top-left corner pooling 为例，如下图所示对每个channel，分别提取特征图的水平和垂直方向的最大值，然后求和。</p>
<p><img src="/img/CornerNet/figure_2.png" alt=""></p>
<p>论文认为corner pooling之所以有效，是因为（1）目标定位框的中心难以确定，和边界框的4条边相关，但是每个顶点只与边界框的两条边相关，所以corner 更容易提取。（2）顶点更有效提供离散的边界空间，实用O(wh)顶点可以表示O(w2h2) anchor boxes。</p>
<h2 id="CornetNet"><a href="#CornetNet" class="headerlink" title="CornetNet"></a>CornetNet</h2><p>CornerNet使用CNNs来预测两组heatmaps为每个物品类别来表示corner的位置，使用embedding vector来表示corner是否属于同一个物品，同时为了产生更加紧密的bounding box，也预测了offset。通过heatmap, embedding vector，offsets,通过后处理的方法就可以获得最后的bounding box。作者提出的算法总体框架如下图所示：</p>
<p><img src="/img/CornerNet/figure_3.png" alt=""></p>
<p>使用了hourglass network作为backbone network，紧接的两个模块分别用于预测top-left corners和bottom-right corners，每一个模块有独立的corner pooling，然后得到heatmaps, embeddings, offsets.</p>
<h3 id="Detecting-Corners"><a href="#Detecting-Corners" class="headerlink" title="Detecting Corners"></a>Detecting Corners</h3><p>论文预测了两组heatmap，每一个heatmap包含C channels(C是目标类别，不包括background),每一个channel是二进制mask，表示相应的corner位置。</p>
<p><img src="/img/CornerNet/figure_4.png" alt=""></p>
<p>对于每个顶点，只有一个groun truth，其他位置都是负样本。在训练过程中为了减少负样本数量，在每个gt顶点设定的半径r区域内都是正样本，如上图所示，半径r的确定根据所学的Iou决定。使用unnormalized 2D Gaussian来减少的半径r范围内的loss，基本思想就是构造高斯函数，中心就是gt位置，离这个中心越远衰减得越厉害，即：</p>
<p><img src="/img/CornerNet/figure_5.png" alt=""></p>
<p>pcij表示类别为c，坐标是（i,j）的预测热点图，ycij表示相应位置的ground-truth，是经过2D Gaussian的输出值，用来调整权值，论文提出变体Focal loss表示检测目标的损失函数：</p>
<p><img src="/img/CornerNet/figure_6.png" alt=""></p>
<p>由于采样过程中的量化带来的misaligment，预测offset来调整corner的位置：</p>
<p><img src="/img/CornerNet/figure_7.png" alt=""></p>
<p>训练中用smooth L1 Loss来计算：</p>
<p><img src="/img/CornerNet/figure_8.png" alt=""></p>
<h3 id="Grouping-Corner"><a href="#Grouping-Corner" class="headerlink" title="Grouping Corner"></a>Grouping Corner</h3><p>这个部分是用来决定一对corner是否来自同一object。具体的做法就是对卷积特征进行embedding（1x1的conv），得到corner的embedding vector，我们希望同属于同一个object的一对 corner的距离尽可能小，不属于的距离尽可能大！所以有两个loss，push loss和pull loss，从名字上来说，pull吧同一个目标的corner拉近，push把不同目标的推远。</p>
<p><img src="/img/CornerNet/figure_9.png" alt=""></p>
<p>etk,elk分别是属于 top-left corner和botto-right corner的embedding，ek是他们的平均值，△在论文中设置为1.</p>
<h3 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h3><p><img src="/img/CornerNet/figure_10.png" alt=""></p>
<p>为了检测某一个点是否是corner，需要从行和列分别检查，以top-left这个点为例，计算过程分为三部分：</p>
<ol>
<li>从上到下做max pooling</li>
<li>从右到左做max pooling</li>
<li>然后合并（相加）<br>如下图中，从下往上计算，每一列都能得到一个单调非递减的结果，相当于对corner的先验做了编码。对于object来说，如果要去找最上边的位置，需要从下到上检查这一列的最大值，最大值的位置是corner的可能存在的位置。</li>
</ol>
<p><img src="/img/CornerNet/figure_11.png" alt=""></p>
<p>实际计算公式为：</p>
<p><img src="/img/CornerNet/figure_12.png" alt=""></p>
<p>这样整个预测框架如下图所示：</p>
<p><img src="/img/CornerNet/figure_13.png" alt=""></p>
<h3 id="Predict-details"><a href="#Predict-details" class="headerlink" title="Predict details"></a>Predict details</h3><ol>
<li>在corner heatmap上用3x3的max poolings做NMS，选择top 100的top-left和top 100的bottom-right</li>
<li>通过预测的offset来调整位置</li>
<li>计算top-left和bottom-right的embedding 的L1 distance，筛掉距离大于0.5或者是不属于同一类别的一对corner。</li>
<li>计算top-left和bottom-right的score的平均值作为最终的score。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1808.01244&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1808.01244&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/umich-vl/CornerNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/umich-vl/CornerNet
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CornerNet是ECCV2018上的一篇文章，与以往的Anchor机制的目标检测方法不同，这篇文章借鉴了人体关键点检测的思路，将目标检测转为关键点检测（Dectecting Objects as Paired Keypoints），是一种不一样的新思路，阅读了这篇文章，做个笔记。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN论文+源码阅读笔记</title>
    <link href="http://yoursite.com/2018/09/20/Mask%20R-CNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/09/20/Mask R-CNN学习笔记/</id>
    <published>2018-09-20T07:54:57.000Z</published>
    <updated>2019-02-21T08:58:25.714Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a><br>源码链接：<a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="external">https://github.com/matterport/Mask_RCNN</a><br>之前一直在做目标检测这块，最近了解了一下实例分割，其实是目标检测更细致的任务，在图像中做到像素级的分割，包括目标检测和准确的像素分割，所以说是结合了之前目标检测的任务（classification and localization），以及语义分割（将像素点分类到特定的所属类别），首先拜读的就是17年何凯明大神的论文<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">Mask R-CNN</a>，并且阅读了keras版本的实现<a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="external">代码</a>，在此做一个学习笔记，<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Mask R-CNN是在faster R-CNN的classification branch和bounding box regression branch基础上，增加了一个segmentation mask branch，以像素到像素的方法来预测分割掩码（segmentation mask），如下图所示。</p>
<p><img src="/img/Mask_R-CNN/figure_1.png" alt=""></p>
<p>Faster R-CNN由于RoI pooling，没有办法做到输入和输出之间的像素到像素的对齐(pixel-to-pixel)，为了解决这个问题，Mask R-CNN提出了一个RoiAlign层，可以极大地提高掩码的准确率；同时解耦掩码预测和分类预测，为每个类都独立地预测二进制掩码，这样不会跨类别竞争。最终运行速度可达5 FPS左右。</p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p>Mask R-CNN和faster R-CNN类似，具有两个阶段，第一阶段是RPN，第二阶段出了预测类别和检测框的偏移外，还能够为每个RoI输出二进制掩码，这三个输出都是并行输出的。此外对每一个RoI提出了multi task：<br>L=L<em>cls</em>+L<em>box</em>+L<em>mask</em><br>Lcls和Lbox和之前faster RCNN中定义的一样，掩码分支对于每个RoI的输出维度为Km2，即K个分辨率为m×m的二进制掩码，每个类别一个，K为类别数量。对每个像素应用sigmod，Lmask为平均二进制交叉熵损失，对于真实类别为k的RoI，仅在第k个掩码上计算Lmask，其他掩码输出不计入损失。</p>
<p><img src="/img/Mask_R-CNN/figure_2.png" alt=""></p>
<h3 id="Mask-Representation"><a href="#Mask-Representation" class="headerlink" title="Mask Representation"></a>Mask Representation</h3><p>掩码用来表述目标在图片中的像素位置，在mask R-CNN中通过卷积的方法，提供了像素到像素的对应来解决。具体来说，使用FCN的方法为每一个RoI预测一个m x m的掩码，与使用FC层预测掩码的方式不同，全卷积的方法需要更少的参数，预测也会更加准确。这种像素到像素的行为需要RoI特征，为了更好地与原图进行对齐，来准确地对应原图的像素关系，这里就提出了一个很关键的模块，RoIAlign层。</p>
<h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p>这个网络层主要是为了更好地与原图像素进行对齐，对之前faster R-CNN使用RoI Pooling操作中两次量化造成的区域不匹配(mis-aligment)问题进行了改进，所以在这里就不得不提一下RoI的局限性，借鉴了<a href="http://blog.leanote.com/post/afanti/b5f4f526490b" target="_blank" rel="external">一篇博客</a>的详细介绍。</p>
<h4 id="RoI-pooling的局限性"><a href="#RoI-pooling的局限性" class="headerlink" title="RoI pooling的局限性"></a>RoI pooling的局限性</h4><p><strong>Faster R-CNN的网络框架</strong></p>
<p><img src="/img/Mask_R-CNN/figure_3.png" alt=""></p>
<p>由上图可以看到，RoI pooling位于RPN、Feature map和classification and regression之间，针对RPN输出的RoI，将其resize到统一的大小，首先将RoI映射到feature map对应的位置，将映射的区域划分为k x k个单元，对每个单元进行maxpooling，这样就得到统一大小k x k的输出了,期间就存在两次量化的过程</p>
<ol>
<li>将候选框量化为整数点坐标值</li>
<li>将量化后的边界区域分割成k x k个单元（bin),对每一个单元的边界进行量化。</li>
</ol>
<p>RoIPooling 采用的是 INTER_NEAREST（即最近邻插值） ，即在resize时，对于 缩放后坐标不能刚好为整数 的情况，采用了 粗暴的四舍五入，相当于选取离目标点最近的点。，经过这样两次量化就出现了边界不匹配的问题了(misaligment),候选框和最开始回归出来的已经有很大偏差了。</p>
<h4 id="RoIAlign-1"><a href="#RoIAlign-1" class="headerlink" title="RoIAlign"></a>RoIAlign</h4><p>Mask R-CNN将最邻近插值换成了双线性插值，这样就有了RoIAlign，主要流程为：</p>
<ol>
<li>遍历每一个候选区域，保持浮点数边界不做量化。</li>
<li>将候选区域分割成k x k个单元，每个单元的边界也不做量化。</li>
<li>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</li>
</ol>
<p>第三步的操作，论文也说的很清楚，这个固定位置指的是每一个单元(bin)中按照固定规则确定的位置，比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。下图的例子中，虚线为特征图，实线为RoI，这里假设RoI分割成2 x 2的单元，在每个单元换分为4个小方块后，每个小方块的中心作为采样点，即图中的点，但是这些点的坐标一般来说是浮点数，采用双线性插值的方法来获得像素值，这样就不存在量化过程，很好地解决了misAligment问题。</p>
<p><img src="/img/Mask_R-CNN/figure_4.png" alt=""></p>
<h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p>Mask R-CNN使用”网络-深度-特征输出层”的方式命名底下层卷积网络。我们评估了深度为50或101层的ResNet25和ResNeXt26网络。使用ResNet的Faster R-CNN从第四阶段的最终卷积层提取特征，我们称之为C4。例如，使用ResNet-50的下层网络由ResNet-50-C4表示。<br>Mask R-CNN扩展了 ResNet和FPN中提出的Faster R-CNN的上层网络。详细信息如下图所示：（上层网络架构：我们扩展了两种现有的Faster R-CNN上层网络架构，分别添加了一个掩码分支。图中数字表示分辨率和通道数，箭头表示卷积、反卷积或全连接层（可以通过上下文推断，卷积减小维度，反卷积增加维度。）所有的卷积都是3×3的，除了输出层，是1×1的。反卷积是2×2的，步进为2，,隐藏层使用ReLU。左中，“res5”表示ResNet的第五阶段，简单起见，我们修改了第一个卷积操作，使用7×7，步长为1的RoI代替14×14，步长为2的RoI25。右图中的“×4”表示堆叠的4个连续的卷积。）</p>
<p><img src="/img/Mask_R-CNN/figure_5.png" alt=""></p>
<h2 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h2><p>主要参考来自于<a href="https://blog.csdn.net/horizonheart/article/details/81188161" target="_blank" rel="external">csdn一篇博客</a>，借用他的图。</p>
<p><img src="/img/Mask_R-CNN/flow_diagram.png" alt=""></p>
<h3 id="backbone-network"><a href="#backbone-network" class="headerlink" title="backbone network"></a>backbone network</h3><p>使用resnet101作为特征提取网络，生成金字塔网络，并在每层提取特征。</p>
<pre><code># Build the shared convolutional layers.
# Bottom-up Layers
# Returns a list of the last layers of each stage, 5 in total.
# Don&#39;t create the thead (stage 5), so we pick the 4th item in the list.
if callable(config.BACKBONE):
    _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,
                                        train_bn=config.TRAIN_BN)
else:
    _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,
                                     stage5=True, train_bn=config.TRAIN_BN)
# Top-down Layers
# TODO: add assert to varify feature map sizes match what&#39;s in config
# FPN：把底层的特征和高层的特征进行融合，便于细致检测。
# 这里P5=C5，然后P4=P5+C4,P2 P3类似，最终得到rpn_feature_maps，注意这里多了个P6,其仅是由P5下采样获得。
P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&#39;fpn_c5p5&#39;)(C5)
P4 = KL.Add(name=&quot;fpn_p4add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p5upsampled&quot;)(P5),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&#39;fpn_c4p4&#39;)(C4)])
P3 = KL.Add(name=&quot;fpn_p3add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p4upsampled&quot;)(P4),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&#39;fpn_c3p3&#39;)(C3)])
P2 = KL.Add(name=&quot;fpn_p2add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p3upsampled&quot;)(P3),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&#39;fpn_c2p2&#39;)(C2)])
# Attach 3x3 conv to all P layers to get the final feature maps.
P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p2&quot;)(P2)
P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p3&quot;)(P3)
P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p4&quot;)(P4)
P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p5&quot;)(P5)
# P6 is used for the 5th anchor scale in RPN. Generated by
# subsampling from P5 with stride of 2.
P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=&quot;fpn_p6&quot;)(P5)

# Note that P6 is used in RPN, but not in the classifier heads.
rpn_feature_maps = [P2, P3, P4, P5, P6]
mrcnn_feature_maps = [P2, P3, P4, P5]
</code></pre><h3 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h3><p>在前面金字塔特征图的基础上，生成anchor</p>
<pre><code># Anchors
# 如果是训练的情况，就在金字塔特征图上生成Anchor
if mode == &quot;training&quot;:
    # 在金字塔特征图上以每个像素为中心，以配置文件的anchor大小为宽高生成anchor
    # 根据特征图相应原图缩小的比例，还原到原始的输入图片上，即得到的是anchor在原始图片上的坐标
    # 获得结果为(N,[y1,x1,y2,x2])
    anchors = self.get_anchors(config.IMAGE_SHAPE)
    # Duplicate across the batch dimension because Keras requires it
    # TODO: can this be optimized to avoid duplicating the anchors?
    anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)
    # A hack to get around Keras&#39;s bad support for constants
    anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=&quot;anchors&quot;)(input_image)
else:
    # 如果是Inference就是输入的Anchor
    anchors = input_anchors
</code></pre><h3 id="RPN-Model"><a href="#RPN-Model" class="headerlink" title="RPN Model"></a>RPN Model</h3><p>将金字塔特征图输入到RPN中，得到网络的分类（前景和背景两类）和bbox的回归值。</p>
<pre><code># RPN Model
# RPN主要实现2个功能：
# 1 &gt; box的前景色和背景色的分类
# 2 &gt; box框体的回归修正
rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,
                      len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)
# Loop through pyramid layers
layer_outputs = []  # list of lists
for p in rpn_feature_maps:
    layer_outputs.append(rpn([p]))
# Concatenate layer outputs
# Convert from list of lists of level outputs to list of lists
# of outputs across levels.
# e.g. [[a1, b1, c1], [a2, b2, c2]] =&gt; [[a1, a2], [b1, b2], [c1, c2]]
output_names = [&quot;rpn_class_logits&quot;, &quot;rpn_class&quot;, &quot;rpn_bbox&quot;]
outputs = list(zip(*layer_outputs))
outputs = [KL.Concatenate(axis=1, name=n)(list(o))
           for o, n in zip(outputs, output_names)]
# rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits(before softmax)
# rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.
# rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors.
rpn_class_logits, rpn_class, rpn_bbox = outputs
</code></pre><p>其中RPN网络也用keras做了实现，Builds a Keras model of the Region Proposal Network.It wraps the RPN graph so it can be used multiple times with shared weights.</p>
<pre><code> # Shared convolutional base of the RPN
shared = KL.Conv2D(512, (3, 3), padding=&#39;same&#39;, activation=&#39;relu&#39;,
                   strides=anchor_stride,
                   name=&#39;rpn_conv_shared&#39;)(feature_map)

# Anchor Score. [batch, height, width, anchors per location * 2].
x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding=&#39;valid&#39;,
              activation=&#39;linear&#39;, name=&#39;rpn_class_raw&#39;)(shared)

# Reshape to [batch, anchors, 2]
rpn_class_logits = KL.Lambda(
    lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)

# Softmax on last dimension of BG/FG.
rpn_probs = KL.Activation(
    &quot;softmax&quot;, name=&quot;rpn_class_xxx&quot;)(rpn_class_logits)

# Bounding box refinement. [batch, H, W, anchors per location * depth]
# where depth is [x, y, log(w), log(h)]
x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=&quot;valid&quot;,
              activation=&#39;linear&#39;, name=&#39;rpn_bbox_pred&#39;)(shared)

# Reshape to [batch, anchors, 4]
rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)

return [rpn_class_logits, rpn_probs, rpn_bbox]
</code></pre><p>代码主要实现是，在特征图上，用kernel_size为所需输出个数（如对分类，为2 * anchors_per_location），stride为1的卷积在特征图上进行卷积，得到RPN的输出。</p>
<h3 id="Generate-proposals"><a href="#Generate-proposals" class="headerlink" title="Generate proposals"></a>Generate proposals</h3><p>这部分网络主要是用来对anchor进行筛选，所谓proposal，主要步骤为：</p>
<pre><code># Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]
scores = inputs[0][:, :, 1]
# Box deltas [batch, num_rois, 4]
deltas = inputs[1]
deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4])
# Anchors
anchors = inputs[2]

# Improve performance by trimming to top anchors by score
# and doing the rest on the smaller subset.
pre_nms_limit = tf.minimum(self.config.PRE_NMS_LIMIT, tf.shape(anchors)[1])
ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True,
                 name=&quot;top_anchors&quot;).indices
scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y),
                           self.config.IMAGES_PER_GPU)
deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y),
                           self.config.IMAGES_PER_GPU)
pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x),
                            self.config.IMAGES_PER_GPU,
                            names=[&quot;pre_nms_anchors&quot;])

# Apply deltas to anchors to get refined anchors.
# [batch, N, (y1, x1, y2, x2)]
boxes = utils.batch_slice([pre_nms_anchors, deltas],
                          lambda x, y: apply_box_deltas_graph(x, y),
                          self.config.IMAGES_PER_GPU,
                          names=[&quot;refined_anchors&quot;])

# Clip to image boundaries. Since we&#39;re in normalized coordinates,
# clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]
window = np.array([0, 0, 1, 1], dtype=np.float32)
boxes = utils.batch_slice(boxes,
                          lambda x: clip_boxes_graph(x, window),
                          self.config.IMAGES_PER_GPU,
                          names=[&quot;refined_anchors_clipped&quot;])

# Filter out small boxes
# According to Xinlei Chen&#39;s paper, this reduces detection accuracy
# for small objects, so we&#39;re skipping it.

# Non-max suppression
def nms(boxes, scores):
    indices = tf.image.non_max_suppression(
        boxes, scores, self.proposal_count,
        self.nms_threshold, name=&quot;rpn_non_max_suppression&quot;)
    proposals = tf.gather(boxes, indices)
    # Pad if needed
    padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0)
    proposals = tf.pad(proposals, [(0, padding), (0, 0)])
    return proposals
proposals = utils.batch_slice([boxes, scores], nms,
                              self.config.IMAGES_PER_GPU)
return proposals
</code></pre><ol>
<li>按score得分排序，取前6000个</li>
<li>将rpn的输出应用到anchors进行修正</li>
<li>舍弃修正后边框超过归一化的0-1区间内的</li>
<li>用非极大值抑制的方法获取最后的anchor</li>
</ol>
<h3 id="Generate-detection-target"><a href="#Generate-detection-target" class="headerlink" title="Generate detection target"></a>Generate detection target</h3><p>训练的时候计算loss需要有target，这一步就是对剩下的anchor产生detection target，以便后续计算loss。主要计算的步骤为：</p>
<ol>
<li>计算proposal和gt_box之间的iou值，大于0.5则被认为是正样本，小于0.5，并且和crow box相交不大的为负样本</li>
<li>对负样本进行采样，保证正样本占有33%的比例，保证正负样本平衡</li>
<li>根据正样本和那个gt_box的iou最大来给正样本分配gt_box和gt_max,以便计算偏差</li>
</ol>
<h3 id="fpn-classifier-graph-amp-fpn-mask-graph"><a href="#fpn-classifier-graph-amp-fpn-mask-graph" class="headerlink" title="fpn classifier graph &amp;fpn mask graph"></a>fpn classifier graph &amp;fpn mask graph</h3><p>这部分为分类网络，当然还有一个并行的mask分支，分类使用的是mrcnn_feature_map，即前面的P2、P3、P4、P5。基本思路是先经过ROIAlign层（取代了RoIPooling），再经过两层卷积后连接两个全连接层分别输出class和box。fpn_mask_graph也是类似，针对mask部分，只不过不同的是，前者经过PyramidROIAlign得到的特征图是7x7大小的，二，而后者经过PyramidROIAlign得到的特征图大小是14x14.</p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>loss包括5个部分组成，分别是rpn网络的两个损失：rpn_class_loss，计算前景和背景分类损失；rpn_bbox_loss，计算rpn_box损失,以及输出的class，box和mask的损失计算。</p>
<pre><code># Losses
rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=&quot;rpn_class_loss&quot;)(
    [input_rpn_match, rpn_class_logits])
rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=&quot;rpn_bbox_loss&quot;)(
    [input_rpn_bbox, input_rpn_match, rpn_bbox])
class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=&quot;mrcnn_class_loss&quot;)(
    [target_class_ids, mrcnn_class_logits, active_class_ids])
bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=&quot;mrcnn_bbox_loss&quot;)(
    [target_bbox, target_class_ids, mrcnn_bbox])
mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=&quot;mrcnn_mask_loss&quot;)(
    [target_mask, target_class_ids, mrcnn_mask])
</code></pre><h3 id="PyramidROIAlign"><a href="#PyramidROIAlign" class="headerlink" title="PyramidROIAlign"></a>PyramidROIAlign</h3><p>PyramidROIAlign输入时金字塔特征图，所以首先需要确认来自于哪一层，作者的计算方法采用如下公式</p>
<p><img src="/img/Mask_R-CNN/PyramidROIAlign.png" alt=""></p>
<p>这里k0=4，从对应特征图中去除坐标对应区域，利用双线性插值进行pooling，这里作者依据论文中的，虽然没有采用论文中的4个点采样的方法，但是采用了论文中提到的也非常有效的1个点采样的方法，而tf.crop_and_resize这个函数crops and resizes an image and handles the bilinear interpolation，所以用这个进行了实现。</p>
<pre><code>    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords
boxes = inputs[0]

# Image meta
# Holds details about the image. See compose_image_meta()
image_meta = inputs[1]

# Feature Maps. List of feature maps from different level of the
# feature pyramid. Each is [batch, height, width, channels]
feature_maps = inputs[2:]

# Assign each ROI to a level in the pyramid based on the ROI area.
y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)
h = y2 - y1
w = x2 - x1
# Use shape of first image. Images in a batch must have the same size.
image_shape = parse_image_meta_graph(image_meta)[&#39;image_shape&#39;][0]
# Equation 1 in the Feature Pyramid Networks paper. Account for
# the fact that our coordinates are normalized here.
# e.g. a 224x224 ROI (in pixels) maps to P4
image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)
roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))
roi_level = tf.minimum(5, tf.maximum(
    2, 4 + tf.cast(tf.round(roi_level), tf.int32)))
roi_level = tf.squeeze(roi_level, 2)

# Loop through levels and apply ROI pooling to each. P2 to P5.
pooled = []
box_to_level = []
for i, level in enumerate(range(2, 6)):
    ix = tf.where(tf.equal(roi_level, level))
    level_boxes = tf.gather_nd(boxes, ix)

    # Box indices for crop_and_resize.
    box_indices = tf.cast(ix[:, 0], tf.int32)

    # Keep track of which box is mapped to which level
    box_to_level.append(ix)

    # Stop gradient propogation to ROI proposals
    level_boxes = tf.stop_gradient(level_boxes)
    box_indices = tf.stop_gradient(box_indices)

    # Crop and Resize
    # From Mask R-CNN paper: &quot;We sample four regular locations, so
    # that we can evaluate either max or average pooling. In fact,
    # interpolating only a single value at each bin center (without
    # pooling) is nearly as effective.&quot;
    #
    # Here we use the simplified approach of a single value per bin,
    # which is how it&#39;s done in tf.crop_and_resize()
    # Result: [batch * num_boxes, pool_height, pool_width, channels]
    pooled.append(tf.image.crop_and_resize(
        feature_maps[i], level_boxes, box_indices, self.pool_shape,
        method=&quot;bilinear&quot;))

# Pack pooled features into one tensor
pooled = tf.concat(pooled, axis=0)

# Pack box_to_level mapping into one array and add another
# column representing the order of pooled boxes
box_to_level = tf.concat(box_to_level, axis=0)
box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)
box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],
                         axis=1)

# Rearrange pooled features to match the order of the original boxes
# Sort box_to_level by batch then box index
# TF doesn&#39;t have a way to sort by two columns, so merge them and sort.
sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]
ix = tf.nn.top_k(sorting_tensor, k=tf.shape(
    box_to_level)[0]).indices[::-1]
ix = tf.gather(box_to_level[:, 2], ix)
pooled = tf.gather(pooled, ix)

# Re-add the batch dimension
pooled = tf.expand_dims(pooled, 0)
return pooled
</code></pre><h3 id="build-rpn-targets"><a href="#build-rpn-targets" class="headerlink" title="build_rpn_targets"></a>build_rpn_targets</h3><p>从loss可以看到，训练的时候，rpn的loss输入需要有target，代码中为rpn_match和rpn_box,计算方法主要也是根据金字塔的给定anchors和gt_box的iou，此处阈值为0.7，来确定postive和negative，并分配对应的gt_box来计算delta。</p>
<pre><code> &quot;&quot;&quot;Given the anchors and GT boxes, compute overlaps and identify positive
anchors and deltas to refine them to match their corresponding GT boxes.

anchors: [num_anchors, (y1, x1, y2, x2)]
gt_class_ids: [num_gt_boxes] Integer class IDs.
gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]

Returns:
rpn_match: [N] (int32) matches between anchors and GT boxes.
           1 = positive anchor, -1 = negative anchor, 0 = neutral
rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
&quot;&quot;&quot;
# RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
# RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

# Handle COCO crowds
# A crowd box in COCO is a bounding box around several instances. Exclude
# them from training. A crowd box is given a negative class ID.
crowd_ix = np.where(gt_class_ids &lt; 0)[0]
if crowd_ix.shape[0] &gt; 0:
    # Filter out crowds from ground truth class IDs and boxes
    non_crowd_ix = np.where(gt_class_ids &gt; 0)[0]
    crowd_boxes = gt_boxes[crowd_ix]
    gt_class_ids = gt_class_ids[non_crowd_ix]
    gt_boxes = gt_boxes[non_crowd_ix]
    # Compute overlaps with crowd boxes [anchors, crowds]
    crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
    crowd_iou_max = np.amax(crowd_overlaps, axis=1)
    no_crowd_bool = (crowd_iou_max &lt; 0.001)
else:
    # All anchors don&#39;t intersect a crowd
    no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

# Compute overlaps [num_anchors, num_gt_boxes]
overlaps = utils.compute_overlaps(anchors, gt_boxes)

# Match anchors to GT Boxes
# If an anchor overlaps a GT box with IoU &gt;= 0.7 then it&#39;s positive.
# If an anchor overlaps a GT box with IoU &lt; 0.3 then it&#39;s negative.
# Neutral anchors are those that don&#39;t match the conditions above,
# and they don&#39;t influence the loss function.
# However, don&#39;t keep any GT box unmatched (rare, but happens). Instead,
# match it to the closest anchor (even if its max IoU is &lt; 0.3).
#
# 1. Set negative anchors first. They get overwritten below if a GT box is
# matched to them. Skip boxes in crowd areas.
anchor_iou_argmax = np.argmax(overlaps, axis=1)
anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
rpn_match[(anchor_iou_max &lt; 0.3) &amp; (no_crowd_bool)] = -1
# 2. Set an anchor for each GT box (regardless of IoU value).
# TODO: If multiple anchors have the same IoU match all of them
gt_iou_argmax = np.argmax(overlaps, axis=0)
rpn_match[gt_iou_argmax] = 1
# 3. Set anchors with high overlap as positive.
rpn_match[anchor_iou_max &gt;= 0.7] = 1

# Subsample to balance positive and negative anchors
# Don&#39;t let positives be more than half the anchors
ids = np.where(rpn_match == 1)[0]
extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
if extra &gt; 0:
    # Reset the extra ones to neutral
    ids = np.random.choice(ids, extra, replace=False)
    rpn_match[ids] = 0
# Same for negative proposals
ids = np.where(rpn_match == -1)[0]
extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -
                    np.sum(rpn_match == 1))
if extra &gt; 0:
    # Rest the extra ones to neutral
    ids = np.random.choice(ids, extra, replace=False)
    rpn_match[ids] = 0

# For positive anchors, compute shift and scale needed to transform them
# to match the corresponding GT boxes.
ids = np.where(rpn_match == 1)[0]
ix = 0  # index into rpn_bbox
# TODO: use box_refinement() rather than duplicating the code here
for i, a in zip(ids, anchors[ids]):
    # Closest gt box (it might have IoU &lt; 0.7)
    gt = gt_boxes[anchor_iou_argmax[i]]

    # Convert coordinates to center plus width/height.
    # GT Box
    gt_h = gt[2] - gt[0]
    gt_w = gt[3] - gt[1]
    gt_center_y = gt[0] + 0.5 * gt_h
    gt_center_x = gt[1] + 0.5 * gt_w
    # Anchor
    a_h = a[2] - a[0]
    a_w = a[3] - a[1]
    a_center_y = a[0] + 0.5 * a_h
    a_center_x = a[1] + 0.5 * a_w

    # Compute the bbox refinement that the RPN should predict.
    rpn_bbox[ix] = [
        (gt_center_y - a_center_y) / a_h,
        (gt_center_x - a_center_x) / a_w,
        np.log(gt_h / a_h),
        np.log(gt_w / a_w),
    ]
    # Normalize
    rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
    ix += 1

return rpn_match, rpn_bbox
</code></pre><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p>可以看到,计算RPN的时候的RPN loss和分类loss其实原始的输入都需要gt_box，只不过训练好之后，分类loss是在RPN的基础上。作者代码将RPN和后面的faster RCNN部分以及增加的mask 分支一起训练，所以在训练代码中加了下面一段：</p>
<pre><code># Stop gradient propogation to ROI proposals
level_boxes = tf.stop_gradient(level_boxes)
box_indices = tf.stop_gradient(box_indices)
</code></pre><p>引用官方的解释，主要是为了不让两部分互相影响。<br>If we don’t stop the gradients, TensorFlow will try to compute the gradients all the way back to the code that generates the anchor box refinement. But we already handle learning the anchor refinement in the RPN, so we don’t want to influence that with additional gradients from stage 2. So, the sooner we stop it, the more unnecessary computation we avoid. Further, it’s not clear (at least I haven’t looked into it) how the gradients calculation back through crop_and_resize works.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1703.06870&lt;/a&gt;&lt;br&gt;源码链接：&lt;a href=&quot;https://github.com/matterport/Mask_RCNN&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/matterport/Mask_RCNN&lt;/a&gt;&lt;br&gt;之前一直在做目标检测这块，最近了解了一下实例分割，其实是目标检测更细致的任务，在图像中做到像素级的分割，包括目标检测和准确的像素分割，所以说是结合了之前目标检测的任务（classification and localization），以及语义分割（将像素点分类到特定的所属类别），首先拜读的就是17年何凯明大神的论文&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Mask R-CNN&lt;/a&gt;，并且阅读了keras版本的实现&lt;a href=&quot;https://github.com/matterport/Mask_RCNN&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;代码&lt;/a&gt;，在此做一个学习笔记，&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="实例分割" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>Synthesize image for Chinese text recognition</title>
    <link href="http://yoursite.com/2018/09/12/Synthesize%20image%20for%20Chinese%20text%20recognition/"/>
    <id>http://yoursite.com/2018/09/12/Synthesize image for Chinese text recognition/</id>
    <published>2018-09-12T07:49:30.000Z</published>
    <updated>2018-09-13T08:11:07.917Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出<a href="http://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="external">SynthText</a>方法合成自然场景下的文本图片，github上有作者给出的<a href="https://github.com/ankush-me/SynthText" target="_blank" rel="external">官方代码</a>，也有国内大神改写的<a href="https://github.com/JarveeLee/SynthText_Chinese_version" target="_blank" rel="external">中文版本代码</a>。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。</p>
<a id="more"></a>
<h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><p>借鉴了SynthText的方法，而且包括语料库、图像背景图、字体、以及色彩模型文件，都是来源于@JarveeLee的中文版代码中的文件。</p>
<ol>
<li>读取语料库，此处来源为一些童话故事txt，</li>
<li>随机取一段字符串，满足所需长度，再随机选择字体、字号大小</li>
<li>在提供的背景图中，随机取一张图，计算裁剪图的Lab值标准差（标准差越小图像色彩分布就不会太过丰富、太过花哨），小于设定的阈值则再根据字体字号计算出的文本尺寸，在原图上进行随机裁剪，可以以一定概率使文本在最终图片中有一定偏移；可以以一定概率随机产生竖直文本。</li>
<li>通过聚类的方法，分析裁剪后图的色彩分布，在色彩模型提供的色彩库中选择与当前裁剪图像色彩偏差大的作为文本颜色，这样最终构成合成图片</li>
</ol>
<h2 id="构建方法"><a href="#构建方法" class="headerlink" title="构建方法"></a>构建方法</h2><p>主要实现代码只有一个文件，其他都是合成需要的文件，合成命令：</p>
<pre><code>python gen_dataset.py
</code></pre><p>newsgroup:文本来源的语料<br>models/colors_new.cp:从III-5K数据集学习到的色彩模型<br>fonts：包含合成时所需字体<br>所需图片bg_img来源于VGG组合成synth_80k时所用的图片集</p>
<ul>
<li>bg_img.tar.gz [8.9G]：压缩的图像文件（需要使用使用imnames.cp中的过滤），链接<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz" title="http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz</a></li>
<li>imnames.cp[180K]：已过滤文件的名称，即，这些文件不包含文本,链接：<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" title="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp</a></li>
</ul>
<h2 id="一些实现结果样例"><a href="#一些实现结果样例" class="headerlink" title="一些实现结果样例"></a>一些实现结果样例</h2><p><img src="/img/img_1.jpg" alt=""></p>
<p><img src="/img/img_2.jpg" alt=""></p>
<p><img src="/img/img_3.jpg" alt=""></p>
<p><img src="/img/img_4.jpg" alt=""></p>
<p>详细实现代码可参见个人<a href="https://github.com/lkj1114889770/Synth_Chinese_OCR_dataset" target="_blank" rel="external">github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/scenetext/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SynthText&lt;/a&gt;方法合成自然场景下的文本图片，github上有作者给出的&lt;a href=&quot;https://github.com/ankush-me/SynthText&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方代码&lt;/a&gt;，也有国内大神改写的&lt;a href=&quot;https://github.com/JarveeLee/SynthText_Chinese_version&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;中文版本代码&lt;/a&gt;。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow输入数据处理框架</title>
    <link href="http://yoursite.com/2018/07/25/Tensorflow%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <id>http://yoursite.com/2018/07/25/Tensorflow输入数据处理框架/</id>
    <published>2018-07-25T13:55:36.000Z</published>
    <updated>2018-07-25T14:45:55.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。</p>
<a id="more"></a>
<h2 id="TFrecords格式介绍"><a href="#TFrecords格式介绍" class="headerlink" title="TFrecords格式介绍"></a>TFrecords格式介绍</h2><p>TFrecords是一种二进制文件，通过tf.train.Example Protocol Buffer的格式存储数据，以下的代码给出了tf.train.Example的定义。</p>
<pre><code>message Example {
    Features features = 1;
};
message Features {
    map&lt;string, Feature&gt; feature = 1;
};
message Feature {
    oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
}
};
</code></pre><p>tf.train.Example包含了一个从属性名称到取值的字典，其中属性名称为一个字符串，属性取值可以是字符串(BytesList)，实数列表(FloatList)或者整数列表(Int64List），比如将解码前的图像存为一个字符串，将lable存为整数列表，或者将bounding box存为实数列表。</p>
<h2 id="COCO数据集的TFrecords文件制作"><a href="#COCO数据集的TFrecords文件制作" class="headerlink" title="COCO数据集的TFrecords文件制作"></a>COCO数据集的TFrecords文件制作</h2><p>COCO数据集是微软做的一个比较大的数据集，可以用来做图像的recognition、segmentation、captioning，我用来做物体检测识别。官方也提供了API操作数据集（<a href="https://github.com/cocodataset/cocoapi" title="https://github.com/cocodataset/cocoapi" target="_blank" rel="external">https://github.com/cocodataset/cocoapi</a>）。根据链接介绍下载安装python的API后，就可以开始Tfrecords的文件制作了。</p>
<pre><code>from pycocotools.coco import COCO
import tensorflow as tf
import numpy as np
from PIL import Image
from time import time
import os

dataDir=&#39;/home/zju/lkj/data/COCO Dataset&#39;
dataType=&#39;train2017&#39;
annFile=&#39;{}/annotations/instances_{}.json&#39;.format(dataDir,dataType)

classes = [&#39;backpack&#39;, &#39;umbrella&#39;, &#39;handbag&#39;, &#39;tie&#39;, &#39;suitcase&#39;, &#39;bottle&#39;, &#39;wine glass&#39;, &#39;cup&#39;, &#39;fork&#39;, &#39;knife&#39;,
            &#39;spoon&#39;, &#39;bowl&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;sandwich&#39;, &#39;orange&#39;, &#39;broccoli&#39;, &#39;carrot&#39;, &#39;hot dog&#39;,
            &#39;donut&#39;, &#39;cake&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;potted plant&#39;, &#39;bed&#39;, &#39;dining table&#39;, &#39;toilet&#39;, &#39;tv&#39;, &#39;laptop&#39;,
            &#39;mouse&#39;, &#39;remote&#39;, &#39;keyboard&#39;, &#39;cell phone&#39;, &#39;microwave&#39;, &#39;oven&#39;, &#39;toaster&#39;, &#39;sink&#39;, &#39;refrigerator&#39;,
            &#39;book&#39;, &#39;clock&#39;, &#39;vase&#39;, &#39;scissors&#39;, &#39;teddy bear&#39;, &#39;hair drier&#39;, &#39;toothbrush&#39;]

# initialize COCO api for instance annotations
coco = COCO(annFile)
classesId = coco.getCatIds(classes)
imgIds = coco.getImgIds()
img_filters=[]
for imgId in imgIds:
    Anns = coco.loadAnns(coco.getAnnIds(imgIds=imgId))
    annIds = list(map(lambda x:x[&#39;category_id&#39;],Anns))
    for annId in annIds:
        if annId in classesId:
            img_filters.append(imgId)
img_filters = set(img_filters)


# 归一化
# size: 图片大小
# box：[x,y,w,h]
# return 归一化结果
def convert(size,box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = box[0]+box[2]/2.0
    y = box[1]+box[3]/2.0
    x = x*dw
    w = box[2]*dw
    y = y*dh
    h = box[3]*dh
    return [x,y,w,h]


def convert_img(img_id):
    img_id_str = str(img_id).zfill(12)
    img_path = &#39;{}/{}/{}.jpg&#39;.format(dataDir, dataType, img_id_str)
    image = Image.open(img_path)
    resized_image = image.resize((416, 416), Image.BICUBIC)
    image_data = np.array(resized_image, dtype=&#39;float32&#39;) / 255
    if image_data.size != 519168: # 不为3通道
        return False
    img_raw = image_data.tobytes()
    return img_raw

def convert_annotation(image_id):
    img_info = coco.loadImgs(image_id)[0]  # 读入的是照片的详细信息，而非图像信息, 返回的是list，只有1个id输入时，取0
    w = int(img_info[&#39;width&#39;])
    h = int(img_info[&#39;height&#39;])
    bboxes = []
    Anns = coco.loadAnns(ids=coco.getAnnIds(imgIds=image_id))
    i = 0
    for Ann in Anns:
        if i&gt;29:
            break
        iscrowd = Ann[&#39;iscrowd&#39;]
        if iscrowd == 1:
            continue
        if Ann[&#39;category_id&#39;] not in classesId:
            continue
        cls_id = classesId.index(Ann[&#39;category_id&#39;])  # 取新的编号
        bbox = Ann[&#39;bbox&#39;]
        bb = convert((w, h), bbox) + [cls_id]
        bboxes.extend(bb)
        i = i + 1

    if len(bboxes) &lt; 30*5:
        bboxes = bboxes + [0, 0, 0, 0, 0]*(30-int(len(bboxes)/5))
    return np.array(bboxes, dtype=np.float32).flatten().tolist()

filename = os.path.join(&#39;train2017&#39;+&#39;.tfrecords&#39;)
writer = tf.python_io.TFRecordWriter(filename)
i=0
start = time()
for imgId in img_filters:
    xywhc = convert_annotation(imgId)
    img_raw = convert_img(imgId)
    if img_raw:
        example = tf.train.Example(features=tf.train.Features(feature={
            &#39;xywhc&#39;:
                    tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
            &#39;img&#39;:
                    tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
            }))
        writer.write(example.SerializeToString())
        # 显示制作进度，剩余时间
        if i%100==99:
            t = time()-start
            print(i,&#39;t={:0.4f}s/100 step&#39;.format(t),&#39;  left time={:0.4f}s&#39;.format((len(img_filters)-i)*t/100))
            start = time()
        i = i+1
print(&#39;Done!&#39;)
writer.close()
</code></pre><p>下面分段对代码进行介绍，这个数据制作是应用于物品检查与分割，并且只有部分物品，所以在程序开头有classes列举（总共45种，完整的COCO数据集包含91种）。COCO数据集中混有灰度图，所以在reshape的时候会一直报错，刚开始还一直想不清楚为什么，后来遍历原始数据集才发现有灰度图的存在，所以reshape成416*416*3会报错,所以程序有一个判断是否为3通道：</p>
<pre><code>image = Image.open(img_path)
resized_image = image.resize((416, 416), Image.BICUBIC)
image_data = np.array(resized_image, dtype=&#39;float32&#39;) / 255
if image_data.size != 519168: # 不为3通道
    return False
</code></pre><p>图像读取后转换成字符串(BytesList):</p>
<pre><code>img_raw = image_data.tobytes()
</code></pre><p>bounding box转换成实数列表(FloatList):</p>
<pre><code>return np.array(bboxes, dtype=np.float32).flatten().tolist()
</code></pre><p>基于此，核心的构建部分为：</p>
<pre><code>example = tf.train.Example(features=tf.train.Features(feature={
    &#39;xywhc&#39;:
            tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
    &#39;img&#39;:
            tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
    }))
writer.write(example.SerializeToString())
</code></pre><h2 id="TFrecords文件读取解析"><a href="#TFrecords文件读取解析" class="headerlink" title="TFrecords文件读取解析"></a>TFrecords文件读取解析</h2><p>对应构建时候的数据格式，进行解析，可以加一些程序对于读取后的图像文件的一些进一步处理，比如图像增强</p>
<pre><code>def parser(example):
    features = {
                &#39;xywhc&#39;: tf.FixedLenFeature([150], tf.float32),
                &#39;img&#39;: tf.FixedLenFeature((), tf.string)}
    feats = tf.parse_single_example(example, features)
    coord = feats[&#39;xywhc&#39;]
    coord = tf.reshape(coord, [30, 5])

    img = tf.decode_raw(feats[&#39;img&#39;], tf.float32)
    img = tf.reshape(img, [416, 416, 3])
    img = tf.image.resize_images(img, [cfg.train.image_resized, cfg.train.image_resized])
    rnd = tf.less(tf.random_uniform(shape=[], minval=0, maxval=2), 1)
    # 添加对于读取后的图像文件的一些进一步处理，图像增强
    def flip_img_coord(_img, _coord):
        zeros = tf.constant([[0, 0, 0, 0, 0]]*30, tf.float32)
        img_flipped = tf.image.flip_left_right(_img)
        idx_invalid = tf.reduce_all(tf.equal(coord, 0), axis=-1)
        coord_temp = tf.concat([tf.minimum(tf.maximum(1 - _coord[:, :1], 0), 1),
                               _coord[:, 1:]], axis=-1)
        coord_flipped = tf.where(idx_invalid, zeros, coord_temp)
        return img_flipped, coord_flipped

    img, coord = tf.cond(rnd, lambda: (tf.identity(img), tf.identity(coord)), lambda: flip_img_coord(img, coord))

    img = tf.image.random_hue(img, max_delta=0.1)
    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)
    img = tf.image.random_brightness(img, max_delta=0.1)
    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)
    img = tf.minimum(img, 1.0)
    img = tf.maximum(img, 0.0)
    return img, coord
</code></pre><p>然后构建一个data_pipeline来作为训练数据的输入框架：</p>
<pre><code>def data_pipeline(file_tfrecords, batch_size):
    dt = tf.data.TFRecordDataset(file_tfrecords)
    dt = dt.map(parser, num_parallel_calls=4)
    dt = dt.prefetch(batch_size)
    dt = dt.shuffle(buffer_size=20*batch_size)
    dt = dt.repeat()
    dt = dt.batch(batch_size)
    iterator = dt.make_one_shot_iterator()
    imgs, true_boxes = iterator.get_next()

    return imgs, true_boxes
</code></pre><p>测试一下整个数据输入模块：</p>
<pre><code>file_path = &#39;train2007.tfrecords&#39;
imgs, true_boxes = data_pipeline(file_path, cfg.batch_size)
sess = tf.Session()
imgs_, true_boxes_ = sess.run([imgs, true_boxes])
print(imgs_.shape, true_boxes_.shape)
for imgs_i, boxes_ in zip(imgs_, true_boxes_):
    valid = (np.sum(boxes_, axis=-1) &gt; 0).tolist()
    print([cfg.names[int(idx)] for idx in boxes_[:, 4][valid].tolist()])
    plt.figure()
    plt.imshow(imgs_i)
plt.show()
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>表扬一下pycharm</title>
    <link href="http://yoursite.com/2018/07/07/%E8%A1%A8%E6%89%AC%E4%B8%80%E4%B8%8Bpycharm/"/>
    <id>http://yoursite.com/2018/07/07/表扬一下pycharm/</id>
    <published>2018-07-07T15:43:06.000Z</published>
    <updated>2019-03-19T08:00:08.870Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。</p>
<a id="more"></a>
<p><img src="img/pycharm/figure1.png" alt=""></p>
<p>炼丹过程中参数修改频繁，还忘了备份已经效果比较好的参数，结果改的调不回去了，本来还有个理想的结果，现在越来越差，真实欲哭无泪，直到发现了上面的那个功能，pycharm是真的优秀，按这样点开，就能发现一天的修改过程，如下图</p>
<p><img src="img/pycharm/figure2.png" alt=""></p>
<p>可以点开看到修改历史，和现有版本进行对比，还可以导出修改历史。嗯，真的是良心IDE，特此表扬，以资鼓励。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="杂" scheme="http://yoursite.com/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>目标检测网络之YOLO学习笔记</title>
    <link href="http://yoursite.com/2018/06/15/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C%E4%B9%8BYOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/06/15/目标检测网络之YOLO学习笔记/</id>
    <published>2018-06-15T06:59:34.000Z</published>
    <updated>2019-09-02T13:21:16.217Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。<br><a id="more"></a></p>
<p>YOLO项目主页<a href="https://pjreddie.com/yolo/" target="_blank" rel="external">地址</a><br>YOLO1 <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">论文</a><br>YOLO2 <a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">论文</a><br>YOLO3 <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">论文</a></p>
<h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><p>YOLO使用来自整张图片的feature map来预测bounding box和class，因此可以保持较高的精度。YOLO将整张图片分成S×S的网格，如果一个目标的中心落入到网格单元中，那么这个网格单元负责这个目标的检测。</p>
<div align="center">
    <img src="/img/yolo/yolo1.1.png" width="300" height="300">
</div>
每个网格单元预测B个bounding box和confidence score，confidence score反应了box包含目标的可信度，论文中将可confidence score定义为：
<div align="center">
    <img src="/img/yolo/yolo1.2.png" height="30">
</div>
，因此，如果没有目标存在confidence score为0，否则应该为IOU(intersection over union)，即真实框和预测框的交集部分。所以每个bounding box的预测值包括(x,y,w,h.confidence score). (x,y)表示预测的box中心相对于网格单元的的位置，(w,h)是用整个图片大小进行归一化的宽度和高度，另外，针对C个类别，每个类别需要预测一个条件概率，即：
<div align="center">
    <img src="/img/yolo/yolo1.3.png" height="30">
</div>
最终得到box中包含某个特定物品的概率为：
<div align="center">
    <img src="/img/yolo/yolo1.4.png" height="40">
</div>
整个过程如下图所示。
<div align="center">
    <img src="/img/yolo/yolo1.5.png" width="700" height="400">
</div>

<p>总结来说，YOLO网络将检测问题转换成regression，首先将整张图片转换成S×S的网格，并且每个网格单元预测B个边界框，这些边界框的(x,y,w,h,confidence score)以及C个类别概率,这些预测被编码为S×S×(B*5+C)的张量。</p>
<h3 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h3><p>YOLO1的网络结构设计借鉴了GoogleNet模型，包含了24个卷积层和2个全连接层，YOLO未使用inception module，而是使用1x1卷积层和）3x3卷积层简单替代，交替出现的1x1卷积层实现了跨通道信息融合以及通道数目降低。</p>
<div align="center">
    <img src="/img/yolo/yolo1.6.png">
</div>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ol>
<li>使用 ImageNet 1000 类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。</li>
<li>用上面得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，加入后面的4层卷积层以及2层全连接层进行detection的训练，detection通常需要有细密纹理的视觉信息,所以为提高图像精度，在训练检测模型时，将输入图像分辨率从224 × 224 resize到448x448。</li>
<li>最后一层预测类概率和边界框坐标。我们通过图像宽度和高度来规范(w,h)，使它们落在0和1之间。我们将边界框(x,y)坐标参数化为特定网格单元位置的偏移量，所以它们边界也在0和1之间。</li>
</ol>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>YOLO1的误差计算对于分类误差和定位误差用了不同的权重，对包含与不包含物品的box的误差权重也进行了区分。具体来说，论文中增加了边界框坐标预测损失，并减少了不包含目标边界框的置信度预测损失，使用两个参数λcoord和λnoobj来完成这个工作，论文中设置了λcoord=5和λnoobj=0.5。<br>另一个问题是平方和误差权重在大框和小框中进行了区分。相同的误差下，小框误差的重要性肯定更好，论文中用了一个很巧妙的方法，<strong>直接预测边界框宽度和高度的平方根，而不是宽度和高度</strong>。根据y=x^1/2的函数就可以知道，函数斜率是随着x的增大而减小的，这样就可以提高小框的误差权重，真的巧妙。<br>YOLO每个网格单元预测多个box。在训练时，每个目标我们只需要一个box来负责，选定的原则是与真实框具有最大的IOU。</p>
<div align="center">
    <img src="/img/yolo/yolo1.7.png">
</div>

<h3 id="Shortcoming"><a href="#Shortcoming" class="headerlink" title="Shortcoming"></a>Shortcoming</h3><p>YOLO对边界框预测强加空间约束，因为每个网格单元只预测两个盒子，只能有一个类别。这个空间约束限制了我们的模型可以预测的邻近目标的数量，因此在小物品的检测上比较局限。</p>
<h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p>为提高物体定位精准性和召回率，YOLO2对网络结构的设计进行了改进，输出层使用卷积层替代YOLO的全连接层，联合使用coco物体检测标注数据和imagenet物体分类标注数据训练物体检测模型。相比YOLO，YOLO9000在识别种类、精度、速度、和定位准确性等方面都有大大提升。</p>
<h3 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>YOLO2取消了dropout，在所有的卷积层中加入Batch Normalization。</p>
<h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>YOLO2将ImageNet以448×448 的分辨率微调最初的分类网络，迭代10 epochs。</p>
<h4 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h4><p>借鉴faster R-CNN的思想，引入anchor box，取消全连接层来进行预测，改用卷积层作为预测层对anchor box的offset和confidence进行预测。去除了一个池化层，使得输出特征具有更高的分辨率，将图片输入尺寸resize为416而非448，使得特征图大小为奇数，所以有一个中心单元格。目标，特别是大目标，倾向于占据图像的中心，所以在中心有一个单一的位置可以很好的预测这些目标，而不是四个位置都在中心附近。YOLO的卷积层将图像下采样32倍，所以通过使用输入图像416，我们得到13×13的输出特征图。同时，使用anchor box进行预测的时候，解耦空间位置预测与类别预测，对每个anchor box都预测object和class，仍然沿用YOLO1，目标检测仍然是预测proposed box和ground truth的IOU，类别预测（class predictions）仍然是存在object下的条件概率。</p>
<h4 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h4><p>YOLO2不再采用手动挑选的box尺寸，而是对训练集的box尺寸进行k-means聚类，因为聚类的目的是想要更好的IOU，所以聚类的距离使用下列公式：</p>
<div align="center">
    <img src="/img/yolo/yolo2.1.png" height="40">
</div>
对不同的k值采用k-means聚类算法，即对数据集的ground truth聚类，在VOC和COCO数据集上的bounding box得到的结果如下图：
<div align="center">
    <img src="/img/yolo/yolo2.2.png">
</div>

<p>根据上图，k=5的时候，模型的复杂度和IOU能够得到一个不错的trade off。</p>
<h4 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h4><p>对于位置坐标，YOLO2没有采用R-CNN的预测偏移，而是仍然类似于YOLO1中的，他预测相对于网格单元的位置坐标，将ground truth也限制在0-1之间，使用logistic activation 来实现。网络为每个边界框预测tx，ty，th，tw和to这5个坐标。如果网格单元从图像的左上角偏移（Cx，Cy），给定的anchor的宽度，高度分别为Pw，Ph那么预测结果为：</p>
<div align="center">
    <img src="/img/yolo/yolo2.3.png">
</div>
<div align="center">
    <img src="/img/yolo/yolo2.4.png">
</div>

<h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>在13×13特征图上检测可以很容易检测到大目标，从更小粒度的特征图中可以更好地检测小物体，YOLO2添加一个passthrough layer从前一层26×26的特征图进行融合。传递层通过将相邻特征堆叠到不同的通道而不是堆叠到空间位置，将较高分辨率特征与低分辨率特征相连，类似于ResNet中的标识映射。这将26×26×512特征映射转换为13×13×2048特征映射，其可以与原始特征连接。</p>
<h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>添加anchor box后，YOLO2将分辨率更改为416×416。然而，由于模型只使用卷积层和池化层，它可以在运行中调整大小。为了使YOLOv2能够在不同大小的图像上运行，相比于固定输入图像大小，YOLO2每隔几次迭代更改网络。每迭代10个batch网络随机选择一个新的图像尺寸大小。因为模型以32的因子下采样，YOLO2从以下32的倍数中抽取：{320,352，…，608}。因此，最小的选项是320×320，最大的是608×608.调整网络的大小，并继续训练。<br>这种训练方法迫使网络学习在各种输入维度上很好地预测。这意味着相同的网络可以预测不同分辨率的检测。网络在更小的尺寸下运行更快，因此YOLO2在速度和精度之间提供了一个简单的折衷。</p>
<h3 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h3><h4 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h4><p>YOLO2大多数3×3的滤波器，并在每个池化步骤后将通道数量加倍，使用全局平均池化进行预测，使用1×1滤波器以压缩3×3卷积之间的特征，最终模型，称为Darknet-19，有19卷积层和5个最大池化层，详见下图。</p>
<div align="center">
    <img src="/img/yolo/yolo2.5.png">
</div>

<h4 id="Training-for-classification"><a href="#Training-for-classification" class="headerlink" title="Training for classification"></a>Training for classification</h4><p>使用Darknet19在标准ImageNet 1000类分类数据集上训练，在训练期间，使用数据增强技巧。</p>
<h4 id="Training-for-detection"><a href="#Training-for-detection" class="headerlink" title="Training for detection"></a>Training for detection</h4><p>为了训练检测器，修改上面的网络，移除最后的卷积层，添加3个3×3卷积层，最后增加1×1卷积层，其输出为我们需要的检测维度，如对于VOC数据集，预测5个box，每个具有5个坐标，每个box20个类，因此125个过滤器。还添加了从最后的3×3×512层到第二到最后的卷积层的传递层passthrough layer，使得模型可以使用细粒度特征。</p>
<h3 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h3><p>构建了一种分层分类模型（WordTree），提出了一种关于分类和检测数据的联合训练机制。</p>
<div align="center">
    <img src="/img/yolo/yolo2.6.png">
</div>
<div align="center">
    <img src="/img/yolo/yolo2.7.png">
</div>

<p>ImageNet数据量更大，用于训练分类，COCO和VOC用于训练检测，ImageN对应分类有9000多种，COCO只有80种对应目标检测，通过wordTree来combine，来自分类的图片只计算分类的loss，来自检测集的图片计算完整的loss。</p>
<h2 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h2><p>YOLO3 对于YOLO2有了一些改进，总的来说有几点：加深了网络，用了上采样，残差网络，多尺度预测，下面详细说明。</p>
<h3 id="Bounding-Box-Prediction"><a href="#Bounding-Box-Prediction" class="headerlink" title="Bounding Box Prediction"></a>Bounding Box Prediction</h3><p>坐标预测仍然沿用YOLO2的，yolov3对每个bounding box预测四个坐标值(tx, ty, tw, th)，对于预测的cell根据图像左上角的偏移(cx, cy)，以及之前得到bounding box的宽和高pw, ph可以对bounding box按如下的方式进行预测：</p>
<div align="center">
    <img src="/img/yolo/yolo3.1.png">
</div>

<p>训练的时候，loss的计算采用sum of squared error loss（平方和距离误差损失），yolov3对每个bounding box通过逻辑回归预测一个物体的得分，如果预测的这个bounding box与真实的边框值大部分重合且比其他所有预测的要好，那么这个值就为1.如果overlap没有达到一个阈值（yolov3中这里设定的阈值是0.5），那么这个预测的bounding box将会被忽略。YOLO3论文中使用的阈值是0.5.每个object只会分配一个bounding box，所以对应没有分配有ground truth object的box，其坐标损失和预测损失不需要计入，只需考虑objectness loss。If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.</p>
<h3 id="Class-Prediction"><a href="#Class-Prediction" class="headerlink" title="Class Prediction"></a>Class Prediction</h3><p>每个框预测分类，bounding box使用多标签分类（multi-label classification）。论文中说没有使用softmax分类，只是使用了简单的逻辑回归进行分类，采用的二值交叉熵损失（binary cross-entropy loss）。<br>Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.<br>This formulation helps when we move to more complex domains like the Open Images Dataset. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.</p>
<h3 id="Predictions-Across-Scales"><a href="#Predictions-Across-Scales" class="headerlink" title="Predictions Across Scales"></a>Predictions Across Scales</h3><p>YOLO3在三种不同尺度来预测box，应用一个类似于特征金字塔网络（feature pyramid network）上提取特征，如下图：</p>
<div align="center">
    <img src="/img/yolo/yolo3.2.png">
</div>

<p>对于第一个scale的预测，即base feature extractor，最后预测得到一个3-d tensor，包含bounding box,objectness,class prediction.比如在COCO数据集中有80类物品，每一个scale预测3个box，所以tensor得到为（N×N×[3*(4+1+80)]）。<br>next scale，从上一步2 layer previous的feature map中进行上采样，然后从特征提取网络中的取earlier feature 与上采样后的进行合并，得到更多信息的语义，以及从earlier feature map可以得到更细粒度的特征。最后的scale采用前述类似的方法进行。可能实际代码更能体现这个过程，如下：<br>三种跨尺度预测</p>
<pre><code>predict boxes at 3 different scales
&#39;&#39;&#39;
def build(self, feat_ex, res18, res10):
    self.conv52 = self.conv_layer(feat_ex, 1, 1, 1024, 512, True, &#39;conv_head_52&#39;)          # 13x512
    self.conv53 = self.conv_layer(self.conv52, 3, 1, 512, 1024, True, &#39;conv_head_53&#39;)   # 13x1024
    self.conv54 = self.conv_layer(self.conv53, 1, 1, 1024, 512, True, &#39;conv_head_54&#39;)   # 13x512
    self.conv55 = self.conv_layer(self.conv54, 3, 1, 512, 1024, True, &#39;conv_head_55&#39;)   # 13x1024
    self.conv56 = self.conv_layer(self.conv55, 1, 1, 1024, 512, True, &#39;conv_head_56&#39;)   # 13x512
    self.conv57 = self.conv_layer(self.conv56, 3, 1, 512, 1024, True, &#39;conv_head_57&#39;)   # 13x1024
    self.conv58 = self.conv_layer(self.conv57, 1, 1, 1024, 75, False, &#39;conv_head_58&#39;)   # 13x75
    # follow yolo layer mask = 6,7,8
    self.conv59 = self.conv_layer(self.conv56, 1, 1, 512, 256, True, &#39;conv_head_59&#39;)    # 13x256
    size = tf.shape(self.conv59)[1]
    self.upsample0 = tf.image.resize_nearest_neighbor(self.conv59, [2*size, 2*size],    # 上采样
                                                      name=&#39;upsample_0&#39;)                # 26x256
    self.route0 = tf.concat([self.upsample0, res18], axis=-1, name=&#39;route_0&#39;)           # 26x768
    self.conv60 = self.conv_layer(self.route0, 1, 1, 768, 256, True, &#39;conv_head_60&#39;)    # 26x256
    self.conv61 = self.conv_layer(self.conv60, 3, 1, 256, 512, True, &#39;conv_head_61&#39;)    # 26x512
    self.conv62 = self.conv_layer(self.conv61, 1, 1, 512, 256, True, &#39;conv_head_62&#39;)    # 26x256
    self.conv63 = self.conv_layer(self.conv62, 3, 1, 256, 512, True, &#39;conv_head_63&#39;)    # 26x512
    self.conv64 = self.conv_layer(self.conv63, 1, 1, 512, 256, True, &#39;conv_head_64&#39;)    # 26x256
    self.conv65 = self.conv_layer(self.conv64, 3, 1, 256, 512, True, &#39;conv_head_65&#39;)    # 26x512
    self.conv66 = self.conv_layer(self.conv65, 1, 1, 512, 75, False, &#39;conv_head_66&#39;)    # 26x75
    # follow yolo layer mask = 3,4,5
    self.conv67 = self.conv_layer(self.conv64, 1, 1, 256, 128, True, &#39;conv_head_67&#39;)    # 26x128
    size = tf.shape(self.conv67)[1]
    self.upsample1 = tf.image.resize_nearest_neighbor(self.conv67, [2 * size, 2 * size],
                                                      name=&#39;upsample_1&#39;)                # 52x128
    self.route1 = tf.concat([self.upsample1, res10], axis=-1, name=&#39;route_1&#39;)           # 52x384
    self.conv68 = self.conv_layer(self.route1, 1, 1, 384, 128, True, &#39;conv_head_68&#39;)    # 52x128
    self.conv69 = self.conv_layer(self.conv68, 3, 1, 128, 256, True, &#39;conv_head_69&#39;)    # 52x256
    self.conv70 = self.conv_layer(self.conv69, 1, 1, 256, 128, True, &#39;conv_head_70&#39;)    # 52x128
    self.conv71 = self.conv_layer(self.conv70, 3, 1, 128, 256, True, &#39;conv_head_71&#39;)    # 52x256
    self.conv72 = self.conv_layer(self.conv71, 1, 1, 256, 128, True, &#39;conv_head_72&#39;)    # 52x128
    self.conv73 = self.conv_layer(self.conv72, 3, 1, 128, 256, True, &#39;conv_head_73&#39;)    # 52x256
    self.conv74 = self.conv_layer(self.conv73, 1, 1, 256, 75, False, &#39;conv_head_74&#39;)    # 52x75
    # follow yolo layer mask = 0,1,2

    return self.conv74, self.conv66, self.conv58
</code></pre><p>上面是最后的预测部分，需要输入的三个特征从Darknet-53网络中得到的，输出地方做了注释，Darknet-53网络结构如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, img, istraining, decay_bn=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.phase_train = istraining</span><br><span class="line">        self.decay_bn = decay_bn</span><br><span class="line">        self.conv0 = self.conv_layer(bottom=img, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">3</span>,   <span class="comment"># 416x3</span></span><br><span class="line">                                     out_channels=<span class="number">32</span>, name=<span class="string">'conv_0'</span>)                <span class="comment"># 416x32</span></span><br><span class="line">        self.conv1 = self.conv_layer(bottom=self.conv0, size=<span class="number">3</span>, stride=<span class="number">2</span>, in_channels=<span class="number">32</span>,</span><br><span class="line">                                     out_channels=<span class="number">64</span>, name=<span class="string">'conv_1'</span>)                <span class="comment"># 208x64</span></span><br><span class="line">        self.conv2 = self.conv_layer(bottom=self.conv1, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">64</span>,</span><br><span class="line">                                     out_channels=<span class="number">32</span>, name=<span class="string">'conv_2'</span>)                <span class="comment"># 208x32</span></span><br><span class="line">        self.conv3 = self.conv_layer(bottom=self.conv2, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">32</span>,</span><br><span class="line">                                     out_channels=<span class="number">64</span>, name=<span class="string">'conv_3'</span>)                <span class="comment"># 208x64</span></span><br><span class="line">        self.res0 = self.conv3 + self.conv1                                         <span class="comment"># 208x64</span></span><br><span class="line">        self.conv4 = self.conv_layer(bottom=self.res0, size=<span class="number">3</span>, stride=<span class="number">2</span>, in_channels=<span class="number">64</span>,</span><br><span class="line">                                     out_channels=<span class="number">128</span>, name=<span class="string">'conv_4'</span>)               <span class="comment"># 104x128</span></span><br><span class="line">        self.conv5 = self.conv_layer(bottom=self.conv4, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                     out_channels=<span class="number">64</span>, name=<span class="string">'conv_5'</span>)                <span class="comment"># 104x64</span></span><br><span class="line">        self.conv6 = self.conv_layer(bottom=self.conv5, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">64</span>,</span><br><span class="line">                                     out_channels=<span class="number">128</span>, name=<span class="string">'conv_6'</span>)               <span class="comment"># 104x128</span></span><br><span class="line">        self.res1 = self.conv6 + self.conv4     <span class="comment"># 128                               # 104x128</span></span><br><span class="line">        self.conv7 = self.conv_layer(bottom=self.res1, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                     out_channels=<span class="number">64</span>, name=<span class="string">'conv_7'</span>)                <span class="comment"># 104x64</span></span><br><span class="line">        self.conv8 = self.conv_layer(bottom=self.conv7, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">64</span>,</span><br><span class="line">                                     out_channels=<span class="number">128</span>, name=<span class="string">'conv_8'</span>)               <span class="comment"># 104x128</span></span><br><span class="line">        self.res2 = self.conv8 + self.res1      <span class="comment"># 128                               # 104x128</span></span><br><span class="line">        self.conv9 = self.conv_layer(bottom=self.res2, size=<span class="number">3</span>, stride=<span class="number">2</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                     out_channels=<span class="number">256</span>, name=<span class="string">'conv_9'</span>)               <span class="comment"># 52x256</span></span><br><span class="line">        self.conv10 = self.conv_layer(bottom=self.conv9, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_10'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv11 = self.conv_layer(bottom=self.conv10, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_11'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res3 = self.conv11 + self.conv9                                        <span class="comment"># 52x256</span></span><br><span class="line">        self.conv12 = self.conv_layer(bottom=self.res3, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_12'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv13 = self.conv_layer(bottom=self.conv12, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_13'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res4 = self.conv13 + self.res3                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv14 = self.conv_layer(bottom=self.res4, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_14'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv15 = self.conv_layer(bottom=self.conv14, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_15'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res5 = self.conv15 + self.res4                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv16 = self.conv_layer(bottom=self.res5, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_16'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv17 = self.conv_layer(bottom=self.conv16, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_17'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res6 = self.conv17 + self.res5                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv18 = self.conv_layer(bottom=self.res6, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_18'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv19 = self.conv_layer(bottom=self.conv18, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_19'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res7 = self.conv19 + self.res6                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv20 = self.conv_layer(bottom=self.res7, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_20'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv21 = self.conv_layer(bottom=self.conv20, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_21'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res8 = self.conv21 + self.res7                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv22 = self.conv_layer(bottom=self.res8, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_22'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv23 = self.conv_layer(bottom=self.conv22, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_23'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res9 = self.conv23 + self.res8                                         <span class="comment"># 52x256</span></span><br><span class="line">        self.conv24 = self.conv_layer(bottom=self.res9, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">128</span>, name=<span class="string">'conv_24'</span>)             <span class="comment"># 52x128</span></span><br><span class="line">        self.conv25 = self.conv_layer(bottom=self.conv24, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">128</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_25'</span>)             <span class="comment"># 52x256</span></span><br><span class="line">        self.res10 = self.conv25 + self.res9                                        <span class="comment"># 52x256 一个输出的特征尺度</span></span><br><span class="line">        self.conv26 = self.conv_layer(bottom=self.res10, size=<span class="number">3</span>, stride=<span class="number">2</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_26'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.conv27 = self.conv_layer(bottom=self.conv26, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_27'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv28 = self.conv_layer(bottom=self.conv27, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_28'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res11 = self.conv28 + self.conv26                                      <span class="comment"># 26x512</span></span><br><span class="line">        self.conv29 = self.conv_layer(bottom=self.res11, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_29'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv30 = self.conv_layer(bottom=self.conv29, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_30'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res12 = self.conv30 + self.res11                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv31 = self.conv_layer(bottom=self.res12, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_31'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv32 = self.conv_layer(bottom=self.conv31, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_32'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res13 = self.conv32 + self.res12                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv33 = self.conv_layer(bottom=self.res13, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_33'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv34 = self.conv_layer(bottom=self.conv33, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_34'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res14 = self.conv34 + self.res13                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv35 = self.conv_layer(bottom=self.res14, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_35'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv36 = self.conv_layer(bottom=self.conv35, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_36'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res15 = self.conv36 + self.res14                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv37 = self.conv_layer(bottom=self.res15, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_37'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv38 = self.conv_layer(bottom=self.conv37, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_38'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res16 = self.conv38 + self.res15                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv39 = self.conv_layer(bottom=self.res16, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_39'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv40 = self.conv_layer(bottom=self.conv39, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_40'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res17 = self.conv40 + self.res16                                       <span class="comment"># 26x512</span></span><br><span class="line">        self.conv41 = self.conv_layer(bottom=self.res17, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">256</span>, name=<span class="string">'conv_41'</span>)             <span class="comment"># 26x256</span></span><br><span class="line">        self.conv42 = self.conv_layer(bottom=self.conv41, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">256</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_42'</span>)             <span class="comment"># 26x512</span></span><br><span class="line">        self.res18 = self.conv42 + self.res17                                       <span class="comment"># 26x512，一个输出的特征尺度</span></span><br><span class="line">        self.conv43 = self.conv_layer(bottom=self.res18, size=<span class="number">3</span>, stride=<span class="number">2</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">1024</span>, name=<span class="string">'conv_43'</span>)            <span class="comment"># 13x1024</span></span><br><span class="line">        self.conv44 = self.conv_layer(bottom=self.conv43, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">1024</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_44'</span>)             <span class="comment"># 13x512</span></span><br><span class="line">        self.conv45 = self.conv_layer(bottom=self.conv44, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">1024</span>, name=<span class="string">'conv_45'</span>)            <span class="comment"># 13x1024</span></span><br><span class="line">        self.res19 = self.conv45 + self.conv43                                      <span class="comment"># 13x1024</span></span><br><span class="line">        self.conv46 = self.conv_layer(bottom=self.res19, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">1024</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_46'</span>)             <span class="comment"># 13x512</span></span><br><span class="line">        self.conv47 = self.conv_layer(bottom=self.conv44, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">1024</span>, name=<span class="string">'conv_47'</span>)            <span class="comment"># 13x1024</span></span><br><span class="line">        self.res20 = self.conv47 + self.res19                                       <span class="comment"># 13x1024</span></span><br><span class="line">        self.conv48 = self.conv_layer(bottom=self.res20, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">1024</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_48'</span>)             <span class="comment"># 13x512</span></span><br><span class="line">        self.conv49 = self.conv_layer(bottom=self.conv48, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">1024</span>, name=<span class="string">'conv_49'</span>)            <span class="comment"># 13x1024</span></span><br><span class="line">        self.res21 = self.conv49 + self.res20                                       <span class="comment"># 13x1024</span></span><br><span class="line">        self.conv50 = self.conv_layer(bottom=self.res21, size=<span class="number">1</span>, stride=<span class="number">1</span>, in_channels=<span class="number">1024</span>,</span><br><span class="line">                                      out_channels=<span class="number">512</span>, name=<span class="string">'conv_50'</span>)             <span class="comment"># 13x512</span></span><br><span class="line">        self.conv51 = self.conv_layer(bottom=self.conv50, size=<span class="number">3</span>, stride=<span class="number">1</span>, in_channels=<span class="number">512</span>,</span><br><span class="line">                                      out_channels=<span class="number">1024</span>, name=<span class="string">'conv_51'</span>)            <span class="comment"># 13x1024</span></span><br><span class="line">        self.res23 = self.conv51 + self.res21                                       <span class="comment"># 13x1024</span></span><br><span class="line">        <span class="keyword">return</span> self.res23  <span class="comment"># 最后输出特征</span></span><br></pre></td></tr></table></figure>
<p>同样采用k-means聚类的到anchor box的尺寸。选取了9种，3中不同的scale：(10×13); (16×30); (33×23); (30×61); (62×45); (59×119); (116 × 90); (156 × 198); (373 × 326).</p>
<h3 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h3><p>YOLO3的新的更深的网络，Darknet-53，实现细节可参见上面的代码</p>
<div align="center">
    <img src="/img/yolo/yolo3.3.png">
</div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>PCL的python库安装for Ubuntu16.04</title>
    <link href="http://yoursite.com/2018/05/11/PCL%E7%9A%84python%E5%BA%93%E5%AE%89%E8%A3%85for-Ubuntu16-04/"/>
    <id>http://yoursite.com/2018/05/11/PCL的python库安装for-Ubuntu16-04/</id>
    <published>2018-05-11T11:43:04.000Z</published>
    <updated>2019-03-19T08:23:18.389Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库<a href="https://github.com/strawlab/python-pcl" target="_blank" rel="external">python_pcl实现库</a> ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。</p>
<a id="more"></a>
<h2 id="PCL安装"><a href="#PCL安装" class="headerlink" title="PCL安装"></a>PCL安装</h2><p>不用编译源码，一行命令直接apt安装，顺带安装各种依赖的乱七八糟的库</p>
<pre><code>sudo apt-get install libpcl-dev 
</code></pre><p>再安装一些pcl可视化等软件包</p>
<pre><code>sudo apt-get install pcl_tools
</code></pre><h2 id="安装-python-pcl"><a href="#安装-python-pcl" class="headerlink" title="安装 python_pcl"></a>安装 python_pcl</h2><p>首先下载python_pcl源文件</p>
<pre><code>git clone https://github.com/strawlab/python-pcl.git
</code></pre><p>编译、安装</p>
<pre><code>python setup.py build_ext -i
python setup.py install
</code></pre><p>在此之前常出现的一个编译问题是cython版本问题，所以在执行上一步之前首先：</p>
<pre><code>pip install cython==0.25.2
</code></pre><h2 id="解决常出现的链接失败的问题"><a href="#解决常出现的链接失败的问题" class="headerlink" title="解决常出现的链接失败的问题"></a>解决常出现的链接失败的问题</h2><p>由于我的默认python为anaconda3的python，可能是anaconda3自带的链接库的问题，所以出现了如下错误：</p>
<pre><code>./lib/libgomp.so.1: version `GOMP_4.0&#39; not found (required by /home/lkj/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so) 
</code></pre><p>上面的意思是anaconda3/lib/libgomp.so.1中没有‘GOMP_4.0’，这个可以使用strings命令查看libgomp.so.1这个文件，显示并无4.0版本，因此寻找其他路径的链接库替代，用locate命令搜索系统中所有的libgomp.so.1，得到：<br><img src="/img/pcl/figure1.png" alt=""><br>然后用strings查看这些文件信息，</p>
<pre><code>/usr/lib/x86_64-linux-gnu/libgomp.so.1 |grep GOMP
</code></pre><p>发现x86_64-linux-gnu/libgomp.so.1包含GOMP_4.0<br><img src="/img/pcl/figure2.png" alt=""><br>因此可以删掉原有的libgomp.so.1，重新做一个新的链接。</p>
<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libgomp.so.1 libgomp.so.1 
</code></pre><p>然后再次在python里面import pcl,又提示libstdc++.so.6出现类似的问题，对上述做类似处理，如果还有链接库的问题，也可以用同样的方法处理,至此实现了python的pcl库安装。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库&lt;a href=&quot;https://github.com/strawlab/python-pcl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python_pcl实现库&lt;/a&gt; ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>基于深度学习的目标检测技术学习笔记(R-CNN系列)</title>
    <link href="http://yoursite.com/2018/04/20/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(R-CNN%E7%B3%BB%E5%88%97)/"/>
    <id>http://yoursite.com/2018/04/20/基于深度学习的目标检测技术学习笔记(R-CNN系列)/</id>
    <published>2018-04-20T03:03:17.000Z</published>
    <updated>2019-03-19T10:10:37.968Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。<br><a id="more"></a></p>
<p>传统的目标检测算法一般是基于滑动窗口选中图中的某一部分作为候选区域，然后提取候选区域的特征，利用分类器（如常见的SVM)进行识别。2014年提出的region proposal+CNN代替传统目标检测使用的滑动窗口+特征工程的方法，设计了R-CNN算法，开启了基于深度学习的目标检测的大门。</p>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="/img/rcnn/1.png" alt=""></p>
<p>R-CNN算法流程为：</p>
<ol>
<li>输入图像，根据SS（selective search）算法提取2000个左右的region proposal（候选框）</li>
<li>将候选框crop/wrap为固定大小后输入CNN中，得到固定维度的输出特征</li>
<li>对提取的CNN特征，利用SVM分类器分类得到对应类别</li>
<li>边界回归（bouding-box regression），用线性回归模型修正候选框的位置</li>
</ol>
<p>R-CNN使得识别的精度和速度都有了提升，但是也存在很大问题，每次候选框都需要经过CNN操作，计算量很大，有很多重复计算；训练步骤繁多。</p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>R-CNN需要每次将候选框resize到固定大小作为CNN输入，这样有很多重复计算。SPP-net的主要思想是去掉了原始图像上的crop/warp等操作，换成了在卷积特征上的空间金字塔池化层（Spatial Pyramid Pooling，SPP）。</p>
<p><img src="/img/rcnn/2.png" alt=""></p>
<p>SPP Net对整幅图像只进行一次CNN操作得到特征图，这样原图中的每一个候选框都对应于特征图上大小不同的某一区域，通过SPP可以将这些不同大小的区域映射为相同的维度，作为之后的输入，这样就能保证只进行一次CNN操作了。SPP包含一种可伸缩的池化层，输出固定尺寸特征。</p>
<p>基于SPP的思想，Fast R-CNN加入了一个ROI Pooling，将不同大小输入映射到一个固定大小的输出。R-CNN之前的操作是目标识别（classification）以及边界回归（bouding-box regression）分开进行。Fast R-CNN做的改进就是将这两个过程合并在一起，这两个任务共享CNN特征图，即成为了一个multi-task模型。</p>
<p><img src="/img/rcnn/3.png" alt=""></p>
<p>多任务自然对应multi-loss，损失函数包括分类误差以及边框回归误差。<br>L<em>cls</em>为分类误差：</p>
<p><img src="/img/rcnn/4.png" alt=""></p>
<p>分类误差只考虑对应的类别被正确分类到的概率，即P<em>l</em>为label对应的概率，当P<em>l</em>=1时，Loss为0，即正确分类的概率越大，loss越小。</p>
<p>L<em>reg</em>为边框回归误差：</p>
<p><img src="/img/rcnn/5.png" alt=""></p>
<p>对预测的边框四个位置描述参数与真实分类对应边框的四个参数偏差进行评估作为损失函数，g函数为smooth L1函数，这样对于噪声点不敏感，鲁棒性强，在|x|&gt;1时，变为线性，降低噪声影响。</p>
<p><img src="/img/rcnn/6.png" alt=""></p>
<p><img src="/img/rcnn/7.png" alt=""></p>
<p>这样加权得到的最终损失函数为：</p>
<p><img src="/img/rcnn/8.png" alt=""></p>
<p>foreground理解为前景，即对应有目标物体，这个时候需要考虑边框回归误差；background为背景，没有包含目标物品，所以不需考虑边框回归误差。</p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>Faster R-CNN对Fast R-CNN又进行了改进，使得Faster。主要是将候选框的选取也引入到网络中，代替了之前SS选取候选框的方式，即引入了RPN（Region Proposal Network），将找候选框的工作也交给了神经网络了。</p>
<p><img src="/img/rcnn/9.png" alt=""></p>
<p>提到RPN网络，就不能不说anchors，即锚点，对应的是一组矩形框，在实际中有3种形状width:height = [1:1, 1:2, 2:1]，对应3种尺寸，所以共计9个矩形框。</p>
<p><img src="/img/rcnn/10.jpg" alt=""></p>
<p>这个矩形框对应的是原始输入图像里面的，并非是卷积特征图上的。即对卷积特征图上每一个点，可以对应原始图上的一个anchors，为其配备9个框作为原始检测框，当然一开始肯定是不准确的，可以在后续的bounding box regression修正检测框位置。</p>
<p>为了生成区域建议框，在最后一个共享的卷积层输出的卷积特征映射上滑动小网络，这个网络全连接到输入卷积特征映射的nxn的空间窗口上。每个滑动窗口映射到一个低维向量上（对于ZF最后卷积层的输出是256channel，即生成256张特征图，所以小网络滑窗在特征图上的点生成向量是256-d，对于VGG是512-d，每个特征映射的一个滑动窗口对应一个数值）。这个向量输出给两个同级的全连接的层——包围盒回归层（reg）和包围盒分类层（cls）。论文中n=3，由于小网络是滑动窗口的形式，所以全连接的层（nxn的）被所有空间位置共享（指所有位置用来计算内积的nxn的层参数相同）。这种结构实现为nxn的卷积层，后接两个同级的1x1的卷积层（分别对应reg和cls）。<br>在每一个滑动窗口的位置，我们同时预测k个区域建议，所以reg层有4k个输出，即k个box的坐标编码。cls层输出2k个得分，即对每个建议框是目标/非目标的估计概率（为简单起见，是用二类的softmax层实现的cls层，还可以用logistic回归来生成k个得分）。k个建议框被相应的k个称为anchor的box参数化。每个anchor以当前滑动窗口中心为中心，并对应一种尺度和长宽比，我们使用3种尺度和3种长宽比，这样在每一个滑动位置就有k=9个anchor。对于大小为WxH（典型值约2,400）的卷积特征映射，总共有WHk个anchor。</p>
<p>Faster R-CNN的损失函数为：</p>
<p><img src="/img/rcnn/11.png" alt=""></p>
<p>这里，i是一个mini-batch中anchor的索引，Pi是anchor i是目标的预测概率。如果anchor为正，Pi<em> 就是1，如果anchor为负，Pi</em> 就是0。ti是一个向量，表示预测的包围盒的4个参数化坐标，t<em>i</em>是与正anchor对应的GT（groundtruth）包围盒的坐标向量。Pi<em> L</em>reg<em>这一项意味着只有正anchor（Pi</em> =1）才有回归损失，其他情况就没有（Pi<em> =0）。cls层和reg层的输出分别由{pi}和{ti}组成，这两项分别由N</em>cls<em>和N</em>reg*以及一个平衡权重λ归一化。<br>边框回归损失函数，用采取类似fast R-CNN介绍的方法。具体地，学习的时候，对于四个参数进行如下处理：</p>
<p><img src="/img/rcnn/12.png" alt=""></p>
<p>x，y，w，h指的是包围盒中心的（x,y）坐标、宽、高。变量x，xa，x* 分别指预测的包围盒、anchor的包围盒、GT的包围盒（对y，w，h也是一样）的x坐标，可以理解为从anchor包围盒到附近的GT包围盒的包围盒回归。</p>
<p>Fast R-CNN训练依赖于固定的目标建议框，而Faster R-CNN中的卷积层是共享的，所以RPN和Fast R-CNN都不能独立训练，论文中提出的是4步训练算法，通过交替优化来学习共享的特征。 </p>
<ol>
<li>训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调用于区域建议任务。</li>
<li>利用第一步的RPN生成的建议框，由Fast R-CNN训练一个单独的检测网络，这个检测网络同样是由ImageNet预训练的模型初始化的，这时候两个网络还没有共享卷积层。</li>
<li>用检测网络初始化RPN训练，但我们固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了。</li>
<li>保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>NHD自动登陆签到脚本</title>
    <link href="http://yoursite.com/2018/03/05/NHD%E8%87%AA%E5%8A%A8%E7%99%BB%E9%99%86%E7%AD%BE%E5%88%B0%E8%84%9A%E6%9C%AC/"/>
    <id>http://yoursite.com/2018/03/05/NHD自动登陆签到脚本/</id>
    <published>2018-03-05T12:46:38.000Z</published>
    <updated>2019-03-19T08:30:16.496Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站<a href="http://www.nexushd.org/index.php" target="_blank" rel="external">NHD</a>，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。<br><a id="more"></a></p>
<p>首先还是老套路，先用浏览器实现一遍登陆以及签到过程，用谷歌浏览器的开发者模式获取这过程中的信息。可以看到首次登陆之后，response的header有set-cookie字段</p>
<p><img src="/img/nhd/1.png" alt=""></p>
<p>而后这个服务器返回的set-cookie字段的值在下一次会话中又出现了</p>
<p><img src="/img/nhd/2.png" alt=""></p>
<p>其实总体思路还是很简单的，与之前的爬虫不同，这次需要用户登陆，所以是POST方法，并将用户名和密码以data参数代入，登陆之后服务器会返回cookie来标识用户。因为http是一种无状态协议，用户首次访问web站点的时候，服务器对用户一无所知。而Cookie就像是服务器给每个来访问的用户贴的标签，而这些标签就是对来访问的客户端的独有的身份的一个标识，这里就如同每个人的身份证一样，带着你的个人信息。而当一个客户端第一次连接过来的时候，服务端就会给他打一个标签，这里就如同给你发了一个身份证，所以之后的访问服务器再带上这个cookie就标识了该账户，具体流程网上找到一张很好的图可以解释。</p>
<p><img src="/img/nhd/3.png" alt=""></p>
<p>这样的话，只需要第一次输入密码，后面浏览器再次访问只要带上这个服务器返回的cookie，服务器就可以知道是该账户在访问，所以python程序也模拟该过程。利用request库中的session对象来创建类似于图中的过程，session对象会保存访问过程中的cookie用于之后对服务器的继续访问。</p>
<pre><code>url = &#39;http://www.nexushd.org/takelogin.php&#39;  #登陆界面
a = session.post(url=url, cookies=cookies, headers=headers, data=data)
#这里的cookie是浏览器首次访问的使用的cookie，之后服务器
#设置的cookie会保存在session对象中
time.sleep(2)

url2 = &#39;http://www.nexushd.org/signin.php&#39;  #签到界面
b = session.get(url=url2, headers=headers)
time.sleep(2)

url3 = &#39;http://www.nexushd.org/signin.php?&#39;
qiandao = {&#39;action&#39;:&#39;post&#39;,&#39;content&#39;:&#39;lalala2333&#39;} #签到信息随便填，lalala2333
r = session.post(url=url3, headers=headers, data=qiandao)
</code></pre><p>而后就是一个判断是否登陆成功的程序，依然使用BeautifulSoup来解析，得到已签到之后退出循环，并将日志信息记录到日志文件。</p>
<pre><code>r = session.post(url=url3, headers=headers, data=qiandao)
r = BeautifulSoup(r.content,&#39;lxml&#39;)
message1 = r.find_all(&#39;a&#39;,{&#39;href&#39;:&quot;signin.php&quot;})[0].contents[0]
message2 = r.find_all(&#39;h2&#39;)[0].getText()
if message2 == &#39;签到成功&#39;:
    f = codecs.open(&#39;logging.txt&#39;, &#39;a&#39;, encoding=&#39;utf-8&#39;)
    str = time.strftime(&#39;%Y-%m-%d  %H:%M:%S&#39;, time.localtime(time.time())) + &#39;-----签到成功&#39; + &#39;\n&#39;
    f.write(str)  # 记录日志信息到日志文件
    f.close()
    print(r.find_all(&#39;span&#39;, {&#39;class&#39;: &#39;medium&#39;})[0].getText())
    print(r.find_all(&#39;td&#39;, {&#39;class&#39;: &#39;text&#39;})[-1].getText().split(&#39;。&#39;)[0])
    break
elif message1 == &#39;已签到&#39;: #如果已经签到
    print(&#39;已经签到过了哦&#39;)
    break
if maxtry &lt; 30:
    print(&#39;签到失败，第&#39;+str(maxtry+1)+&#39;次重试&#39;)
    maxtry = maxtry+1
    time.sleep(5)
else:
    print(&quot;自动签到失败，请手动签到，或者检查网络连接&quot;)
    break
</code></pre><p>为了能够开机自动运行程序，将该程序添加至windows启动运行。代码中读取在配置文件中的账户信息，并且通过读取上一次签到成功时间来判断是否成功签到过。</p>
<pre><code>maxtry=0  #记录重试次数
f = codecs.open(&#39;profile.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;)  #读取配置文件，包含账户及密码
line=f.readline()
f.close()

username = line.split()[0] #你的用户名
password = line.split()[1]  #你的密码
data = {&#39;username&#39;: username, &#39;password&#39;:password}

flag = True
day_now = time.localtime(time.time()).tm_mday
f = codecs.open(&#39;logging.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;)
lines = f.readlines()
f.close()
#######更换账户登陆时，最好清除以前账户的日志信息
try:  #如果第一次使用可能没有签到记录
    day_log = int(lines[-1].split()[0].split(&#39;-&#39;)[-1])
except:
    day_log=33

day_log = int(lines[-1].split()[0].split(&#39;-&#39;)[-1])
if day_now == day_log:
    print(username+&#39;今天签到过了哦&#39;)
    flag = False
</code></pre><p>将配置文件profile.txt和日志文件logging.txt以及代码qiandao.py放入windows的启动运行的文件夹，这个文件夹可以通过在cmd窗口下输入</p>
<pre><code>shell:Startup
</code></pre><p>打开</p>
<p>在这个启动文件夹下写一个bat脚本来运行python代码</p>
<pre><code>D:
cd D:\simulation file\pyCharm\python3\qiandao
python qiandao.py
pause
</code></pre><p>至此就完整实现了电脑开机自动登陆签到NHD啦。效果如下<br>测试结果：<br><img src="/img/nhd/4.png" alt=""></p>
<p>完整代码详见个人<a href="https://github.com/lkj1114889770/WebScraping/tree/master/qiandao" target="_blank" rel="external">github</a>了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站&lt;a href=&quot;http://www.nexushd.org/index.php&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NHD&lt;/a&gt;，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。&lt;br&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>我从崖边跌落</title>
    <link href="http://yoursite.com/2018/02/12/%E6%88%91%E4%BB%8E%E5%B4%96%E8%BE%B9%E8%B7%8C%E8%90%BD/"/>
    <id>http://yoursite.com/2018/02/12/我从崖边跌落/</id>
    <published>2018-02-12T10:58:56.000Z</published>
    <updated>2019-03-19T08:32:17.549Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我从崖边跌落<br>落入星空辽阔<br>银河不清不浊<br>不知何以摆脱</p>
<p><a href="http://music.163.com/#/song?id=415086030&amp;market=baiduqk" target="_blank" rel="external">谢春花《我从崖边跌落》</a></p>
<p><img src="/img/yabian/1.jpg" alt=""></p>
<a id="more"></a>
<p><img src="/img/yabian/2.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我从崖边跌落&lt;br&gt;落入星空辽阔&lt;br&gt;银河不清不浊&lt;br&gt;不知何以摆脱&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://music.163.com/#/song?id=415086030&amp;amp;market=baiduqk&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;谢春花《我从崖边跌落》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/yabian/1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
</feed>
