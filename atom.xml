<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>听歌的小孩</title>
  <subtitle>好好学习，好好科研</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-20T06:47:39.958Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>听歌的小孩</name>
    <email>kaijianliu@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记-Region Proposal by Guided Anchoring(GA-RPN)</title>
    <link href="http://yoursite.com/2019/03/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Region-Proposal-by-Guided-Anchoring-GA-RPN/"/>
    <id>http://yoursite.com/2019/03/20/论文笔记-Region-Proposal-by-Guided-Anchoring-GA-RPN/</id>
    <published>2019-03-20T06:13:40.000Z</published>
    <updated>2019-03-20T06:47:39.958Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/pdf/1901.03278.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1901.03278.pdf</a></p>
<p>Anchor是目标检测领域的基石，目前比较先进的目标检测算法都是基于密集Anchor机制，通过预定的尺度和宽高比在feature map空间域上进行采样。这篇CVPR2019的文章提出了一种Guided Anchoring，根据语义信息来生成Anchor，主要是思想是定位目标的中心点以及相应的尺寸和宽高比，而且采用了feature adaption模块来缓解特征的不一致性。将GA-RPN应用到Fast R-CNN, Faster R-CNN和RetinaNet, 这篇论文在mAP上分别提高了 2.2%, 2.7% and 1.2%. </p>
<a id="more"></a>
<h2 id="Guided-Anchoring"><a href="#Guided-Anchoring" class="headerlink" title="Guided Anchoring"></a>Guided Anchoring</h2><p><img src="/img/GA-RPN/1.png" alt=""></p>
<p>待续，晚点写。。。。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/1901.03278.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/pdf/1901.03278.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anchor是目标检测领域的基石，目前比较先进的目标检测算法都是基于密集Anchor机制，通过预定的尺度和宽高比在feature map空间域上进行采样。这篇CVPR2019的文章提出了一种Guided Anchoring，根据语义信息来生成Anchor，主要是思想是定位目标的中心点以及相应的尺寸和宽高比，而且采用了feature adaption模块来缓解特征的不一致性。将GA-RPN应用到Fast R-CNN, Faster R-CNN和RetinaNet, 这篇论文在mAP上分别提高了 2.2%, 2.7% and 1.2%. &lt;/p&gt;
    
    </summary>
    
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之排序</title>
    <link href="http://yoursite.com/2019/03/01/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2019/03/01/数据结构与算法之排序/</id>
    <published>2019-03-01T03:09:50.000Z</published>
    <updated>2019-03-01T07:40:52.038Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>排序就是按照某种逻辑顺序将一组对象重新排列的过程，这篇博客就对常见的排序算法进行总结，包括：冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序。</p>
<a id="more"></a>
<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><p>冒泡排序通过比较相邻的元素大小来完成排序。这里定义比较边界，也就是进行大小比较的边界。对于长度为n的数组，第一趟的比较边界为[0,n-1]，也就是说从a[0]开始，相邻元素两两比较大小，如果满足条件就进行交换，否则继续比较，一直到最后一个比较的元素为a[n-1]为止，此时第一趟排序完成；每一次排序完后最大元素沉入底部，比较边界变成[0,n-2]。对于长度为n的序列，最多需要n趟完成排序，所以冒泡排序就由两层循环构成，最外层循环用于控制排序的趟数，最内层循环用于比较相邻数字的大小并在本趟排序完成时更新比较边界。</p>
<p><img src="/img/sort/maopao.gif" alt=""></p>
<p>在排序后期可能数组已经有序了而算法却还在一趟趟的比较数组元素大小，可以引入一个标记，如果在一趟排序中，数组元素没有发生过交换说明数组已经有序，跳出循环即可。</p>
<p><strong>冒泡排序的时间复杂度为O(n^2),辅助空间为O(1),属于原地排序。</strong></p>
<h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><p>基本思想很简单：首先找到数组中最小的那个元素，然后和数组中第一个元素交换位置；然后在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置；如重复直到整个数组排序好。</p>
<p><strong>对于长度为N的数组，选择排序需要大约N^2/2次比较和N次交换，辅助空间也是O(1),属于原地排序。</strong></p>
<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><p>插入排序将数组分为有序部分和无序部分，与选择排序一样索引左边的都是有序的，为了给更小的元素腾出空间，他们都可能会被移动，当索引到达数组右端的时候，数组就排序完成了。</p>
<p><img src="/img/sort/charu.gif" alt=""></p>
<p>插入排序对一个很大且其中元素已经有序（或接近有序）的数组进行排序会比对随机顺序的数组或是逆序数组进行排序要快得多。</p>
<p>插入排序平均需要~N^2/4次比较和~N^2/4次交换，最坏情况下需要~N^2/2次比较和~N^2/2次交换，最好情况下需要N-1次比较和0次交换。</p>
<h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><p>对于大规模乱序数组，插入排序很慢，因为它只会交换相邻的元素，因此元素需要一点一点地从一端移动到另一端。<strong>希尔排序改进了插入排序，交换不相邻的元素以对数组局部进行排序，并最终用插入排序将局部有序数组排序。</strong></p>
<p>本质上来说，希尔排序就是把数列进行分组(不停使用插入排序)，直至从宏观上看起来有序，最后插入排序起来就容易了(无须多次移位或交换）。<br>下图为来自网上的一个图解例子：</p>
<p><img src="/img/sort/xier.png" alt=""></p>
<p>希尔排序适合于中等大小的数组且不需要额外的内存空间，因为采取了分治策略，<strong>平均时间复杂度为O(NlgN)</strong>。</p>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><p>归并排序将两个有序的数组归并成一个更大的有序数组，所以在排序的时候将一个数组地跪地分成两半进行分别排序，然后将结果归并起来。<strong>归并排序由两个过程完成：有序表的合并和排序的递归实现</strong>。</p>
<p><img src="/img/sort/guibing.gif" alt=""></p>
<p>将待排序序列分为A和B两部分，如果A和B都是有序的，只需要调用有序序列的合并算法mergeArray就完成了排序，可是A和B不是有序的，再分别将A和B一分为二，直至最终的序列只有一个元素，我们认为只有一个元素的序列是有序的，合并这些序列，就得到了新的有序序列，然后返回给上层调用者，上上层调用这再合并这些序列，得到更长的有序序列，这就是递归形式的归并排序，示意图如下图所示：</p>
<p><img src="/img/sort/guibing2.png" alt=""></p>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>快速排序是冒泡排序的改进版，也是最好的一种内排序。快速排序也是一种分治的排序算法，它将一个数组分成两个子数组，将两部分独立地进行排序。与归并排序不同的是：归并排序将数组分成两个数组进行排序后归并成一个有序数组；快速排序则是当两个子数组都有序之后整个数组也就有序了。<br>其主要步骤以下面一个例子为例：</p>
<ol>
<li>设置两个变量i、j，排序开始的时候：i=0，j=n-1；</li>
<li>第一个数组值作为比较值，首先保存到temp中，即temp=A[0]；</li>
<li>然后j– ,向前搜索,找到小于temp后,因为s[i]的值保存在temp中,所以直接赋值,s[i]=s[j]</li>
<li>然后i++,向后搜索,找到大于temp后,因为s[j]的值保存在第2步的s[i]中,所以直接赋值,s[j]=s[i],然后j–,避免死循环</li>
<li>重复第3、4步，直到i=j,最后将temp值返回s[i]中</li>
<li>然后采用“二分”的思想,以i为分界线,拆分成两个数组 s[0,i-1]、s[i+1,n-1]又开始排序</li>
</ol>
<p><img src="/img/sort/kuaisu.png" alt=""></p>
<p>快速排序空间复杂度为O(NlgN)。</p>
<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><p>堆排序基于优先队列数据结构，高效地实现删除最大元素和元素插入操作。</p>
<h3 id="堆的定义"><a href="#堆的定义" class="headerlink" title="堆的定义"></a>堆的定义</h3><p>当一棵二叉树的没个节点都大于其子节点的时候，称之为堆有序，其根节点即是最大节点。具体实现就是将二叉树的节点按照层级顺序放入数组，根节点在1，它的子节点在2,3，而子节点的子节点在4,5,6,7，以此类推。</p>
<p><img src="/img/sort/figure2.4.2.png" alt=""></p>
<p>堆的上浮和下沉操作都可以使得堆有序化。</p>
<p><img src="/img/sort/figure2.4.3.png" alt=""></p>
<p><img src="/img/sort/figure2.4.5.png" alt=""></p>
<h3 id="堆排序-1"><a href="#堆排序-1" class="headerlink" title="堆排序"></a>堆排序</h3><p>堆排序主要为两个阶段：构造一个堆有序的数组使得最大元素位于开头，构建堆的目的就是使以每个节点作为根节点的树都满足堆的定义，因此从堆（完全二叉树）的<strong>最下侧非叶子节点开始构建初始堆</strong>，根据堆的性质，这个节点的索引是⌊n/2⌋。从下向上，一直到堆顶节点也满足堆的定义，表示完成堆的初始化；堆的下沉排序，将堆中最大元素删除，然后放入堆缩小后数组中空出的位置。</p>
<p><img src="/img/sort/figure2.4.7.png" alt=""></p>
<p>堆排序是现有的唯一同时最优利用时间和空间的排序算法，在最坏的情况下也能保证~2NlgN次比较和恒定的额外空间。</p>
<h3 id="各排序算法比较"><a href="#各排序算法比较" class="headerlink" title="各排序算法比较"></a>各排序算法比较</h3><p><img src="/img/sort/bijiao.jpg" alt=""></p>
<p><font face="微软雅黑" color="black" size="4"><strong>参考</strong></font><br>《算法（第4版》</p>
<p><a href="https://www.cnblogs.com/lifexy/p/7597276.html" target="_blank" rel="external">https://www.cnblogs.com/lifexy/p/7597276.html</a></p>
<p><a href="https://www.cnblogs.com/lz3018/p/5742255.html" target="_blank" rel="external">https://www.cnblogs.com/lz3018/p/5742255.html</a></p>
<p><a href="https://www.cnblogs.com/beli/p/6297741.html" target="_blank" rel="external">https://www.cnblogs.com/beli/p/6297741.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排序就是按照某种逻辑顺序将一组对象重新排列的过程，这篇博客就对常见的排序算法进行总结，包括：冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法之查找</title>
    <link href="http://yoursite.com/2019/02/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E6%9F%A5%E6%89%BE/"/>
    <id>http://yoursite.com/2019/02/26/数据结构与算法之查找/</id>
    <published>2019-02-26T07:42:27.000Z</published>
    <updated>2019-03-02T07:17:12.286Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>查找是针对符号表（有时也称之为字典或者索引）存储的键(key)值(value)信息，按照指定的键来查找信息，在这里就对常见的查找算法以及实现高效查找的符号表数据结构进行介绍。</p>
<a id="more"></a>
<h2 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h2><p>顺序查找是在无序队列中找出与关键字相同的数的具体位置，原理是让关键字与队列中的数逐个比较，知道找出与给定关键字相同的数为止。</p>
<h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><p>二分查找针对的是有序的符号表，如果是无序的首先就要进行排序操作。其基本思想很简单，首先将key和中间键比较，如果相等则返回其索引；如果小于中间键则在左半部分同样取中间键进行比较；大于则在右半部分取中间键进行比较。</p>
<pre><code>**在N个键的有序符号表中进行二分查找最多需要（logN+1）次比较），算法复杂度为O(logN).**
</code></pre><h2 id="分块查找"><a href="#分块查找" class="headerlink" title="分块查找"></a>分块查找</h2><p>分块查找是二分查找和顺序查找的一种改进，分块查找要求索引表是有序的，对于块内节点没有排序要求（块内无序，块间有序），因此特别适合节点动态变换的情况。这种带索引表的分块有序表查找性能取决于两步查找时间之和：第一步可以采用简单书序查找或者是二分查找查找索引表；第二部对于块之内只能顺序查找。</p>
<p>假设索引表有n个元素，每块含有s个元素，平均查找长度为(n/s+s)/2+1,时间复杂度为O(n)~O(lgn).</p>
<h2 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h2><p>二叉查找树（Binary Search Tree）每个节点都含有一个可比较大小的键，每个节点都含有两个链接（有点类似于链表的扩展），基本定义为：</p>
<pre><code>class Node{
    Key key;  // 键
    Value val;  // 值
    Node left,right;  // 指向子树的链接
    int N;  // 以该节点为根的子数节点总数
}
</code></pre><p><img src="/img/search/figure3.2.2.png" alt=""></p>
<p>具有的一条最重要的性质为<strong>：每个节点的键都大于左子树的任意节点键值，并且小于右子树任意节点键值。</strong></p>
<h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><p><img src="/img/search/figure3.2.4.png" alt=""></p>
<p>在二叉查找树中查找，可以采用递归的方法：如果树是空的，则查找未命中；如果被查找键和根节点的键相等，则查找命中；否则就递归地再适当的子树中查找，如果被查找的键较小就选择左子树，较大就选择右子树。</p>
<h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p>插入操作和查找类似：如果树是空的，就返回含有该键值对的新节点；如果被查找的键小于根节点的键，就继续在左子树插入该键，否则在右子树插入该键。</p>
<p><img src="/img/search/figure3.2.6.png" alt=""></p>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>从二叉树中删除一个元素如图3.2.13，总结来说，就是在被删除元素的右子树中不断检索左子树，作为后继节点替换被删除元素，从而达到删除对应元素的目的。</p>
<p><img src="/img/search/figure3.2.13.png" alt=""></p>
<h3 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h3><p>二叉查找树实现良好的性能依赖于其中键的分布足够随机以能够消除长路径。最坏的情况下，N个节点都存在于一个查找路径上，查找增长数量级为N，平均查找2lnN=1.39lgN.</p>
<h2 id="平衡查找树"><a href="#平衡查找树" class="headerlink" title="平衡查找树"></a>平衡查找树</h2><p>平衡查找树含有N个节点，且能保持树高为~lgN,这样就能保证所有查找在lgN次内比较结束。</p>
<h3 id="2-3查找树"><a href="#2-3查找树" class="headerlink" title="2-3查找树"></a>2-3查找树</h3><p>一棵2-3查找树由以下节点组成：</p>
<ol>
<li>2-节点，含有一个键和两个链接，左链接的子树小于该节点，有链接的子树大于该节点。</li>
<li>3-节点，含有两个个键和三个链接，除左右链接外，还有一个中链接，指向键都位于两个键之间的树。</li>
</ol>
<p><img src="/img/search/figure3.3.1.png" alt=""></p>
<p><strong>因为包括3节点，所以可以保持平衡性，即任意一条路径长度都相同，在这种情况下，插入算法都是局部变换，除了相关的节点和链接之外，不必修改其他节点。在一棵大小为N的2-3树种，查找和插入操作访问的节点不超过lgN个。</strong></p>
<p><img src="/img/search/figure3.3.10.png" alt=""></p>
<p><img src="/img/search/figure3.3.11.png" alt=""></p>
<h3 id="红黑二叉查找树"><a href="#红黑二叉查找树" class="headerlink" title="红黑二叉查找树"></a>红黑二叉查找树</h3><p>红黑二叉查找树的思想是用标准的二叉查找树（完全由2-节点构成）和一些额外的信息（替换3-节点）。树种包括两种链接：红链接将两个2-节点链接构成一个3-节点，黑链接即是2-3树中的普通链接。</p>
<p><img src="/img/search/figure3.3.12.png" alt=""></p>
<p>红黑树是满足下列条件的二叉查找树“</p>
<ul>
<li>红链接均为左链接；</li>
<li>没有任何节点同时和两条红链接相接；</li>
<li>树是完美黑色平衡的，任意空链接到根节点的路径上的黑链接数量相同。</li>
</ul>
<p><img src="/img/search/figure3.3.15.png" alt=""></p>
<p><strong>某些插入操作之后可能出现右红链接或者两条连续的红链接，这时候就需要进行旋转，以保证红黑树的有序性和完美平衡性。</strong></p>
<p><img src="/img/search/figure3.3.17.png" alt=""></p>
<p>如果左右节点均为红色，需要进行颜色转换：</p>
<p><img src="/img/search/figure3.3.21.png" alt=""></p>
<p>一个完整的红黑树构造轨迹如图3.3.24：</p>
<p><img src="/img/search/figure3.3.24.png" alt=""></p>
<p><img src="/img/search/table3.3.1.png" alt=""></p>
<h2 id="散列表"><a href="#散列表" class="headerlink" title="散列表"></a>散列表</h2><p>散列表（也称之为哈希表）将键作为数组索引，从而可以快速访问任意键的值。散列表的算法主要分为两步：用<strong>散列函数（哈希函数）</strong>将键转化为数组的索引；针对多个键散列到同一个索引值的情况，<strong>处理碰撞冲突</strong>。<br><strong>直接定址</strong>与<strong>解决冲突</strong>是哈希表的两大特点，其思想很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。散列表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整散列函数函数算法即可在时间和空间上做出取舍。</p>
<h3 id="基于拉链法的散列表"><a href="#基于拉链法的散列表" class="headerlink" title="基于拉链法的散列表"></a>基于拉链法的散列表</h3><p>基于拉链法的散列表采用链表的数据结构，数组中元素为链表，将散列值相同的键值保存在一个链表中。</p>
<p><img src="/img/search/figure3.4.3.png" alt=""></p>
<h3 id="基于线性探测法的散列表"><a href="#基于线性探测法的散列表" class="headerlink" title="基于线性探测法的散列表"></a>基于线性探测法的散列表</h3><p>这种方法会用一个比较大一点的并行数组来保存键值对，一个保存键，一个保存值，解决碰撞的策略也是极其简单粗暴：当碰撞发生时（一个散列表的值已经被另一个不同的键占用），直接检查散列表的下一位置（索引值加1），不同则继续查找（到数组结尾时折回开头），直到找到该键或者遇到一个空元素，这时保存值。</p>
<p><img src="/img/search/1.png" alt=""></p>
<p>Hash（key） = key % 10（表长）；<br>89放入9的位置；18放入8的位置；49与89冲突，往后加到尾了就再回到头，0的位置为空放入；58与18冲突，往后加有89，再加有49，再加就放入1的位置；9与89冲突一直加到2的位置放入。<br>Hash（key） + 0，Hash（key）+1，……</p>
<p><font face="微软雅黑" color="black" size="4"><strong>参考</strong></font><br>《算法（第4版》，本篇博客大部分图的出于此书。</p>
<p><a href="https://blog.csdn.net/sayhello_world/article/details/77200009" target="_blank" rel="external">https://blog.csdn.net/sayhello_world/article/details/77200009</a></p>
<p><a href="https://blog.csdn.net/simplehap/article/details/70577454" target="_blank" rel="external">https://blog.csdn.net/simplehap/article/details/70577454</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;查找是针对符号表（有时也称之为字典或者索引）存储的键(key)值(value)信息，按照指定的键来查找信息，在这里就对常见的查找算法以及实现高效查找的符号表数据结构进行介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之链表</title>
    <link href="http://yoursite.com/2019/02/21/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2019/02/21/数据结构之链表/</id>
    <published>2019-02-21T07:37:15.000Z</published>
    <updated>2019-03-04T05:41:17.765Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>链表由一系列的不必在内存中连续的结构组成，每一个结构包括节点（数据域）和指向该节点之后节点的指针（指针域）构成。</p>
<pre><code>struct Node{
    int value;
    Node *next;
}
</code></pre><p>上面即是一种典型的单向链表，还有一种双向链表，除了数据域之外，还包括分别指向上一个节点和下一个节点的指针；如果让链表最后一个节点指向第一个节点，就构成了循环链表。</p>
<h2 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h2><a id="more"></a>
<h3 id="插入节点"><a href="#插入节点" class="headerlink" title="插入节点"></a>插入节点</h3><p>在p之后插入值为i的节点</p>
<pre><code>void insert(Node *p, int i){
    Node* node = new Node;
    node-&gt;value = i;
    node-&gt;next = p-&gt;next;
    p-&gt;next = node;
}
</code></pre><h3 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h3><p>删除某个节点p时，只需要用p后一个节点覆盖p当前节点：</p>
<pre><code>void delete(NOde *p){
    p-&gt;value = p-&gt;next-&gt;value;
    p-&gt;next = p-&gt;next-&gt;next;
}
</code></pre><h3 id="找出倒数第k个节点"><a href="#找出倒数第k个节点" class="headerlink" title="找出倒数第k个节点"></a>找出倒数第k个节点</h3><p>采用两个指针p1,p2，p1先走k步，然后p1和p2一起走，当p1到达节点终点时，p2即指向倒数第k个节点。</p>
<pre><code>Node* findk(Node* head, int k){
    Node *p1 = head;
    Node *p2 = head;
    for(int i=0;i&lt;k;i++){
        if(p1==NULL) return NULL;
        p1=p1-&gt;next;
    }
    while(p1-&gt;next !=NULL &amp;&amp; p2-&gt;next != NULL){
        p1=p1-&gt;next;
        p2=p2-&gt;next;
    }
    return p2;
}
</code></pre><p>同样的道理，如果要找出链表的中间节点，可以令p1和p2分别以前进2步、前进1步的速度遍历，当p1到达节点终点时，p2即指向终点节点。</p>
<h3 id="判断是否有环"><a href="#判断是否有环" class="headerlink" title="判断是否有环"></a>判断是否有环</h3><p>直观的判断是，存在环路的时候，一直遍历都不会遇到NULL节点，但是由于不知道链表的长度吗，所以采用这种判断不知道多久才能终止。转化成一个追及问题，如果有环，那么两个遍历速度不一致的指针一定会相遇，采用类似上面，有两个指针pslow，pfast,pslow每次走一步，pfast每次走两步，若是有环，pfast就能追上pslow，否则pfast会碰到NULL。</p>
<pre><code>bool isLoop(Node* head){
    if(head==NULL) return false;
    Node* pfast = head;
    Node* pslow = head;
    while(pfast!=NULL){
        if(pfast-&gt;next==NULL) return false;
        pslow = pslow-&gt;next;
        pfast = pfast-&gt;next-&gt;next;
        if(pfast==pslow) return true;
    }
    return false;
}
</code></pre><h3 id="反向遍历"><a href="#反向遍历" class="headerlink" title="反向遍历"></a>反向遍历</h3><p>可以采用先将链表存在栈中，然后先进后出即为反向遍历：</p>
<pre><code>void reverse(Node* head){
    stack&lt;Node*&gt; nodestack;
    Node* p = head;
    while(p!=NULL){
        nodestack.push(p);
        p=p-&gt;next;
    }
    while(!nodestack.empty()){
        cout&lt;&lt;nodestack.top().value&lt;&lt;endl;
        nodestack.pop();
    }
}
</code></pre><p>或者是采用递归的方法：</p>
<pre><code>void reverse(Node* head){
    if(head!=NULL){
        if(head-&gt;next!=NULL){
            reverse(head-&gt;next);
        }
        cout&lt;&lt;head-&gt;value&lt;&lt;endl;
    }
}
</code></pre><h3 id="链表反转"><a href="#链表反转" class="headerlink" title="链表反转"></a>链表反转</h3><p>采用就地反转法，有一篇博客讲的很清楚<a href="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" title="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" target="_blank" rel="external">https://www.cnblogs.com/byrhuangqiang/p/4311336.html</a>，这里就转载一下人家的方法，主要思路就是把当前链表的下一个节点PCur插入到头结点dummy的下一个节点中，就地反转。<br>dummy-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5的就地反转过程：<br>dummy-&gt;2-&gt;1-&gt;3-&gt;4-&gt;5<br>dummy-&gt;3-&gt;2-&gt;1-&gt;4-&gt;5<br>dummy-&gt;4&gt;-3-&gt;2-&gt;1-&gt;5<br>dummy-&gt;5-&gt;4-&gt;3-&gt;2-&gt;1</p>
<p>初始状态为：</p>
<p><img src="/img/LinkedList/reverseList1.png" width="300" hegiht="150" align="center"></p>
<p>pCur指向每一次需要反转的节点，将prev对应的节点连接到下一个需要反转的节点，将pCur连接的当前需要反转的节点作为插入到dummy之后，即移到了头部，调整pCur指向下一个需要反转的节点，这样循环直到下一个需要反转的节点为NULL，其动态过程如下图：</p>
<p><img src="/img/LinkedList/reverseList2.png" width="300" hegiht="450" align="center"></p>
<pre><code>Node* reverseList(Node* head){
    if(head==NULL) return head;
    Node* dummy = new Node;
    dummy-&gt;next = head;
    Node* prev = dummy-&gt;next;
    Node* pCur = prev-&gt;next;
    while(pCur!=NULL){
        prev-&gt;next = pCur-&gt;next;
        pCur-&gt;next = dummy-&gt;next;
        dummy-&gt;next = pCur;
        pCur = prev-&gt;next;
    }
    return dummy-&gt;next;
}
</code></pre><h3 id="链表相交"><a href="#链表相交" class="headerlink" title="链表相交"></a>链表相交</h3><p>两个链表相交，则相交之后必重合，从交点到末尾节点均相同。那么如果两个链表p1，p2相交的话，首先假设获得链表的长度分别为m，n，那么让较长的先走|m-n|步，再同步走的话，如果两个链表相交，那么p1，p2指针必定会相撞。</p>
<font face="微软雅黑" color="black" size="4"><strong>参考</strong></font>

<p><a href="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" title="https://www.cnblogs.com/byrhuangqiang/p/4311336.html" target="_blank" rel="external">https://www.cnblogs.com/byrhuangqiang/p/4311336.html</a><br><a href="https://www.cnblogs.com/byonecry/p/4458821.html" title="https://www.cnblogs.com/byonecry/p/4458821.html" target="_blank" rel="external">https://www.cnblogs.com/byonecry/p/4458821.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h2&gt;&lt;p&gt;链表由一系列的不必在内存中连续的结构组成，每一个结构包括节点（数据域）和指向该节点之后节点的指针（指针域）构成。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct Node{
    int value;
    Node *next;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面即是一种典型的单向链表，还有一种双向链表，除了数据域之外，还包括分别指向上一个节点和下一个节点的指针；如果让链表最后一个节点指向第一个节点，就构成了循环链表。&lt;/p&gt;
&lt;h2 id=&quot;一些例子&quot;&gt;&lt;a href=&quot;#一些例子&quot; class=&quot;headerlink&quot; title=&quot;一些例子&quot;&gt;&lt;/a&gt;一些例子&lt;/h2&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>CornerNet:将目标检测转为关键点预测</title>
    <link href="http://yoursite.com/2018/09/26/CornerNet-%E5%B0%86%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%BD%AC%E4%B8%BA%E5%85%B3%E9%94%AE%E7%82%B9%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2018/09/26/CornerNet-将目标检测转为关键点预测/</id>
    <published>2018-09-26T06:22:54.000Z</published>
    <updated>2018-09-26T08:28:31.846Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="external">https://arxiv.org/abs/1808.01244</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="external">https://github.com/umich-vl/CornerNet
</a></p>
<p>CornerNet是ECCV2018上的一篇文章，与以往的Anchor机制的目标检测方法不同，这篇文章借鉴了人体关键点检测的思路，将目标检测转为关键点检测（Dectecting Objects as Paired Keypoints），是一种不一样的新思路，阅读了这篇文章，做个笔记。<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>目前目标检测算法的主要思路还是设置大量的Anchor作为预选框，通过训练的方式获取最后的bounding box，这样就带来两个问题：</p>
<ol>
<li>大量的Anchor只有少部分和gt有比较大的overlap，从而带来正负样本巨大的不均衡的问题，减慢训练过程</li>
<li>Anchor的设置本身也是需要超参数的(形状、个数怎么设置)，在multi-scale的时候会更加明显。</li>
</ol>
<p>作者因此提出了一种新的one-stage解决方法，将目标检测转为一堆关键点检测，the top-left corner 和bottom-right corner，使用卷积神经网络来为一个类别预测heatmap获取top-left corners,同样预测另一个heatmap获取bottom-right corners,还预测embedding vector对顶点进行分组，确定是否属于同一个目标，如下图所示。</p>
<p><img src="/img/CornerNet/figure_1.png" alt=""></p>
<p>另一个创新点是提出了corner pooling，一种为了更好地获取corner的新的pooling layer。以top-left corner pooling 为例，如下图所示对每个channel，分别提取特征图的水平和垂直方向的最大值，然后求和。</p>
<p><img src="/img/CornerNet/figure_2.png" alt=""></p>
<p>论文认为corner pooling之所以有效，是因为（1）目标定位框的中心难以确定，和边界框的4条边相关，但是每个顶点只与边界框的两条边相关，所以corner 更容易提取。（2）顶点更有效提供离散的边界空间，实用O(wh)顶点可以表示O(w2h2) anchor boxes。</p>
<h2 id="CornetNet"><a href="#CornetNet" class="headerlink" title="CornetNet"></a>CornetNet</h2><p>CornerNet使用CNNs来预测两组heatmaps为每个物品类别来表示corner的位置，使用embedding vector来表示corner是否属于同一个物品，同时为了产生更加紧密的bounding box，也预测了offset。通过heatmap, embedding vector，offsets,通过后处理的方法就可以获得最后的bounding box。作者提出的算法总体框架如下图所示：</p>
<p><img src="/img/CornerNet/figure_3.png" alt=""></p>
<p>使用了hourglass network作为backbone network，紧接的两个模块分别用于预测top-left corners和bottom-right corners，每一个模块有独立的corner pooling，然后得到heatmaps, embeddings, offsets.</p>
<h3 id="Detecting-Corners"><a href="#Detecting-Corners" class="headerlink" title="Detecting Corners"></a>Detecting Corners</h3><p>论文预测了两组heatmap，每一个heatmap包含C channels(C是目标类别，不包括background),每一个channel是二进制mask，表示相应的corner位置。</p>
<p><img src="/img/CornerNet/figure_4.png" alt=""></p>
<p>对于每个顶点，只有一个groun truth，其他位置都是负样本。在训练过程中为了减少负样本数量，在每个gt顶点设定的半径r区域内都是正样本，如上图所示，半径r的确定根据所学的Iou决定。使用unnormalized 2D Gaussian来减少的半径r范围内的loss，基本思想就是构造高斯函数，中心就是gt位置，离这个中心越远衰减得越厉害，即：</p>
<p><img src="/img/CornerNet/figure_5.png" alt=""></p>
<p>pcij表示类别为c，坐标是（i,j）的预测热点图，ycij表示相应位置的ground-truth，是经过2D Gaussian的输出值，用来调整权值，论文提出变体Focal loss表示检测目标的损失函数：</p>
<p><img src="/img/CornerNet/figure_6.png" alt=""></p>
<p>由于采样过程中的量化带来的misaligment，预测offset来调整corner的位置：</p>
<p><img src="/img/CornerNet/figure_7.png" alt=""></p>
<p>训练中用smooth L1 Loss来计算：</p>
<p><img src="/img/CornerNet/figure_8.png" alt=""></p>
<h3 id="Grouping-Corner"><a href="#Grouping-Corner" class="headerlink" title="Grouping Corner"></a>Grouping Corner</h3><p>这个部分是用来决定一对corner是否来自同一object。具体的做法就是对卷积特征进行embedding（1x1的conv），得到corner的embedding vector，我们希望同属于同一个object的一对 corner的距离尽可能小，不属于的距离尽可能大！所以有两个loss，push loss和pull loss，从名字上来说，pull吧同一个目标的corner拉近，push把不同目标的推远。</p>
<p><img src="/img/CornerNet/figure_9.png" alt=""></p>
<p>etk,elk分别是属于 top-left corner和botto-right corner的embedding，ek是他们的平均值，△在论文中设置为1.</p>
<h3 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h3><p><img src="/img/CornerNet/figure_10.png" alt=""></p>
<p>为了检测某一个点是否是corner，需要从行和列分别检查，以top-left这个点为例，计算过程分为三部分：</p>
<ol>
<li>从上到下做max pooling</li>
<li>从右到左做max pooling</li>
<li>然后合并（相加）<br>如下图中，从下往上计算，每一列都能得到一个单调非递减的结果，相当于对corner的先验做了编码。对于object来说，如果要去找最上边的位置，需要从下到上检查这一列的最大值，最大值的位置是corner的可能存在的位置。</li>
</ol>
<p><img src="/img/CornerNet/figure_11.png" alt=""></p>
<p>实际计算公式为：</p>
<p><img src="/img/CornerNet/figure_12.png" alt=""></p>
<p>这样整个预测框架如下图所示：</p>
<p><img src="/img/CornerNet/figure_13.png" alt=""></p>
<h3 id="Predict-details"><a href="#Predict-details" class="headerlink" title="Predict details"></a>Predict details</h3><ol>
<li>在corner heatmap上用3x3的max poolings做NMS，选择top 100的top-left和top 100的bottom-right</li>
<li>通过预测的offset来调整位置</li>
<li>计算top-left和bottom-right的embedding 的L1 distance，筛掉距离大于0.5或者是不属于同一类别的一对corner。</li>
<li>计算top-left和bottom-right的score的平均值作为最终的score。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1808.01244&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1808.01244&lt;/a&gt;&lt;br&gt;代码链接：&lt;a href=&quot;https://github.com/umich-vl/CornerNet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/umich-vl/CornerNet
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CornerNet是ECCV2018上的一篇文章，与以往的Anchor机制的目标检测方法不同，这篇文章借鉴了人体关键点检测的思路，将目标检测转为关键点检测（Dectecting Objects as Paired Keypoints），是一种不一样的新思路，阅读了这篇文章，做个笔记。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN论文+源码阅读笔记</title>
    <link href="http://yoursite.com/2018/09/20/Mask%20R-CNN%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/09/20/Mask R-CNN学习笔记/</id>
    <published>2018-09-20T07:54:57.000Z</published>
    <updated>2019-02-21T08:58:25.714Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文链接：<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a><br>源码链接：<a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="external">https://github.com/matterport/Mask_RCNN</a><br>之前一直在做目标检测这块，最近了解了一下实例分割，其实是目标检测更细致的任务，在图像中做到像素级的分割，包括目标检测和准确的像素分割，所以说是结合了之前目标检测的任务（classification and localization），以及语义分割（将像素点分类到特定的所属类别），首先拜读的就是17年何凯明大神的论文<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">Mask R-CNN</a>，并且阅读了keras版本的实现<a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="external">代码</a>，在此做一个学习笔记，<br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Mask R-CNN是在faster R-CNN的classification branch和bounding box regression branch基础上，增加了一个segmentation mask branch，以像素到像素的方法来预测分割掩码（segmentation mask），如下图所示。</p>
<p><img src="/img/Mask_R-CNN/figure_1.png" alt=""></p>
<p>Faster R-CNN由于RoI pooling，没有办法做到输入和输出之间的像素到像素的对齐(pixel-to-pixel)，为了解决这个问题，Mask R-CNN提出了一个RoiAlign层，可以极大地提高掩码的准确率；同时解耦掩码预测和分类预测，为每个类都独立地预测二进制掩码，这样不会跨类别竞争。最终运行速度可达5 FPS左右。</p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p>Mask R-CNN和faster R-CNN类似，具有两个阶段，第一阶段是RPN，第二阶段出了预测类别和检测框的偏移外，还能够为每个RoI输出二进制掩码，这三个输出都是并行输出的。此外对每一个RoI提出了multi task：<br>L=L<em>cls</em>+L<em>box</em>+L<em>mask</em><br>Lcls和Lbox和之前faster RCNN中定义的一样，掩码分支对于每个RoI的输出维度为Km2，即K个分辨率为m×m的二进制掩码，每个类别一个，K为类别数量。对每个像素应用sigmod，Lmask为平均二进制交叉熵损失，对于真实类别为k的RoI，仅在第k个掩码上计算Lmask，其他掩码输出不计入损失。</p>
<p><img src="/img/Mask_R-CNN/figure_2.png" alt=""></p>
<h3 id="Mask-Representation"><a href="#Mask-Representation" class="headerlink" title="Mask Representation"></a>Mask Representation</h3><p>掩码用来表述目标在图片中的像素位置，在mask R-CNN中通过卷积的方法，提供了像素到像素的对应来解决。具体来说，使用FCN的方法为每一个RoI预测一个m x m的掩码，与使用FC层预测掩码的方式不同，全卷积的方法需要更少的参数，预测也会更加准确。这种像素到像素的行为需要RoI特征，为了更好地与原图进行对齐，来准确地对应原图的像素关系，这里就提出了一个很关键的模块，RoIAlign层。</p>
<h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p>这个网络层主要是为了更好地与原图像素进行对齐，对之前faster R-CNN使用RoI Pooling操作中两次量化造成的区域不匹配(mis-aligment)问题进行了改进，所以在这里就不得不提一下RoI的局限性，借鉴了<a href="http://blog.leanote.com/post/afanti/b5f4f526490b" target="_blank" rel="external">一篇博客</a>的详细介绍。</p>
<h4 id="RoI-pooling的局限性"><a href="#RoI-pooling的局限性" class="headerlink" title="RoI pooling的局限性"></a>RoI pooling的局限性</h4><p><strong>Faster R-CNN的网络框架</strong></p>
<p><img src="/img/Mask_R-CNN/figure_3.png" alt=""></p>
<p>由上图可以看到，RoI pooling位于RPN、Feature map和classification and regression之间，针对RPN输出的RoI，将其resize到统一的大小，首先将RoI映射到feature map对应的位置，将映射的区域划分为k x k个单元，对每个单元进行maxpooling，这样就得到统一大小k x k的输出了,期间就存在两次量化的过程</p>
<ol>
<li>将候选框量化为整数点坐标值</li>
<li>将量化后的边界区域分割成k x k个单元（bin),对每一个单元的边界进行量化。</li>
</ol>
<p>RoIPooling 采用的是 INTER_NEAREST（即最近邻插值） ，即在resize时，对于 缩放后坐标不能刚好为整数 的情况，采用了 粗暴的四舍五入，相当于选取离目标点最近的点。，经过这样两次量化就出现了边界不匹配的问题了(misaligment),候选框和最开始回归出来的已经有很大偏差了。</p>
<h4 id="RoIAlign-1"><a href="#RoIAlign-1" class="headerlink" title="RoIAlign"></a>RoIAlign</h4><p>Mask R-CNN将最邻近插值换成了双线性插值，这样就有了RoIAlign，主要流程为：</p>
<ol>
<li>遍历每一个候选区域，保持浮点数边界不做量化。</li>
<li>将候选区域分割成k x k个单元，每个单元的边界也不做量化。</li>
<li>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</li>
</ol>
<p>第三步的操作，论文也说的很清楚，这个固定位置指的是每一个单元(bin)中按照固定规则确定的位置，比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。下图的例子中，虚线为特征图，实线为RoI，这里假设RoI分割成2 x 2的单元，在每个单元换分为4个小方块后，每个小方块的中心作为采样点，即图中的点，但是这些点的坐标一般来说是浮点数，采用双线性插值的方法来获得像素值，这样就不存在量化过程，很好地解决了misAligment问题。</p>
<p><img src="/img/Mask_R-CNN/figure_4.png" alt=""></p>
<h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p>Mask R-CNN使用”网络-深度-特征输出层”的方式命名底下层卷积网络。我们评估了深度为50或101层的ResNet25和ResNeXt26网络。使用ResNet的Faster R-CNN从第四阶段的最终卷积层提取特征，我们称之为C4。例如，使用ResNet-50的下层网络由ResNet-50-C4表示。<br>Mask R-CNN扩展了 ResNet和FPN中提出的Faster R-CNN的上层网络。详细信息如下图所示：（上层网络架构：我们扩展了两种现有的Faster R-CNN上层网络架构，分别添加了一个掩码分支。图中数字表示分辨率和通道数，箭头表示卷积、反卷积或全连接层（可以通过上下文推断，卷积减小维度，反卷积增加维度。）所有的卷积都是3×3的，除了输出层，是1×1的。反卷积是2×2的，步进为2，,隐藏层使用ReLU。左中，“res5”表示ResNet的第五阶段，简单起见，我们修改了第一个卷积操作，使用7×7，步长为1的RoI代替14×14，步长为2的RoI25。右图中的“×4”表示堆叠的4个连续的卷积。）</p>
<p><img src="/img/Mask_R-CNN/figure_5.png" alt=""></p>
<h2 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h2><p>主要参考来自于<a href="https://blog.csdn.net/horizonheart/article/details/81188161" target="_blank" rel="external">csdn一篇博客</a>，借用他的图。</p>
<p><img src="/img/Mask_R-CNN/flow_diagram.png" alt=""></p>
<h3 id="backbone-network"><a href="#backbone-network" class="headerlink" title="backbone network"></a>backbone network</h3><p>使用resnet101作为特征提取网络，生成金字塔网络，并在每层提取特征。</p>
<pre><code># Build the shared convolutional layers.
# Bottom-up Layers
# Returns a list of the last layers of each stage, 5 in total.
# Don&apos;t create the thead (stage 5), so we pick the 4th item in the list.
if callable(config.BACKBONE):
    _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,
                                        train_bn=config.TRAIN_BN)
else:
    _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,
                                     stage5=True, train_bn=config.TRAIN_BN)
# Top-down Layers
# TODO: add assert to varify feature map sizes match what&apos;s in config
# FPN：把底层的特征和高层的特征进行融合，便于细致检测。
# 这里P5=C5，然后P4=P5+C4,P2 P3类似，最终得到rpn_feature_maps，注意这里多了个P6,其仅是由P5下采样获得。
P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&apos;fpn_c5p5&apos;)(C5)
P4 = KL.Add(name=&quot;fpn_p4add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p5upsampled&quot;)(P5),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&apos;fpn_c4p4&apos;)(C4)])
P3 = KL.Add(name=&quot;fpn_p3add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p4upsampled&quot;)(P4),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&apos;fpn_c3p3&apos;)(C3)])
P2 = KL.Add(name=&quot;fpn_p2add&quot;)([
    KL.UpSampling2D(size=(2, 2), name=&quot;fpn_p3upsampled&quot;)(P3),
    KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name=&apos;fpn_c2p2&apos;)(C2)])
# Attach 3x3 conv to all P layers to get the final feature maps.
P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p2&quot;)(P2)
P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p3&quot;)(P3)
P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p4&quot;)(P4)
P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=&quot;SAME&quot;, name=&quot;fpn_p5&quot;)(P5)
# P6 is used for the 5th anchor scale in RPN. Generated by
# subsampling from P5 with stride of 2.
P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=&quot;fpn_p6&quot;)(P5)

# Note that P6 is used in RPN, but not in the classifier heads.
rpn_feature_maps = [P2, P3, P4, P5, P6]
mrcnn_feature_maps = [P2, P3, P4, P5]
</code></pre><h3 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h3><p>在前面金字塔特征图的基础上，生成anchor</p>
<pre><code># Anchors
# 如果是训练的情况，就在金字塔特征图上生成Anchor
if mode == &quot;training&quot;:
    # 在金字塔特征图上以每个像素为中心，以配置文件的anchor大小为宽高生成anchor
    # 根据特征图相应原图缩小的比例，还原到原始的输入图片上，即得到的是anchor在原始图片上的坐标
    # 获得结果为(N,[y1,x1,y2,x2])
    anchors = self.get_anchors(config.IMAGE_SHAPE)
    # Duplicate across the batch dimension because Keras requires it
    # TODO: can this be optimized to avoid duplicating the anchors?
    anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)
    # A hack to get around Keras&apos;s bad support for constants
    anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=&quot;anchors&quot;)(input_image)
else:
    # 如果是Inference就是输入的Anchor
    anchors = input_anchors
</code></pre><h3 id="RPN-Model"><a href="#RPN-Model" class="headerlink" title="RPN Model"></a>RPN Model</h3><p>将金字塔特征图输入到RPN中，得到网络的分类（前景和背景两类）和bbox的回归值。</p>
<pre><code># RPN Model
# RPN主要实现2个功能：
# 1 &gt; box的前景色和背景色的分类
# 2 &gt; box框体的回归修正
rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,
                      len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)
# Loop through pyramid layers
layer_outputs = []  # list of lists
for p in rpn_feature_maps:
    layer_outputs.append(rpn([p]))
# Concatenate layer outputs
# Convert from list of lists of level outputs to list of lists
# of outputs across levels.
# e.g. [[a1, b1, c1], [a2, b2, c2]] =&gt; [[a1, a2], [b1, b2], [c1, c2]]
output_names = [&quot;rpn_class_logits&quot;, &quot;rpn_class&quot;, &quot;rpn_bbox&quot;]
outputs = list(zip(*layer_outputs))
outputs = [KL.Concatenate(axis=1, name=n)(list(o))
           for o, n in zip(outputs, output_names)]
# rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits(before softmax)
# rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.
# rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors.
rpn_class_logits, rpn_class, rpn_bbox = outputs
</code></pre><p>其中RPN网络也用keras做了实现，Builds a Keras model of the Region Proposal Network.It wraps the RPN graph so it can be used multiple times with shared weights.</p>
<pre><code> # Shared convolutional base of the RPN
shared = KL.Conv2D(512, (3, 3), padding=&apos;same&apos;, activation=&apos;relu&apos;,
                   strides=anchor_stride,
                   name=&apos;rpn_conv_shared&apos;)(feature_map)

# Anchor Score. [batch, height, width, anchors per location * 2].
x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding=&apos;valid&apos;,
              activation=&apos;linear&apos;, name=&apos;rpn_class_raw&apos;)(shared)

# Reshape to [batch, anchors, 2]
rpn_class_logits = KL.Lambda(
    lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)

# Softmax on last dimension of BG/FG.
rpn_probs = KL.Activation(
    &quot;softmax&quot;, name=&quot;rpn_class_xxx&quot;)(rpn_class_logits)

# Bounding box refinement. [batch, H, W, anchors per location * depth]
# where depth is [x, y, log(w), log(h)]
x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=&quot;valid&quot;,
              activation=&apos;linear&apos;, name=&apos;rpn_bbox_pred&apos;)(shared)

# Reshape to [batch, anchors, 4]
rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)

return [rpn_class_logits, rpn_probs, rpn_bbox]
</code></pre><p>代码主要实现是，在特征图上，用kernel_size为所需输出个数（如对分类，为2 * anchors_per_location），stride为1的卷积在特征图上进行卷积，得到RPN的输出。</p>
<h3 id="Generate-proposals"><a href="#Generate-proposals" class="headerlink" title="Generate proposals"></a>Generate proposals</h3><p>这部分网络主要是用来对anchor进行筛选，所谓proposal，主要步骤为：</p>
<pre><code># Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]
scores = inputs[0][:, :, 1]
# Box deltas [batch, num_rois, 4]
deltas = inputs[1]
deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4])
# Anchors
anchors = inputs[2]

# Improve performance by trimming to top anchors by score
# and doing the rest on the smaller subset.
pre_nms_limit = tf.minimum(self.config.PRE_NMS_LIMIT, tf.shape(anchors)[1])
ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True,
                 name=&quot;top_anchors&quot;).indices
scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y),
                           self.config.IMAGES_PER_GPU)
deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y),
                           self.config.IMAGES_PER_GPU)
pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x),
                            self.config.IMAGES_PER_GPU,
                            names=[&quot;pre_nms_anchors&quot;])

# Apply deltas to anchors to get refined anchors.
# [batch, N, (y1, x1, y2, x2)]
boxes = utils.batch_slice([pre_nms_anchors, deltas],
                          lambda x, y: apply_box_deltas_graph(x, y),
                          self.config.IMAGES_PER_GPU,
                          names=[&quot;refined_anchors&quot;])

# Clip to image boundaries. Since we&apos;re in normalized coordinates,
# clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]
window = np.array([0, 0, 1, 1], dtype=np.float32)
boxes = utils.batch_slice(boxes,
                          lambda x: clip_boxes_graph(x, window),
                          self.config.IMAGES_PER_GPU,
                          names=[&quot;refined_anchors_clipped&quot;])

# Filter out small boxes
# According to Xinlei Chen&apos;s paper, this reduces detection accuracy
# for small objects, so we&apos;re skipping it.

# Non-max suppression
def nms(boxes, scores):
    indices = tf.image.non_max_suppression(
        boxes, scores, self.proposal_count,
        self.nms_threshold, name=&quot;rpn_non_max_suppression&quot;)
    proposals = tf.gather(boxes, indices)
    # Pad if needed
    padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0)
    proposals = tf.pad(proposals, [(0, padding), (0, 0)])
    return proposals
proposals = utils.batch_slice([boxes, scores], nms,
                              self.config.IMAGES_PER_GPU)
return proposals
</code></pre><ol>
<li>按score得分排序，取前6000个</li>
<li>将rpn的输出应用到anchors进行修正</li>
<li>舍弃修正后边框超过归一化的0-1区间内的</li>
<li>用非极大值抑制的方法获取最后的anchor</li>
</ol>
<h3 id="Generate-detection-target"><a href="#Generate-detection-target" class="headerlink" title="Generate detection target"></a>Generate detection target</h3><p>训练的时候计算loss需要有target，这一步就是对剩下的anchor产生detection target，以便后续计算loss。主要计算的步骤为：</p>
<ol>
<li>计算proposal和gt_box之间的iou值，大于0.5则被认为是正样本，小于0.5，并且和crow box相交不大的为负样本</li>
<li>对负样本进行采样，保证正样本占有33%的比例，保证正负样本平衡</li>
<li>根据正样本和那个gt_box的iou最大来给正样本分配gt_box和gt_max,以便计算偏差</li>
</ol>
<h3 id="fpn-classifier-graph-amp-fpn-mask-graph"><a href="#fpn-classifier-graph-amp-fpn-mask-graph" class="headerlink" title="fpn classifier graph &amp;fpn mask graph"></a>fpn classifier graph &amp;fpn mask graph</h3><p>这部分为分类网络，当然还有一个并行的mask分支，分类使用的是mrcnn_feature_map，即前面的P2、P3、P4、P5。基本思路是先经过ROIAlign层（取代了RoIPooling），再经过两层卷积后连接两个全连接层分别输出class和box。fpn_mask_graph也是类似，针对mask部分，只不过不同的是，前者经过PyramidROIAlign得到的特征图是7x7大小的，二，而后者经过PyramidROIAlign得到的特征图大小是14x14.</p>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>loss包括5个部分组成，分别是rpn网络的两个损失：rpn_class_loss，计算前景和背景分类损失；rpn_bbox_loss，计算rpn_box损失,以及输出的class，box和mask的损失计算。</p>
<pre><code># Losses
rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=&quot;rpn_class_loss&quot;)(
    [input_rpn_match, rpn_class_logits])
rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=&quot;rpn_bbox_loss&quot;)(
    [input_rpn_bbox, input_rpn_match, rpn_bbox])
class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=&quot;mrcnn_class_loss&quot;)(
    [target_class_ids, mrcnn_class_logits, active_class_ids])
bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=&quot;mrcnn_bbox_loss&quot;)(
    [target_bbox, target_class_ids, mrcnn_bbox])
mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=&quot;mrcnn_mask_loss&quot;)(
    [target_mask, target_class_ids, mrcnn_mask])
</code></pre><h3 id="PyramidROIAlign"><a href="#PyramidROIAlign" class="headerlink" title="PyramidROIAlign"></a>PyramidROIAlign</h3><p>PyramidROIAlign输入时金字塔特征图，所以首先需要确认来自于哪一层，作者的计算方法采用如下公式</p>
<p><img src="/img/Mask_R-CNN/PyramidROIAlign.png" alt=""></p>
<p>这里k0=4，从对应特征图中去除坐标对应区域，利用双线性插值进行pooling，这里作者依据论文中的，虽然没有采用论文中的4个点采样的方法，但是采用了论文中提到的也非常有效的1个点采样的方法，而tf.crop_and_resize这个函数crops and resizes an image and handles the bilinear interpolation，所以用这个进行了实现。</p>
<pre><code>    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords
boxes = inputs[0]

# Image meta
# Holds details about the image. See compose_image_meta()
image_meta = inputs[1]

# Feature Maps. List of feature maps from different level of the
# feature pyramid. Each is [batch, height, width, channels]
feature_maps = inputs[2:]

# Assign each ROI to a level in the pyramid based on the ROI area.
y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)
h = y2 - y1
w = x2 - x1
# Use shape of first image. Images in a batch must have the same size.
image_shape = parse_image_meta_graph(image_meta)[&apos;image_shape&apos;][0]
# Equation 1 in the Feature Pyramid Networks paper. Account for
# the fact that our coordinates are normalized here.
# e.g. a 224x224 ROI (in pixels) maps to P4
image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)
roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))
roi_level = tf.minimum(5, tf.maximum(
    2, 4 + tf.cast(tf.round(roi_level), tf.int32)))
roi_level = tf.squeeze(roi_level, 2)

# Loop through levels and apply ROI pooling to each. P2 to P5.
pooled = []
box_to_level = []
for i, level in enumerate(range(2, 6)):
    ix = tf.where(tf.equal(roi_level, level))
    level_boxes = tf.gather_nd(boxes, ix)

    # Box indices for crop_and_resize.
    box_indices = tf.cast(ix[:, 0], tf.int32)

    # Keep track of which box is mapped to which level
    box_to_level.append(ix)

    # Stop gradient propogation to ROI proposals
    level_boxes = tf.stop_gradient(level_boxes)
    box_indices = tf.stop_gradient(box_indices)

    # Crop and Resize
    # From Mask R-CNN paper: &quot;We sample four regular locations, so
    # that we can evaluate either max or average pooling. In fact,
    # interpolating only a single value at each bin center (without
    # pooling) is nearly as effective.&quot;
    #
    # Here we use the simplified approach of a single value per bin,
    # which is how it&apos;s done in tf.crop_and_resize()
    # Result: [batch * num_boxes, pool_height, pool_width, channels]
    pooled.append(tf.image.crop_and_resize(
        feature_maps[i], level_boxes, box_indices, self.pool_shape,
        method=&quot;bilinear&quot;))

# Pack pooled features into one tensor
pooled = tf.concat(pooled, axis=0)

# Pack box_to_level mapping into one array and add another
# column representing the order of pooled boxes
box_to_level = tf.concat(box_to_level, axis=0)
box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)
box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],
                         axis=1)

# Rearrange pooled features to match the order of the original boxes
# Sort box_to_level by batch then box index
# TF doesn&apos;t have a way to sort by two columns, so merge them and sort.
sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]
ix = tf.nn.top_k(sorting_tensor, k=tf.shape(
    box_to_level)[0]).indices[::-1]
ix = tf.gather(box_to_level[:, 2], ix)
pooled = tf.gather(pooled, ix)

# Re-add the batch dimension
pooled = tf.expand_dims(pooled, 0)
return pooled
</code></pre><h3 id="build-rpn-targets"><a href="#build-rpn-targets" class="headerlink" title="build_rpn_targets"></a>build_rpn_targets</h3><p>从loss可以看到，训练的时候，rpn的loss输入需要有target，代码中为rpn_match和rpn_box,计算方法主要也是根据金字塔的给定anchors和gt_box的iou，此处阈值为0.7，来确定postive和negative，并分配对应的gt_box来计算delta。</p>
<pre><code> &quot;&quot;&quot;Given the anchors and GT boxes, compute overlaps and identify positive
anchors and deltas to refine them to match their corresponding GT boxes.

anchors: [num_anchors, (y1, x1, y2, x2)]
gt_class_ids: [num_gt_boxes] Integer class IDs.
gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]

Returns:
rpn_match: [N] (int32) matches between anchors and GT boxes.
           1 = positive anchor, -1 = negative anchor, 0 = neutral
rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
&quot;&quot;&quot;
# RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
# RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

# Handle COCO crowds
# A crowd box in COCO is a bounding box around several instances. Exclude
# them from training. A crowd box is given a negative class ID.
crowd_ix = np.where(gt_class_ids &lt; 0)[0]
if crowd_ix.shape[0] &gt; 0:
    # Filter out crowds from ground truth class IDs and boxes
    non_crowd_ix = np.where(gt_class_ids &gt; 0)[0]
    crowd_boxes = gt_boxes[crowd_ix]
    gt_class_ids = gt_class_ids[non_crowd_ix]
    gt_boxes = gt_boxes[non_crowd_ix]
    # Compute overlaps with crowd boxes [anchors, crowds]
    crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
    crowd_iou_max = np.amax(crowd_overlaps, axis=1)
    no_crowd_bool = (crowd_iou_max &lt; 0.001)
else:
    # All anchors don&apos;t intersect a crowd
    no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

# Compute overlaps [num_anchors, num_gt_boxes]
overlaps = utils.compute_overlaps(anchors, gt_boxes)

# Match anchors to GT Boxes
# If an anchor overlaps a GT box with IoU &gt;= 0.7 then it&apos;s positive.
# If an anchor overlaps a GT box with IoU &lt; 0.3 then it&apos;s negative.
# Neutral anchors are those that don&apos;t match the conditions above,
# and they don&apos;t influence the loss function.
# However, don&apos;t keep any GT box unmatched (rare, but happens). Instead,
# match it to the closest anchor (even if its max IoU is &lt; 0.3).
#
# 1. Set negative anchors first. They get overwritten below if a GT box is
# matched to them. Skip boxes in crowd areas.
anchor_iou_argmax = np.argmax(overlaps, axis=1)
anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
rpn_match[(anchor_iou_max &lt; 0.3) &amp; (no_crowd_bool)] = -1
# 2. Set an anchor for each GT box (regardless of IoU value).
# TODO: If multiple anchors have the same IoU match all of them
gt_iou_argmax = np.argmax(overlaps, axis=0)
rpn_match[gt_iou_argmax] = 1
# 3. Set anchors with high overlap as positive.
rpn_match[anchor_iou_max &gt;= 0.7] = 1

# Subsample to balance positive and negative anchors
# Don&apos;t let positives be more than half the anchors
ids = np.where(rpn_match == 1)[0]
extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
if extra &gt; 0:
    # Reset the extra ones to neutral
    ids = np.random.choice(ids, extra, replace=False)
    rpn_match[ids] = 0
# Same for negative proposals
ids = np.where(rpn_match == -1)[0]
extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -
                    np.sum(rpn_match == 1))
if extra &gt; 0:
    # Rest the extra ones to neutral
    ids = np.random.choice(ids, extra, replace=False)
    rpn_match[ids] = 0

# For positive anchors, compute shift and scale needed to transform them
# to match the corresponding GT boxes.
ids = np.where(rpn_match == 1)[0]
ix = 0  # index into rpn_bbox
# TODO: use box_refinement() rather than duplicating the code here
for i, a in zip(ids, anchors[ids]):
    # Closest gt box (it might have IoU &lt; 0.7)
    gt = gt_boxes[anchor_iou_argmax[i]]

    # Convert coordinates to center plus width/height.
    # GT Box
    gt_h = gt[2] - gt[0]
    gt_w = gt[3] - gt[1]
    gt_center_y = gt[0] + 0.5 * gt_h
    gt_center_x = gt[1] + 0.5 * gt_w
    # Anchor
    a_h = a[2] - a[0]
    a_w = a[3] - a[1]
    a_center_y = a[0] + 0.5 * a_h
    a_center_x = a[1] + 0.5 * a_w

    # Compute the bbox refinement that the RPN should predict.
    rpn_bbox[ix] = [
        (gt_center_y - a_center_y) / a_h,
        (gt_center_x - a_center_x) / a_w,
        np.log(gt_h / a_h),
        np.log(gt_w / a_w),
    ]
    # Normalize
    rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
    ix += 1

return rpn_match, rpn_bbox
</code></pre><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><p>可以看到,计算RPN的时候的RPN loss和分类loss其实原始的输入都需要gt_box，只不过训练好之后，分类loss是在RPN的基础上。作者代码将RPN和后面的faster RCNN部分以及增加的mask 分支一起训练，所以在训练代码中加了下面一段：</p>
<pre><code># Stop gradient propogation to ROI proposals
level_boxes = tf.stop_gradient(level_boxes)
box_indices = tf.stop_gradient(box_indices)
</code></pre><p>引用官方的解释，主要是为了不让两部分互相影响。<br>If we don’t stop the gradients, TensorFlow will try to compute the gradients all the way back to the code that generates the anchor box refinement. But we already handle learning the anchor refinement in the RPN, so we don’t want to influence that with additional gradients from stage 2. So, the sooner we stop it, the more unnecessary computation we avoid. Further, it’s not clear (at least I haven’t looked into it) how the gradients calculation back through crop_and_resize works.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文链接：&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://arxiv.org/abs/1703.06870&lt;/a&gt;&lt;br&gt;源码链接：&lt;a href=&quot;https://github.com/matterport/Mask_RCNN&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/matterport/Mask_RCNN&lt;/a&gt;&lt;br&gt;之前一直在做目标检测这块，最近了解了一下实例分割，其实是目标检测更细致的任务，在图像中做到像素级的分割，包括目标检测和准确的像素分割，所以说是结合了之前目标检测的任务（classification and localization），以及语义分割（将像素点分类到特定的所属类别），首先拜读的就是17年何凯明大神的论文&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Mask R-CNN&lt;/a&gt;，并且阅读了keras版本的实现&lt;a href=&quot;https://github.com/matterport/Mask_RCNN&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;代码&lt;/a&gt;，在此做一个学习笔记，&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
      <category term="实例分割" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>Synthesize image for Chinese text recognition</title>
    <link href="http://yoursite.com/2018/09/12/Synthesize%20image%20for%20Chinese%20text%20recognition/"/>
    <id>http://yoursite.com/2018/09/12/Synthesize image for Chinese text recognition/</id>
    <published>2018-09-12T07:49:30.000Z</published>
    <updated>2018-09-13T08:11:07.917Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出<a href="http://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="external">SynthText</a>方法合成自然场景下的文本图片，github上有作者给出的<a href="https://github.com/ankush-me/SynthText" target="_blank" rel="external">官方代码</a>，也有国内大神改写的<a href="https://github.com/JarveeLee/SynthText_Chinese_version" target="_blank" rel="external">中文版本代码</a>。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。</p>
<a id="more"></a>
<h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><p>借鉴了SynthText的方法，而且包括语料库、图像背景图、字体、以及色彩模型文件，都是来源于@JarveeLee的中文版代码中的文件。</p>
<ol>
<li>读取语料库，此处来源为一些童话故事txt，</li>
<li>随机取一段字符串，满足所需长度，再随机选择字体、字号大小</li>
<li>在提供的背景图中，随机取一张图，计算裁剪图的Lab值标准差（标准差越小图像色彩分布就不会太过丰富、太过花哨），小于设定的阈值则再根据字体字号计算出的文本尺寸，在原图上进行随机裁剪，可以以一定概率使文本在最终图片中有一定偏移；可以以一定概率随机产生竖直文本。</li>
<li>通过聚类的方法，分析裁剪后图的色彩分布，在色彩模型提供的色彩库中选择与当前裁剪图像色彩偏差大的作为文本颜色，这样最终构成合成图片</li>
</ol>
<h2 id="构建方法"><a href="#构建方法" class="headerlink" title="构建方法"></a>构建方法</h2><p>主要实现代码只有一个文件，其他都是合成需要的文件，合成命令：</p>
<pre><code>python gen_dataset.py
</code></pre><p>newsgroup:文本来源的语料<br>models/colors_new.cp:从III-5K数据集学习到的色彩模型<br>fonts：包含合成时所需字体<br>所需图片bg_img来源于VGG组合成synth_80k时所用的图片集</p>
<ul>
<li>bg_img.tar.gz [8.9G]：压缩的图像文件（需要使用使用imnames.cp中的过滤），链接<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz" title="http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/bg_img.tar.gz</a></li>
<li>imnames.cp[180K]：已过滤文件的名称，即，这些文件不包含文本,链接：<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" title="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp</a></li>
</ul>
<h2 id="一些实现结果样例"><a href="#一些实现结果样例" class="headerlink" title="一些实现结果样例"></a>一些实现结果样例</h2><p><img src="/img/img_1.jpg" alt=""></p>
<p><img src="/img/img_2.jpg" alt=""></p>
<p><img src="/img/img_3.jpg" alt=""></p>
<p><img src="/img/img_4.jpg" alt=""></p>
<p>详细实现代码可参见个人<a href="https://github.com/lkj1114889770/Synth_Chinese_OCR_dataset" target="_blank" rel="external">github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/scenetext/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SynthText&lt;/a&gt;方法合成自然场景下的文本图片，github上有作者给出的&lt;a href=&quot;https://github.com/ankush-me/SynthText&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方代码&lt;/a&gt;，也有国内大神改写的&lt;a href=&quot;https://github.com/JarveeLee/SynthText_Chinese_version&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;中文版本代码&lt;/a&gt;。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow输入数据处理框架</title>
    <link href="http://yoursite.com/2018/07/25/Tensorflow%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <id>http://yoursite.com/2018/07/25/Tensorflow输入数据处理框架/</id>
    <published>2018-07-25T13:55:36.000Z</published>
    <updated>2018-07-25T14:45:55.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。</p>
<a id="more"></a>
<h2 id="TFrecords格式介绍"><a href="#TFrecords格式介绍" class="headerlink" title="TFrecords格式介绍"></a>TFrecords格式介绍</h2><p>TFrecords是一种二进制文件，通过tf.train.Example Protocol Buffer的格式存储数据，以下的代码给出了tf.train.Example的定义。</p>
<pre><code>message Example {
    Features features = 1;
};
message Features {
    map&lt;string, Feature&gt; feature = 1;
};
message Feature {
    oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
}
};
</code></pre><p>tf.train.Example包含了一个从属性名称到取值的字典，其中属性名称为一个字符串，属性取值可以是字符串(BytesList)，实数列表(FloatList)或者整数列表(Int64List），比如将解码前的图像存为一个字符串，将lable存为整数列表，或者将bounding box存为实数列表。</p>
<h2 id="COCO数据集的TFrecords文件制作"><a href="#COCO数据集的TFrecords文件制作" class="headerlink" title="COCO数据集的TFrecords文件制作"></a>COCO数据集的TFrecords文件制作</h2><p>COCO数据集是微软做的一个比较大的数据集，可以用来做图像的recognition、segmentation、captioning，我用来做物体检测识别。官方也提供了API操作数据集（<a href="https://github.com/cocodataset/cocoapi" title="https://github.com/cocodataset/cocoapi" target="_blank" rel="external">https://github.com/cocodataset/cocoapi</a>）。根据链接介绍下载安装python的API后，就可以开始Tfrecords的文件制作了。</p>
<pre><code>from pycocotools.coco import COCO
import tensorflow as tf
import numpy as np
from PIL import Image
from time import time
import os

dataDir=&apos;/home/zju/lkj/data/COCO Dataset&apos;
dataType=&apos;train2017&apos;
annFile=&apos;{}/annotations/instances_{}.json&apos;.format(dataDir,dataType)

classes = [&apos;backpack&apos;, &apos;umbrella&apos;, &apos;handbag&apos;, &apos;tie&apos;, &apos;suitcase&apos;, &apos;bottle&apos;, &apos;wine glass&apos;, &apos;cup&apos;, &apos;fork&apos;, &apos;knife&apos;,
            &apos;spoon&apos;, &apos;bowl&apos;, &apos;banana&apos;, &apos;apple&apos;, &apos;sandwich&apos;, &apos;orange&apos;, &apos;broccoli&apos;, &apos;carrot&apos;, &apos;hot dog&apos;,
            &apos;donut&apos;, &apos;cake&apos;, &apos;chair&apos;, &apos;couch&apos;, &apos;potted plant&apos;, &apos;bed&apos;, &apos;dining table&apos;, &apos;toilet&apos;, &apos;tv&apos;, &apos;laptop&apos;,
            &apos;mouse&apos;, &apos;remote&apos;, &apos;keyboard&apos;, &apos;cell phone&apos;, &apos;microwave&apos;, &apos;oven&apos;, &apos;toaster&apos;, &apos;sink&apos;, &apos;refrigerator&apos;,
            &apos;book&apos;, &apos;clock&apos;, &apos;vase&apos;, &apos;scissors&apos;, &apos;teddy bear&apos;, &apos;hair drier&apos;, &apos;toothbrush&apos;]

# initialize COCO api for instance annotations
coco = COCO(annFile)
classesId = coco.getCatIds(classes)
imgIds = coco.getImgIds()
img_filters=[]
for imgId in imgIds:
    Anns = coco.loadAnns(coco.getAnnIds(imgIds=imgId))
    annIds = list(map(lambda x:x[&apos;category_id&apos;],Anns))
    for annId in annIds:
        if annId in classesId:
            img_filters.append(imgId)
img_filters = set(img_filters)


# 归一化
# size: 图片大小
# box：[x,y,w,h]
# return 归一化结果
def convert(size,box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = box[0]+box[2]/2.0
    y = box[1]+box[3]/2.0
    x = x*dw
    w = box[2]*dw
    y = y*dh
    h = box[3]*dh
    return [x,y,w,h]


def convert_img(img_id):
    img_id_str = str(img_id).zfill(12)
    img_path = &apos;{}/{}/{}.jpg&apos;.format(dataDir, dataType, img_id_str)
    image = Image.open(img_path)
    resized_image = image.resize((416, 416), Image.BICUBIC)
    image_data = np.array(resized_image, dtype=&apos;float32&apos;) / 255
    if image_data.size != 519168: # 不为3通道
        return False
    img_raw = image_data.tobytes()
    return img_raw

def convert_annotation(image_id):
    img_info = coco.loadImgs(image_id)[0]  # 读入的是照片的详细信息，而非图像信息, 返回的是list，只有1个id输入时，取0
    w = int(img_info[&apos;width&apos;])
    h = int(img_info[&apos;height&apos;])
    bboxes = []
    Anns = coco.loadAnns(ids=coco.getAnnIds(imgIds=image_id))
    i = 0
    for Ann in Anns:
        if i&gt;29:
            break
        iscrowd = Ann[&apos;iscrowd&apos;]
        if iscrowd == 1:
            continue
        if Ann[&apos;category_id&apos;] not in classesId:
            continue
        cls_id = classesId.index(Ann[&apos;category_id&apos;])  # 取新的编号
        bbox = Ann[&apos;bbox&apos;]
        bb = convert((w, h), bbox) + [cls_id]
        bboxes.extend(bb)
        i = i + 1

    if len(bboxes) &lt; 30*5:
        bboxes = bboxes + [0, 0, 0, 0, 0]*(30-int(len(bboxes)/5))
    return np.array(bboxes, dtype=np.float32).flatten().tolist()

filename = os.path.join(&apos;train2017&apos;+&apos;.tfrecords&apos;)
writer = tf.python_io.TFRecordWriter(filename)
i=0
start = time()
for imgId in img_filters:
    xywhc = convert_annotation(imgId)
    img_raw = convert_img(imgId)
    if img_raw:
        example = tf.train.Example(features=tf.train.Features(feature={
            &apos;xywhc&apos;:
                    tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
            &apos;img&apos;:
                    tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
            }))
        writer.write(example.SerializeToString())
        # 显示制作进度，剩余时间
        if i%100==99:
            t = time()-start
            print(i,&apos;t={:0.4f}s/100 step&apos;.format(t),&apos;  left time={:0.4f}s&apos;.format((len(img_filters)-i)*t/100))
            start = time()
        i = i+1
print(&apos;Done!&apos;)
writer.close()
</code></pre><p>下面分段对代码进行介绍，这个数据制作是应用于物品检查与分割，并且只有部分物品，所以在程序开头有classes列举（总共45种，完整的COCO数据集包含91种）。COCO数据集中混有灰度图，所以在reshape的时候会一直报错，刚开始还一直想不清楚为什么，后来遍历原始数据集才发现有灰度图的存在，所以reshape成416*416*3会报错,所以程序有一个判断是否为3通道：</p>
<pre><code>image = Image.open(img_path)
resized_image = image.resize((416, 416), Image.BICUBIC)
image_data = np.array(resized_image, dtype=&apos;float32&apos;) / 255
if image_data.size != 519168: # 不为3通道
    return False
</code></pre><p>图像读取后转换成字符串(BytesList):</p>
<pre><code>img_raw = image_data.tobytes()
</code></pre><p>bounding box转换成实数列表(FloatList):</p>
<pre><code>return np.array(bboxes, dtype=np.float32).flatten().tolist()
</code></pre><p>基于此，核心的构建部分为：</p>
<pre><code>example = tf.train.Example(features=tf.train.Features(feature={
    &apos;xywhc&apos;:
            tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
    &apos;img&apos;:
            tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
    }))
writer.write(example.SerializeToString())
</code></pre><h2 id="TFrecords文件读取解析"><a href="#TFrecords文件读取解析" class="headerlink" title="TFrecords文件读取解析"></a>TFrecords文件读取解析</h2><p>对应构建时候的数据格式，进行解析，可以加一些程序对于读取后的图像文件的一些进一步处理，比如图像增强</p>
<pre><code>def parser(example):
    features = {
                &apos;xywhc&apos;: tf.FixedLenFeature([150], tf.float32),
                &apos;img&apos;: tf.FixedLenFeature((), tf.string)}
    feats = tf.parse_single_example(example, features)
    coord = feats[&apos;xywhc&apos;]
    coord = tf.reshape(coord, [30, 5])

    img = tf.decode_raw(feats[&apos;img&apos;], tf.float32)
    img = tf.reshape(img, [416, 416, 3])
    img = tf.image.resize_images(img, [cfg.train.image_resized, cfg.train.image_resized])
    rnd = tf.less(tf.random_uniform(shape=[], minval=0, maxval=2), 1)
    # 添加对于读取后的图像文件的一些进一步处理，图像增强
    def flip_img_coord(_img, _coord):
        zeros = tf.constant([[0, 0, 0, 0, 0]]*30, tf.float32)
        img_flipped = tf.image.flip_left_right(_img)
        idx_invalid = tf.reduce_all(tf.equal(coord, 0), axis=-1)
        coord_temp = tf.concat([tf.minimum(tf.maximum(1 - _coord[:, :1], 0), 1),
                               _coord[:, 1:]], axis=-1)
        coord_flipped = tf.where(idx_invalid, zeros, coord_temp)
        return img_flipped, coord_flipped

    img, coord = tf.cond(rnd, lambda: (tf.identity(img), tf.identity(coord)), lambda: flip_img_coord(img, coord))

    img = tf.image.random_hue(img, max_delta=0.1)
    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)
    img = tf.image.random_brightness(img, max_delta=0.1)
    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)
    img = tf.minimum(img, 1.0)
    img = tf.maximum(img, 0.0)
    return img, coord
</code></pre><p>然后构建一个data_pipeline来作为训练数据的输入框架：</p>
<pre><code>def data_pipeline(file_tfrecords, batch_size):
    dt = tf.data.TFRecordDataset(file_tfrecords)
    dt = dt.map(parser, num_parallel_calls=4)
    dt = dt.prefetch(batch_size)
    dt = dt.shuffle(buffer_size=20*batch_size)
    dt = dt.repeat()
    dt = dt.batch(batch_size)
    iterator = dt.make_one_shot_iterator()
    imgs, true_boxes = iterator.get_next()

    return imgs, true_boxes
</code></pre><p>测试一下整个数据输入模块：</p>
<pre><code>file_path = &apos;train2007.tfrecords&apos;
imgs, true_boxes = data_pipeline(file_path, cfg.batch_size)
sess = tf.Session()
imgs_, true_boxes_ = sess.run([imgs, true_boxes])
print(imgs_.shape, true_boxes_.shape)
for imgs_i, boxes_ in zip(imgs_, true_boxes_):
    valid = (np.sum(boxes_, axis=-1) &gt; 0).tolist()
    print([cfg.names[int(idx)] for idx in boxes_[:, 4][valid].tolist()])
    plt.figure()
    plt.imshow(imgs_i)
plt.show()
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>表扬一下pycharm</title>
    <link href="http://yoursite.com/2018/07/07/%E8%A1%A8%E6%89%AC%E4%B8%80%E4%B8%8Bpycharm/"/>
    <id>http://yoursite.com/2018/07/07/表扬一下pycharm/</id>
    <published>2018-07-07T15:43:06.000Z</published>
    <updated>2019-03-19T08:00:08.870Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。</p>
<a id="more"></a>
<p><img src="img/pycharm/figure1.png" alt=""></p>
<p>炼丹过程中参数修改频繁，还忘了备份已经效果比较好的参数，结果改的调不回去了，本来还有个理想的结果，现在越来越差，真实欲哭无泪，直到发现了上面的那个功能，pycharm是真的优秀，按这样点开，就能发现一天的修改过程，如下图</p>
<p><img src="img/pycharm/figure2.png" alt=""></p>
<p>可以点开看到修改历史，和现有版本进行对比，还可以导出修改历史。嗯，真的是良心IDE，特此表扬，以资鼓励。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="杂" scheme="http://yoursite.com/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>目标检测网络之YOLO学习笔记</title>
    <link href="http://yoursite.com/2018/06/15/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C%E4%B9%8BYOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/06/15/目标检测网络之YOLO学习笔记/</id>
    <published>2018-06-15T06:59:34.000Z</published>
    <updated>2019-03-19T13:02:51.471Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。<br><a id="more"></a></p>
<p>YOLO项目主页<a href="https://pjreddie.com/yolo/" target="_blank" rel="external">地址</a><br>YOLO1 <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">论文</a><br>YOLO2 <a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">论文</a><br>YOLO3 <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">论文</a></p>
<h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><p>YOLO使用来自整张图片的feature map来预测bounding box和class，因此可以保持较高的精度。YOLO将整张图片分成S×S的网格，如果一个目标的中心落入到网格单元中，那么这个网格单元负责这个目标的检测。</p>
<div align="center"><br>    <img src="/img/yolo/yolo1.1.png" width="300" height="300"><br></div><br>每个网格单元预测B个bounding box和confidence score，confidence score反应了box包含目标的可信度，论文中将可confidence score定义为：<br><div align="center"><br>    <img src="/img/yolo/yolo1.2.png" height="30"><br></div><br>，因此，如果没有目标存在confidence score为0，否则应该为IOU(intersection over union)，即真实框和预测框的交集部分。所以每个bounding box的预测值包括(x,y,w,h.confidence score). (x,y)表示预测的box中心相对于网格单元的的位置，(w,h)是用整个图片大小进行归一化的宽度和高度，另外，针对C个类别，每个类别需要预测一个条件概率，即：<br><div align="center"><br>    <img src="/img/yolo/yolo1.3.png" height="30"><br></div><br>最终得到box中包含某个特定物品的概率为：<br><div align="center"><br>    <img src="/img/yolo/yolo1.4.png" height="40"><br></div><br>整个过程如下图所示。<br><div align="center"><br>    <img src="/img/yolo/yolo1.5.png" width="700" height="400"><br></div>

<p>总结来说，YOLO网络将检测问题转换成regression，首先将整张图片转换成S×S的网格，并且每个网格单元预测B个边界框，这些边界框的(x,y,w,h,confidence score)以及C个类别概率,这些预测被编码为S×S×(B*5+C)的张量。</p>
<h3 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h3><p>YOLO1的网络结构设计借鉴了GoogleNet模型，包含了24个卷积层和2个全连接层，YOLO未使用inception module，而是使用1x1卷积层和）3x3卷积层简单替代，交替出现的1x1卷积层实现了跨通道信息融合以及通道数目降低。</p>
<div align="center"><br>    <img src="/img/yolo/yolo1.6.png"><br></div>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ol>
<li>使用 ImageNet 1000 类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。</li>
<li>用上面得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，加入后面的4层卷积层以及2层全连接层进行detection的训练，detection通常需要有细密纹理的视觉信息,所以为提高图像精度，在训练检测模型时，将输入图像分辨率从224 × 224 resize到448x448。</li>
<li>最后一层预测类概率和边界框坐标。我们通过图像宽度和高度来规范(w,h)，使它们落在0和1之间。我们将边界框(x,y)坐标参数化为特定网格单元位置的偏移量，所以它们边界也在0和1之间。</li>
</ol>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>YOLO1的误差计算对于分类误差和定位误差用了不同的权重，对包含与不包含物品的box的误差权重也进行了区分。具体来说，论文中增加了边界框坐标预测损失，并减少了不包含目标边界框的置信度预测损失，使用两个参数λcoord和λnoobj来完成这个工作，论文中设置了λcoord=5和λnoobj=0.5。<br>另一个问题是平方和误差权重在大框和小框中进行了区分。相同的误差下，小框误差的重要性肯定更好，论文中用了一个很巧妙的方法，<strong>直接预测边界框宽度和高度的平方根，而不是宽度和高度</strong>。根据y=x^1/2的函数就可以知道，函数斜率是随着x的增大而减小的，这样就可以提高小框的误差权重，真的巧妙。<br>YOLO每个网格单元预测多个box。在训练时，每个目标我们只需要一个box来负责，选定的原则是与真实框具有最大的IOU。</p>
<div align="center"><br>    <img src="/img/yolo/yolo1.7.png"><br></div>

<h3 id="Shortcoming"><a href="#Shortcoming" class="headerlink" title="Shortcoming"></a>Shortcoming</h3><p>YOLO对边界框预测强加空间约束，因为每个网格单元只预测两个盒子，只能有一个类别。这个空间约束限制了我们的模型可以预测的邻近目标的数量，因此在小物品的检测上比较局限。</p>
<h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p>为提高物体定位精准性和召回率，YOLO2对网络结构的设计进行了改进，输出层使用卷积层替代YOLO的全连接层，联合使用coco物体检测标注数据和imagenet物体分类标注数据训练物体检测模型。相比YOLO，YOLO9000在识别种类、精度、速度、和定位准确性等方面都有大大提升。</p>
<h3 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>YOLO2取消了dropout，在所有的卷积层中加入Batch Normalization。</p>
<h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>YOLO2将ImageNet以448×448 的分辨率微调最初的分类网络，迭代10 epochs。</p>
<h4 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h4><p>借鉴faster R-CNN的思想，引入anchor box，取消全连接层来进行预测，改用卷积层作为预测层对anchor box的offset和confidence进行预测。去除了一个池化层，使得输出特征具有更高的分辨率，将图片输入尺寸resize为416而非448，使得特征图大小为奇数，所以有一个中心单元格。目标，特别是大目标，倾向于占据图像的中心，所以在中心有一个单一的位置可以很好的预测这些目标，而不是四个位置都在中心附近。YOLO的卷积层将图像下采样32倍，所以通过使用输入图像416，我们得到13×13的输出特征图。同时，使用anchor box进行预测的时候，解耦空间位置预测与类别预测，对每个anchor box都预测object和class，仍然沿用YOLO1，目标检测仍然是预测proposed box和ground truth的IOU，类别预测（class predictions）仍然是存在object下的条件概率。</p>
<h4 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h4><p>YOLO2不再采用手动挑选的box尺寸，而是对训练集的box尺寸进行k-means聚类，因为聚类的目的是想要更好的IOU，所以聚类的距离使用下列公式：</p>
<div align="center"><br>    <img src="/img/yolo/yolo2.1.png" height="40"><br></div><br>对不同的k值采用k-means聚类算法，即对数据集的ground truth聚类，在VOC和COCO数据集上的bounding box得到的结果如下图：<br><div align="center"><br>    <img src="/img/yolo/yolo2.2.png"><br></div>

<p>根据上图，k=5的时候，模型的复杂度和IOU能够得到一个不错的trade off。</p>
<h4 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h4><p>对于位置坐标，YOLO2没有采用R-CNN的预测偏移，而是仍然类似于YOLO1中的，他预测相对于网格单元的位置坐标，将ground truth也限制在0-1之间，使用logistic activation 来实现。网络为每个边界框预测tx，ty，th，tw和to这5个坐标。如果网格单元从图像的左上角偏移（Cx，Cy），给定的anchor的宽度，高度分别为Pw，Ph那么预测结果为：</p>
<div align="center"><br>    <img src="/img/yolo/yolo2.3.png"><br></div><br><div align="center"><br>    <img src="/img/yolo/yolo2.4.png"><br></div>

<h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>在13×13特征图上检测可以很容易检测到大目标，从更小粒度的特征图中可以更好地检测小物体，YOLO2添加一个passthrough layer从前一层26×26的特征图进行融合。传递层通过将相邻特征堆叠到不同的通道而不是堆叠到空间位置，将较高分辨率特征与低分辨率特征相连，类似于ResNet中的标识映射。这将26×26×512特征映射转换为13×13×2048特征映射，其可以与原始特征连接。</p>
<h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>添加anchor box后，YOLO2将分辨率更改为416×416。然而，由于模型只使用卷积层和池化层，它可以在运行中调整大小。为了使YOLOv2能够在不同大小的图像上运行，相比于固定输入图像大小，YOLO2每隔几次迭代更改网络。每迭代10个batch网络随机选择一个新的图像尺寸大小。因为模型以32的因子下采样，YOLO2从以下32的倍数中抽取：{320,352，…，608}。因此，最小的选项是320×320，最大的是608×608.调整网络的大小，并继续训练。<br>这种训练方法迫使网络学习在各种输入维度上很好地预测。这意味着相同的网络可以预测不同分辨率的检测。网络在更小的尺寸下运行更快，因此YOLO2在速度和精度之间提供了一个简单的折衷。</p>
<h3 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h3><h4 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h4><p>YOLO2大多数3×3的滤波器，并在每个池化步骤后将通道数量加倍，使用全局平均池化进行预测，使用1×1滤波器以压缩3×3卷积之间的特征，最终模型，称为Darknet-19，有19卷积层和5个最大池化层，详见下图。</p>
<div align="center"><br>    <img src="/img/yolo/yolo2.5.png"><br></div>

<h4 id="Training-for-classification"><a href="#Training-for-classification" class="headerlink" title="Training for classification"></a>Training for classification</h4><p>使用Darknet19在标准ImageNet 1000类分类数据集上训练，在训练期间，使用数据增强技巧。</p>
<h4 id="Training-for-detection"><a href="#Training-for-detection" class="headerlink" title="Training for detection"></a>Training for detection</h4><p>为了训练检测器，修改上面的网络，移除最后的卷积层，添加3个3×3卷积层，最后增加1×1卷积层，其输出为我们需要的检测维度，如对于VOC数据集，预测5个box，每个具有5个坐标，每个box20个类，因此125个过滤器。还添加了从最后的3×3×512层到第二到最后的卷积层的传递层passthrough layer，使得模型可以使用细粒度特征。</p>
<h3 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h3><p>构建了一种分层分类模型（WordTree），提出了一种关于分类和检测数据的联合训练机制。</p>
<div align="center"><br>    <img src="/img/yolo/yolo2.6.png"><br></div><br><div align="center"><br>    <img src="/img/yolo/yolo2.7.png"><br></div>

<p>ImageNet数据量更大，用于训练分类，COCO和VOC用于训练检测，ImageN对应分类有9000多种，COCO只有80种对应目标检测，通过wordTree来combine，来自分类的图片只计算分类的loss，来自检测集的图片计算完整的loss。</p>
<h2 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h2><p>YOLO3 对于YOLO2有了一些改进，总的来说有几点：加深了网络，用了上采样，残差网络，多尺度预测，下面详细说明。</p>
<h3 id="Bounding-Box-Prediction"><a href="#Bounding-Box-Prediction" class="headerlink" title="Bounding Box Prediction"></a>Bounding Box Prediction</h3><p>坐标预测仍然沿用YOLO2的，yolov3对每个bounding box预测四个坐标值(tx, ty, tw, th)，对于预测的cell根据图像左上角的偏移(cx, cy)，以及之前得到bounding box的宽和高pw, ph可以对bounding box按如下的方式进行预测：</p>
<div align="center"><br>    <img src="/img/yolo/yolo3.1.png"><br></div>

<p>训练的时候，loss的计算采用sum of squared error loss（平方和距离误差损失），yolov3对每个bounding box通过逻辑回归预测一个物体的得分，如果预测的这个bounding box与真实的边框值大部分重合且比其他所有预测的要好，那么这个值就为1.如果overlap没有达到一个阈值（yolov3中这里设定的阈值是0.5），那么这个预测的bounding box将会被忽略。YOLO3论文中使用的阈值是0.5.每个object只会分配一个bounding box，所以对应没有分配有ground truth object的box，其坐标损失和预测损失不需要计入，只需考虑objectness loss。If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.</p>
<h3 id="Class-Prediction"><a href="#Class-Prediction" class="headerlink" title="Class Prediction"></a>Class Prediction</h3><p>每个框预测分类，bounding box使用多标签分类（multi-label classification）。论文中说没有使用softmax分类，只是使用了简单的逻辑回归进行分类，采用的二值交叉熵损失（binary cross-entropy loss）。<br>Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.<br>This formulation helps when we move to more complex domains like the Open Images Dataset. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.</p>
<h3 id="Predictions-Across-Scales"><a href="#Predictions-Across-Scales" class="headerlink" title="Predictions Across Scales"></a>Predictions Across Scales</h3><p>YOLO3在三种不同尺度来预测box，应用一个类似于特征金字塔网络（feature pyramid network）上提取特征，如下图：</p>
<div align="center"><br>    <img src="/img/yolo/yolo3.2.png"><br></div>

<p>对于第一个scale的预测，即base feature extractor，最后预测得到一个3-d tensor，包含bounding box,objectness,class prediction.比如在COCO数据集中有80类物品，每一个scale预测3个box，所以tensor得到为（N×N×[3*(4+1+80)]）。<br>next scale，从上一步2 layer previous的feature map中进行上采样，然后从特征提取网络中的取earlier feature 与上采样后的进行合并，得到更多信息的语义，以及从earlier feature map可以得到更细粒度的特征。最后的scale采用前述类似的方法进行。可能实际代码更能体现这个过程，如下：<br>三种跨尺度预测</p>
<pre><code>predict boxes at 3 different scales
&apos;&apos;&apos;
def build(self, feat_ex, res18, res10):
    self.conv52 = self.conv_layer(feat_ex, 1, 1, 1024, 512, True, &apos;conv_head_52&apos;)          # 13x512
    self.conv53 = self.conv_layer(self.conv52, 3, 1, 512, 1024, True, &apos;conv_head_53&apos;)   # 13x1024
    self.conv54 = self.conv_layer(self.conv53, 1, 1, 1024, 512, True, &apos;conv_head_54&apos;)   # 13x512
    self.conv55 = self.conv_layer(self.conv54, 3, 1, 512, 1024, True, &apos;conv_head_55&apos;)   # 13x1024
    self.conv56 = self.conv_layer(self.conv55, 1, 1, 1024, 512, True, &apos;conv_head_56&apos;)   # 13x512
    self.conv57 = self.conv_layer(self.conv56, 3, 1, 512, 1024, True, &apos;conv_head_57&apos;)   # 13x1024
    self.conv58 = self.conv_layer(self.conv57, 1, 1, 1024, 75, False, &apos;conv_head_58&apos;)   # 13x75
    # follow yolo layer mask = 6,7,8
    self.conv59 = self.conv_layer(self.conv56, 1, 1, 512, 256, True, &apos;conv_head_59&apos;)    # 13x256
    size = tf.shape(self.conv59)[1]
    self.upsample0 = tf.image.resize_nearest_neighbor(self.conv59, [2*size, 2*size],    # 上采样
                                                      name=&apos;upsample_0&apos;)                # 26x256
    self.route0 = tf.concat([self.upsample0, res18], axis=-1, name=&apos;route_0&apos;)           # 26x768
    self.conv60 = self.conv_layer(self.route0, 1, 1, 768, 256, True, &apos;conv_head_60&apos;)    # 26x256
    self.conv61 = self.conv_layer(self.conv60, 3, 1, 256, 512, True, &apos;conv_head_61&apos;)    # 26x512
    self.conv62 = self.conv_layer(self.conv61, 1, 1, 512, 256, True, &apos;conv_head_62&apos;)    # 26x256
    self.conv63 = self.conv_layer(self.conv62, 3, 1, 256, 512, True, &apos;conv_head_63&apos;)    # 26x512
    self.conv64 = self.conv_layer(self.conv63, 1, 1, 512, 256, True, &apos;conv_head_64&apos;)    # 26x256
    self.conv65 = self.conv_layer(self.conv64, 3, 1, 256, 512, True, &apos;conv_head_65&apos;)    # 26x512
    self.conv66 = self.conv_layer(self.conv65, 1, 1, 512, 75, False, &apos;conv_head_66&apos;)    # 26x75
    # follow yolo layer mask = 3,4,5
    self.conv67 = self.conv_layer(self.conv64, 1, 1, 256, 128, True, &apos;conv_head_67&apos;)    # 26x128
    size = tf.shape(self.conv67)[1]
    self.upsample1 = tf.image.resize_nearest_neighbor(self.conv67, [2 * size, 2 * size],
                                                      name=&apos;upsample_1&apos;)                # 52x128
    self.route1 = tf.concat([self.upsample1, res10], axis=-1, name=&apos;route_1&apos;)           # 52x384
    self.conv68 = self.conv_layer(self.route1, 1, 1, 384, 128, True, &apos;conv_head_68&apos;)    # 52x128
    self.conv69 = self.conv_layer(self.conv68, 3, 1, 128, 256, True, &apos;conv_head_69&apos;)    # 52x256
    self.conv70 = self.conv_layer(self.conv69, 1, 1, 256, 128, True, &apos;conv_head_70&apos;)    # 52x128
    self.conv71 = self.conv_layer(self.conv70, 3, 1, 128, 256, True, &apos;conv_head_71&apos;)    # 52x256
    self.conv72 = self.conv_layer(self.conv71, 1, 1, 256, 128, True, &apos;conv_head_72&apos;)    # 52x128
    self.conv73 = self.conv_layer(self.conv72, 3, 1, 128, 256, True, &apos;conv_head_73&apos;)    # 52x256
    self.conv74 = self.conv_layer(self.conv73, 1, 1, 256, 75, False, &apos;conv_head_74&apos;)    # 52x75
    # follow yolo layer mask = 0,1,2

    return self.conv74, self.conv66, self.conv58
</code></pre><p>上面是最后的预测部分，需要输入的三个特征从Darknet-53网络中得到的，输出地方做了注释，Darknet-53网络结构如下：</p>
<pre><code>def build(self, img, istraining, decay_bn=0.99):
   self.phase_train = istraining
   self.decay_bn = decay_bn
   self.conv0 = self.conv_layer(bottom=img, size=3, stride=1, in_channels=3,   # 416x3
                                out_channels=32, name=&apos;conv_0&apos;)                # 416x32
   self.conv1 = self.conv_layer(bottom=self.conv0, size=3, stride=2, in_channels=32,
                                out_channels=64, name=&apos;conv_1&apos;)                # 208x64
   self.conv2 = self.conv_layer(bottom=self.conv1, size=1, stride=1, in_channels=64,
                                out_channels=32, name=&apos;conv_2&apos;)                # 208x32
   self.conv3 = self.conv_layer(bottom=self.conv2, size=3, stride=1, in_channels=32,
                                out_channels=64, name=&apos;conv_3&apos;)                # 208x64
   self.res0 = self.conv3 + self.conv1                                         # 208x64
   self.conv4 = self.conv_layer(bottom=self.res0, size=3, stride=2, in_channels=64,
                                out_channels=128, name=&apos;conv_4&apos;)               # 104x128
   self.conv5 = self.conv_layer(bottom=self.conv4, size=1, stride=1, in_channels=128,
                                out_channels=64, name=&apos;conv_5&apos;)                # 104x64
   self.conv6 = self.conv_layer(bottom=self.conv5, size=3, stride=1, in_channels=64,
                                out_channels=128, name=&apos;conv_6&apos;)               # 104x128
   self.res1 = self.conv6 + self.conv4     # 128                               # 104x128
   self.conv7 = self.conv_layer(bottom=self.res1, size=1, stride=1, in_channels=128,
                                out_channels=64, name=&apos;conv_7&apos;)                # 104x64
   self.conv8 = self.conv_layer(bottom=self.conv7, size=3, stride=1, in_channels=64,
                                out_channels=128, name=&apos;conv_8&apos;)               # 104x128
   self.res2 = self.conv8 + self.res1      # 128                               # 104x128
   self.conv9 = self.conv_layer(bottom=self.res2, size=3, stride=2, in_channels=128,
                                out_channels=256, name=&apos;conv_9&apos;)               # 52x256
   self.conv10 = self.conv_layer(bottom=self.conv9, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_10&apos;)             # 52x128
   self.conv11 = self.conv_layer(bottom=self.conv10, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_11&apos;)             # 52x256
   self.res3 = self.conv11 + self.conv9                                        # 52x256
   self.conv12 = self.conv_layer(bottom=self.res3, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_12&apos;)             # 52x128
   self.conv13 = self.conv_layer(bottom=self.conv12, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_13&apos;)             # 52x256
   self.res4 = self.conv13 + self.res3                                         # 52x256
   self.conv14 = self.conv_layer(bottom=self.res4, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_14&apos;)             # 52x128
   self.conv15 = self.conv_layer(bottom=self.conv14, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_15&apos;)             # 52x256
   self.res5 = self.conv15 + self.res4                                         # 52x256
   self.conv16 = self.conv_layer(bottom=self.res5, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_16&apos;)             # 52x128
   self.conv17 = self.conv_layer(bottom=self.conv16, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_17&apos;)             # 52x256
   self.res6 = self.conv17 + self.res5                                         # 52x256
   self.conv18 = self.conv_layer(bottom=self.res6, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_18&apos;)             # 52x128
   self.conv19 = self.conv_layer(bottom=self.conv18, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_19&apos;)             # 52x256
   self.res7 = self.conv19 + self.res6                                         # 52x256
   self.conv20 = self.conv_layer(bottom=self.res7, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_20&apos;)             # 52x128
   self.conv21 = self.conv_layer(bottom=self.conv20, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_21&apos;)             # 52x256
   self.res8 = self.conv21 + self.res7                                         # 52x256
   self.conv22 = self.conv_layer(bottom=self.res8, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_22&apos;)             # 52x128
   self.conv23 = self.conv_layer(bottom=self.conv22, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_23&apos;)             # 52x256
   self.res9 = self.conv23 + self.res8                                         # 52x256
   self.conv24 = self.conv_layer(bottom=self.res9, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_24&apos;)             # 52x128
   self.conv25 = self.conv_layer(bottom=self.conv24, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_25&apos;)             # 52x256
   self.res10 = self.conv25 + self.res9                                        # 52x256 一个输出的特征尺度
   self.conv26 = self.conv_layer(bottom=self.res10, size=3, stride=2, in_channels=256,
                                 out_channels=512, name=&apos;conv_26&apos;)             # 26x512
   self.conv27 = self.conv_layer(bottom=self.conv26, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_27&apos;)             # 26x256
   self.conv28 = self.conv_layer(bottom=self.conv27, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_28&apos;)             # 26x512
   self.res11 = self.conv28 + self.conv26                                      # 26x512
   self.conv29 = self.conv_layer(bottom=self.res11, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_29&apos;)             # 26x256
   self.conv30 = self.conv_layer(bottom=self.conv29, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_30&apos;)             # 26x512
   self.res12 = self.conv30 + self.res11                                       # 26x512
   self.conv31 = self.conv_layer(bottom=self.res12, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_31&apos;)             # 26x256
   self.conv32 = self.conv_layer(bottom=self.conv31, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_32&apos;)             # 26x512
   self.res13 = self.conv32 + self.res12                                       # 26x512
   self.conv33 = self.conv_layer(bottom=self.res13, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_33&apos;)             # 26x256
   self.conv34 = self.conv_layer(bottom=self.conv33, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_34&apos;)             # 26x512
   self.res14 = self.conv34 + self.res13                                       # 26x512
   self.conv35 = self.conv_layer(bottom=self.res14, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_35&apos;)             # 26x256
   self.conv36 = self.conv_layer(bottom=self.conv35, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_36&apos;)             # 26x512
   self.res15 = self.conv36 + self.res14                                       # 26x512
   self.conv37 = self.conv_layer(bottom=self.res15, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_37&apos;)             # 26x256
   self.conv38 = self.conv_layer(bottom=self.conv37, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_38&apos;)             # 26x512
   self.res16 = self.conv38 + self.res15                                       # 26x512
   self.conv39 = self.conv_layer(bottom=self.res16, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_39&apos;)             # 26x256
   self.conv40 = self.conv_layer(bottom=self.conv39, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_40&apos;)             # 26x512
   self.res17 = self.conv40 + self.res16                                       # 26x512
   self.conv41 = self.conv_layer(bottom=self.res17, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_41&apos;)             # 26x256
   self.conv42 = self.conv_layer(bottom=self.conv41, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_42&apos;)             # 26x512
   self.res18 = self.conv42 + self.res17                                       # 26x512，一个输出的特征尺度
   self.conv43 = self.conv_layer(bottom=self.res18, size=3, stride=2, in_channels=512,
                                 out_channels=1024, name=&apos;conv_43&apos;)            # 13x1024
   self.conv44 = self.conv_layer(bottom=self.conv43, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_44&apos;)             # 13x512
   self.conv45 = self.conv_layer(bottom=self.conv44, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_45&apos;)            # 13x1024
   self.res19 = self.conv45 + self.conv43                                      # 13x1024
   self.conv46 = self.conv_layer(bottom=self.res19, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_46&apos;)             # 13x512
   self.conv47 = self.conv_layer(bottom=self.conv44, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_47&apos;)            # 13x1024
   self.res20 = self.conv47 + self.res19                                       # 13x1024
   self.conv48 = self.conv_layer(bottom=self.res20, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_48&apos;)             # 13x512
   self.conv49 = self.conv_layer(bottom=self.conv48, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_49&apos;)            # 13x1024
   self.res21 = self.conv49 + self.res20                                       # 13x1024
   self.conv50 = self.conv_layer(bottom=self.res21, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_50&apos;)             # 13x512
   self.conv51 = self.conv_layer(bottom=self.conv50, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_51&apos;)            # 13x1024
   self.res23 = self.conv51 + self.res21                                       # 13x1024
   return self.res23  # 最后输出特征
</code></pre><p>同样采用k-means聚类的到anchor box的尺寸。选取了9种，3中不同的scale：(10×13); (16×30); (33×23); (30×61); (62×45); (59×119); (116 × 90); (156 × 198); (373 × 326).</p>
<h3 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h3><p>YOLO3的新的更深的网络，Darknet-53，实现细节可参见上面的代码</p>
<div align="center"><br>    <img src="/img/yolo/yolo3.3.png"><br></div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>PCL的python库安装for Ubuntu16.04</title>
    <link href="http://yoursite.com/2018/05/11/PCL%E7%9A%84python%E5%BA%93%E5%AE%89%E8%A3%85for-Ubuntu16-04/"/>
    <id>http://yoursite.com/2018/05/11/PCL的python库安装for-Ubuntu16-04/</id>
    <published>2018-05-11T11:43:04.000Z</published>
    <updated>2019-03-19T08:23:18.389Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库<a href="https://github.com/strawlab/python-pcl" target="_blank" rel="external">python_pcl实现库</a> ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。</p>
<a id="more"></a>
<h2 id="PCL安装"><a href="#PCL安装" class="headerlink" title="PCL安装"></a>PCL安装</h2><p>不用编译源码，一行命令直接apt安装，顺带安装各种依赖的乱七八糟的库</p>
<pre><code>sudo apt-get install libpcl-dev 
</code></pre><p>再安装一些pcl可视化等软件包</p>
<pre><code>sudo apt-get install pcl_tools
</code></pre><h2 id="安装-python-pcl"><a href="#安装-python-pcl" class="headerlink" title="安装 python_pcl"></a>安装 python_pcl</h2><p>首先下载python_pcl源文件</p>
<pre><code>git clone https://github.com/strawlab/python-pcl.git
</code></pre><p>编译、安装</p>
<pre><code>python setup.py build_ext -i
python setup.py install
</code></pre><p>在此之前常出现的一个编译问题是cython版本问题，所以在执行上一步之前首先：</p>
<pre><code>pip install cython==0.25.2
</code></pre><h2 id="解决常出现的链接失败的问题"><a href="#解决常出现的链接失败的问题" class="headerlink" title="解决常出现的链接失败的问题"></a>解决常出现的链接失败的问题</h2><p>由于我的默认python为anaconda3的python，可能是anaconda3自带的链接库的问题，所以出现了如下错误：</p>
<pre><code>./lib/libgomp.so.1: version `GOMP_4.0&apos; not found (required by /home/lkj/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so) 
</code></pre><p>上面的意思是anaconda3/lib/libgomp.so.1中没有‘GOMP_4.0’，这个可以使用strings命令查看libgomp.so.1这个文件，显示并无4.0版本，因此寻找其他路径的链接库替代，用locate命令搜索系统中所有的libgomp.so.1，得到：<br><img src="/img/pcl/figure1.png" alt=""><br>然后用strings查看这些文件信息，</p>
<pre><code>/usr/lib/x86_64-linux-gnu/libgomp.so.1 |grep GOMP
</code></pre><p>发现x86_64-linux-gnu/libgomp.so.1包含GOMP_4.0<br><img src="/img/pcl/figure2.png" alt=""><br>因此可以删掉原有的libgomp.so.1，重新做一个新的链接。</p>
<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libgomp.so.1 libgomp.so.1 
</code></pre><p>然后再次在python里面import pcl,又提示libstdc++.so.6出现类似的问题，对上述做类似处理，如果还有链接库的问题，也可以用同样的方法处理,至此实现了python的pcl库安装。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库&lt;a href=&quot;https://github.com/strawlab/python-pcl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python_pcl实现库&lt;/a&gt; ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>基于深度学习的目标检测技术学习笔记(R-CNN系列)</title>
    <link href="http://yoursite.com/2018/04/20/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(R-CNN%E7%B3%BB%E5%88%97)/"/>
    <id>http://yoursite.com/2018/04/20/基于深度学习的目标检测技术学习笔记(R-CNN系列)/</id>
    <published>2018-04-20T03:03:17.000Z</published>
    <updated>2019-03-19T10:10:37.968Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。<br><a id="more"></a></p>
<p>传统的目标检测算法一般是基于滑动窗口选中图中的某一部分作为候选区域，然后提取候选区域的特征，利用分类器（如常见的SVM)进行识别。2014年提出的region proposal+CNN代替传统目标检测使用的滑动窗口+特征工程的方法，设计了R-CNN算法，开启了基于深度学习的目标检测的大门。</p>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="/img/rcnn/1.png" alt=""></p>
<p>R-CNN算法流程为：</p>
<ol>
<li>输入图像，根据SS（selective search）算法提取2000个左右的region proposal（候选框）</li>
<li>将候选框crop/wrap为固定大小后输入CNN中，得到固定维度的输出特征</li>
<li>对提取的CNN特征，利用SVM分类器分类得到对应类别</li>
<li>边界回归（bouding-box regression），用线性回归模型修正候选框的位置</li>
</ol>
<p>R-CNN使得识别的精度和速度都有了提升，但是也存在很大问题，每次候选框都需要经过CNN操作，计算量很大，有很多重复计算；训练步骤繁多。</p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>R-CNN需要每次将候选框resize到固定大小作为CNN输入，这样有很多重复计算。SPP-net的主要思想是去掉了原始图像上的crop/warp等操作，换成了在卷积特征上的空间金字塔池化层（Spatial Pyramid Pooling，SPP）。</p>
<p><img src="/img/rcnn/2.png" alt=""></p>
<p>SPP Net对整幅图像只进行一次CNN操作得到特征图，这样原图中的每一个候选框都对应于特征图上大小不同的某一区域，通过SPP可以将这些不同大小的区域映射为相同的维度，作为之后的输入，这样就能保证只进行一次CNN操作了。SPP包含一种可伸缩的池化层，输出固定尺寸特征。</p>
<p>基于SPP的思想，Fast R-CNN加入了一个ROI Pooling，将不同大小输入映射到一个固定大小的输出。R-CNN之前的操作是目标识别（classification）以及边界回归（bouding-box regression）分开进行。Fast R-CNN做的改进就是将这两个过程合并在一起，这两个任务共享CNN特征图，即成为了一个multi-task模型。</p>
<p><img src="/img/rcnn/3.png" alt=""></p>
<p>多任务自然对应multi-loss，损失函数包括分类误差以及边框回归误差。<br>L<em>cls</em>为分类误差：</p>
<p><img src="/img/rcnn/4.png" alt=""></p>
<p>分类误差只考虑对应的类别被正确分类到的概率，即P<em>l</em>为label对应的概率，当P<em>l</em>=1时，Loss为0，即正确分类的概率越大，loss越小。</p>
<p>L<em>reg</em>为边框回归误差：</p>
<p><img src="/img/rcnn/5.png" alt=""></p>
<p>对预测的边框四个位置描述参数与真实分类对应边框的四个参数偏差进行评估作为损失函数，g函数为smooth L1函数，这样对于噪声点不敏感，鲁棒性强，在|x|&gt;1时，变为线性，降低噪声影响。</p>
<p><img src="/img/rcnn/6.png" alt=""></p>
<p><img src="/img/rcnn/7.png" alt=""></p>
<p>这样加权得到的最终损失函数为：</p>
<p><img src="/img/rcnn/8.png" alt=""></p>
<p>foreground理解为前景，即对应有目标物体，这个时候需要考虑边框回归误差；background为背景，没有包含目标物品，所以不需考虑边框回归误差。</p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>Faster R-CNN对Fast R-CNN又进行了改进，使得Faster。主要是将候选框的选取也引入到网络中，代替了之前SS选取候选框的方式，即引入了RPN（Region Proposal Network），将找候选框的工作也交给了神经网络了。</p>
<p><img src="/img/rcnn/9.png" alt=""></p>
<p>提到RPN网络，就不能不说anchors，即锚点，对应的是一组矩形框，在实际中有3种形状width:height = [1:1, 1:2, 2:1]，对应3种尺寸，所以共计9个矩形框。</p>
<p><img src="/img/rcnn/10.jpg" alt=""></p>
<p>这个矩形框对应的是原始输入图像里面的，并非是卷积特征图上的。即对卷积特征图上每一个点，可以对应原始图上的一个anchors，为其配备9个框作为原始检测框，当然一开始肯定是不准确的，可以在后续的bounding box regression修正检测框位置。</p>
<p>为了生成区域建议框，在最后一个共享的卷积层输出的卷积特征映射上滑动小网络，这个网络全连接到输入卷积特征映射的nxn的空间窗口上。每个滑动窗口映射到一个低维向量上（对于ZF最后卷积层的输出是256channel，即生成256张特征图，所以小网络滑窗在特征图上的点生成向量是256-d，对于VGG是512-d，每个特征映射的一个滑动窗口对应一个数值）。这个向量输出给两个同级的全连接的层——包围盒回归层（reg）和包围盒分类层（cls）。论文中n=3，由于小网络是滑动窗口的形式，所以全连接的层（nxn的）被所有空间位置共享（指所有位置用来计算内积的nxn的层参数相同）。这种结构实现为nxn的卷积层，后接两个同级的1x1的卷积层（分别对应reg和cls）。<br>在每一个滑动窗口的位置，我们同时预测k个区域建议，所以reg层有4k个输出，即k个box的坐标编码。cls层输出2k个得分，即对每个建议框是目标/非目标的估计概率（为简单起见，是用二类的softmax层实现的cls层，还可以用logistic回归来生成k个得分）。k个建议框被相应的k个称为anchor的box参数化。每个anchor以当前滑动窗口中心为中心，并对应一种尺度和长宽比，我们使用3种尺度和3种长宽比，这样在每一个滑动位置就有k=9个anchor。对于大小为WxH（典型值约2,400）的卷积特征映射，总共有WHk个anchor。</p>
<p>Faster R-CNN的损失函数为：</p>
<p><img src="/img/rcnn/11.png" alt=""></p>
<p>这里，i是一个mini-batch中anchor的索引，Pi是anchor i是目标的预测概率。如果anchor为正，Pi<em> 就是1，如果anchor为负，Pi</em> 就是0。ti是一个向量，表示预测的包围盒的4个参数化坐标，t<em>i</em>是与正anchor对应的GT（groundtruth）包围盒的坐标向量。Pi<em> L</em>reg<em>这一项意味着只有正anchor（Pi</em> =1）才有回归损失，其他情况就没有（Pi<em> =0）。cls层和reg层的输出分别由{pi}和{ti}组成，这两项分别由N</em>cls<em>和N</em>reg*以及一个平衡权重λ归一化。<br>边框回归损失函数，用采取类似fast R-CNN介绍的方法。具体地，学习的时候，对于四个参数进行如下处理：</p>
<p><img src="/img/rcnn/12.png" alt=""></p>
<p>x，y，w，h指的是包围盒中心的（x,y）坐标、宽、高。变量x，xa，x* 分别指预测的包围盒、anchor的包围盒、GT的包围盒（对y，w，h也是一样）的x坐标，可以理解为从anchor包围盒到附近的GT包围盒的包围盒回归。</p>
<p>Fast R-CNN训练依赖于固定的目标建议框，而Faster R-CNN中的卷积层是共享的，所以RPN和Fast R-CNN都不能独立训练，论文中提出的是4步训练算法，通过交替优化来学习共享的特征。 </p>
<ol>
<li>训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调用于区域建议任务。</li>
<li>利用第一步的RPN生成的建议框，由Fast R-CNN训练一个单独的检测网络，这个检测网络同样是由ImageNet预训练的模型初始化的，这时候两个网络还没有共享卷积层。</li>
<li>用检测网络初始化RPN训练，但我们固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了。</li>
<li>保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="CV" scheme="http://yoursite.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>NHD自动登陆签到脚本</title>
    <link href="http://yoursite.com/2018/03/05/NHD%E8%87%AA%E5%8A%A8%E7%99%BB%E9%99%86%E7%AD%BE%E5%88%B0%E8%84%9A%E6%9C%AC/"/>
    <id>http://yoursite.com/2018/03/05/NHD自动登陆签到脚本/</id>
    <published>2018-03-05T12:46:38.000Z</published>
    <updated>2019-03-19T08:30:16.496Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站<a href="http://www.nexushd.org/index.php" target="_blank" rel="external">NHD</a>，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。<br><a id="more"></a></p>
<p>首先还是老套路，先用浏览器实现一遍登陆以及签到过程，用谷歌浏览器的开发者模式获取这过程中的信息。可以看到首次登陆之后，response的header有set-cookie字段</p>
<p><img src="/img/nhd/1.png" alt=""></p>
<p>而后这个服务器返回的set-cookie字段的值在下一次会话中又出现了</p>
<p><img src="/img/nhd/2.png" alt=""></p>
<p>其实总体思路还是很简单的，与之前的爬虫不同，这次需要用户登陆，所以是POST方法，并将用户名和密码以data参数代入，登陆之后服务器会返回cookie来标识用户。因为http是一种无状态协议，用户首次访问web站点的时候，服务器对用户一无所知。而Cookie就像是服务器给每个来访问的用户贴的标签，而这些标签就是对来访问的客户端的独有的身份的一个标识，这里就如同每个人的身份证一样，带着你的个人信息。而当一个客户端第一次连接过来的时候，服务端就会给他打一个标签，这里就如同给你发了一个身份证，所以之后的访问服务器再带上这个cookie就标识了该账户，具体流程网上找到一张很好的图可以解释。</p>
<p><img src="/img/nhd/3.png" alt=""></p>
<p>这样的话，只需要第一次输入密码，后面浏览器再次访问只要带上这个服务器返回的cookie，服务器就可以知道是该账户在访问，所以python程序也模拟该过程。利用request库中的session对象来创建类似于图中的过程，session对象会保存访问过程中的cookie用于之后对服务器的继续访问。</p>
<pre><code>url = &apos;http://www.nexushd.org/takelogin.php&apos;  #登陆界面
a = session.post(url=url, cookies=cookies, headers=headers, data=data)
#这里的cookie是浏览器首次访问的使用的cookie，之后服务器
#设置的cookie会保存在session对象中
time.sleep(2)

url2 = &apos;http://www.nexushd.org/signin.php&apos;  #签到界面
b = session.get(url=url2, headers=headers)
time.sleep(2)

url3 = &apos;http://www.nexushd.org/signin.php?&apos;
qiandao = {&apos;action&apos;:&apos;post&apos;,&apos;content&apos;:&apos;lalala2333&apos;} #签到信息随便填，lalala2333
r = session.post(url=url3, headers=headers, data=qiandao)
</code></pre><p>而后就是一个判断是否登陆成功的程序，依然使用BeautifulSoup来解析，得到已签到之后退出循环，并将日志信息记录到日志文件。</p>
<pre><code>r = session.post(url=url3, headers=headers, data=qiandao)
r = BeautifulSoup(r.content,&apos;lxml&apos;)
message1 = r.find_all(&apos;a&apos;,{&apos;href&apos;:&quot;signin.php&quot;})[0].contents[0]
message2 = r.find_all(&apos;h2&apos;)[0].getText()
if message2 == &apos;签到成功&apos;:
    f = codecs.open(&apos;logging.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;)
    str = time.strftime(&apos;%Y-%m-%d  %H:%M:%S&apos;, time.localtime(time.time())) + &apos;-----签到成功&apos; + &apos;\n&apos;
    f.write(str)  # 记录日志信息到日志文件
    f.close()
    print(r.find_all(&apos;span&apos;, {&apos;class&apos;: &apos;medium&apos;})[0].getText())
    print(r.find_all(&apos;td&apos;, {&apos;class&apos;: &apos;text&apos;})[-1].getText().split(&apos;。&apos;)[0])
    break
elif message1 == &apos;已签到&apos;: #如果已经签到
    print(&apos;已经签到过了哦&apos;)
    break
if maxtry &lt; 30:
    print(&apos;签到失败，第&apos;+str(maxtry+1)+&apos;次重试&apos;)
    maxtry = maxtry+1
    time.sleep(5)
else:
    print(&quot;自动签到失败，请手动签到，或者检查网络连接&quot;)
    break
</code></pre><p>为了能够开机自动运行程序，将该程序添加至windows启动运行。代码中读取在配置文件中的账户信息，并且通过读取上一次签到成功时间来判断是否成功签到过。</p>
<pre><code>maxtry=0  #记录重试次数
f = codecs.open(&apos;profile.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;)  #读取配置文件，包含账户及密码
line=f.readline()
f.close()

username = line.split()[0] #你的用户名
password = line.split()[1]  #你的密码
data = {&apos;username&apos;: username, &apos;password&apos;:password}

flag = True
day_now = time.localtime(time.time()).tm_mday
f = codecs.open(&apos;logging.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;)
lines = f.readlines()
f.close()
#######更换账户登陆时，最好清除以前账户的日志信息
try:  #如果第一次使用可能没有签到记录
    day_log = int(lines[-1].split()[0].split(&apos;-&apos;)[-1])
except:
    day_log=33

day_log = int(lines[-1].split()[0].split(&apos;-&apos;)[-1])
if day_now == day_log:
    print(username+&apos;今天签到过了哦&apos;)
    flag = False
</code></pre><p>将配置文件profile.txt和日志文件logging.txt以及代码qiandao.py放入windows的启动运行的文件夹，这个文件夹可以通过在cmd窗口下输入</p>
<pre><code>shell:Startup
</code></pre><p>打开</p>
<p>在这个启动文件夹下写一个bat脚本来运行python代码</p>
<pre><code>D:
cd D:\simulation file\pyCharm\python3\qiandao
python qiandao.py
pause
</code></pre><p>至此就完整实现了电脑开机自动登陆签到NHD啦。效果如下<br>测试结果：<br><img src="/img/nhd/4.png" alt=""></p>
<p>完整代码详见个人<a href="https://github.com/lkj1114889770/WebScraping/tree/master/qiandao" target="_blank" rel="external">github</a>了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站&lt;a href=&quot;http://www.nexushd.org/index.php&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NHD&lt;/a&gt;，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。&lt;br&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>我从崖边跌落</title>
    <link href="http://yoursite.com/2018/02/12/%E6%88%91%E4%BB%8E%E5%B4%96%E8%BE%B9%E8%B7%8C%E8%90%BD/"/>
    <id>http://yoursite.com/2018/02/12/我从崖边跌落/</id>
    <published>2018-02-12T10:58:56.000Z</published>
    <updated>2019-03-19T08:32:17.549Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我从崖边跌落<br>落入星空辽阔<br>银河不清不浊<br>不知何以摆脱</p>
<p><a href="http://music.163.com/#/song?id=415086030&amp;market=baiduqk" target="_blank" rel="external">谢春花《我从崖边跌落》</a></p>
<p><img src="/img/yabian/1.jpg" alt=""></p>
<a id="more"></a>
<p><img src="/img/yabian/2.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我从崖边跌落&lt;br&gt;落入星空辽阔&lt;br&gt;银河不清不浊&lt;br&gt;不知何以摆脱&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://music.163.com/#/song?id=415086030&amp;amp;market=baiduqk&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;谢春花《我从崖边跌落》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/yabian/1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>电影推荐系统构建</title>
    <link href="http://yoursite.com/2018/01/11/%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/01/11/电影推荐系统构建/</id>
    <published>2018-01-11T06:12:27.000Z</published>
    <updated>2019-03-19T08:37:31.504Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>很久没有更新博客了，最近刚做完数据挖掘的大作业，选了一个电影数据集挖掘的课题，做了一个推荐系统，在这里简单地记录一下。<br>电影推荐系统数据集来源于<a href="https://www.kaggle.com/rounakbanik/the-movies-dataset" target="_blank" rel="external">kaggle</a>上的MovieLens完整的45,000条数据，电影数据包括2017年7月前发布的电影，包括270,000个用户的超过26,000,000条评论，以及从GroupLens官方网站获得的评分。基于此电影数据集，完成下面的数据挖掘目标。<br><a id="more"></a><br>•    电影数据集处理及可视化分析<br>•    基于用户投票的推荐算法<br>•    基于内容的推荐算法<br>•    基于协同过滤的推荐算法<br>•    数据库技术的应用<br>•    简单的电影推荐网站构建<br>当然这次代码也有很大程度上参考了这个数据集下的大佬分享的kernel，提供了许多不错的精致代码。</p>
<h2 id="数据集介绍及分析"><a href="#数据集介绍及分析" class="headerlink" title="数据集介绍及分析"></a>数据集介绍及分析</h2><p>movies_metadata.csv: 电影基本信息描述文件，包括 45000部电影的演员、工作人员、情节关键字、预算、收入、海报、发布日期、语言、制作公司、国家、TMDB投票计数和平均投票信息.<br>keywords.csv: 包含电影的关键词信息，每条数据为json格式.。<br>credits.csv: 演员和电影工作人员的信息，每条数据为json格式。<br>links.csv: 包含所有电影TMDB IDs和IMDB IDs 对应信息。<br>links_small.csv: 9,000部电影的TMDB IDs和IMDB IDs 对应信息.<br>rating.csv:用户对于所有电影的打分，1-5。<br>ratings_small.csv: 电影打分子集，700个用户对于9,000部电影的100,000个评分。<br>针对电影的情况，首先我们看一下电影的平均投票分布，如下图所示，由图中可以看出，电影集中分布在6分左右，也是比较符合实际情况，一般的电影居多，高分电影以及烂片数量相对较少。</p>
<pre><code>%matplotlib inline
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(&apos;ignore&apos;)
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv(&apos;movies_metadata.csv&apos;)
df[&apos;vote_average&apos;] = df[&apos;vote_average&apos;].replace(0, np.nan)
sns.distplot(df[&apos;vote_average&apos;].fillna(df[&apos;vote_average&apos;].median()))
</code></pre><p><img src="/img/movie/1.png" alt=""></p>
<pre><code>df[&apos;year&apos;] = pd.to_datetime(df[&apos;release_date&apos;], errors=&apos;coerce&apos;).apply(lambda x: str(x).split(&apos;-&apos;)[0] if x != np.nan else np.nan)
year_gen = pd.DataFrame(df[&apos;year&apos;].value_counts()).reset_index()
year_gen.columns = [&apos;year&apos;, &apos;counts&apos;]
year_gen.drop([87,135,],inplace=True)
year_gen[&apos;year&apos;]=year_gen[&apos;year&apos;].astype(&apos;int&apos;)
plt.plot(year_gen.year,year_gen.counts)
</code></pre><p><img src="/img/movie/2.png" alt=""></p>
<p>从上图中的电影分布可以看出，从1880年左右以来，电影的数量基本上是逐年增长的趋势，特别是进入21实际以来，增长速度很快（出现一段下降是因为2017年的完整数据收集不完整）。<br>下面再分析数据集中的电影的区域分布，利用一个比较强的可视化工具plotly，画出电影数量的区域分布，因为美国的电影产出相对其他国家高出太多，所以画图是先忽略了美国，这样画其他国家的数量之间的比较才会更加明显。</p>
<pre><code>data = [ dict(
        type = &apos;choropleth&apos;,
        locations = con_df[&apos;country&apos;],
        locationmode = &apos;country names&apos;,
        z = con_df[&apos;num_movies&apos;],
        text = con_df[&apos;country&apos;],
        colorscale = [[0,&apos;rgb(255, 255, 255)&apos;],[1,&apos;rgb(255, 0, 0)&apos;]],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = &apos;rgb(180,180,180)&apos;,
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            tickprefix = &apos;&apos;,
            title = &apos;数量图例&apos;),
      ) ]

layout = dict(
    title = &apos;电影数据集中电影数量分布（除美国外）&apos;,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = &apos;Mercator&apos;
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename=&apos;d3-world-map&apos; )
plt.figure(figsize=(12,5))
sns.barplot(x=&apos;country&apos;, y=&apos;num_movies&apos;, data=country)
plt.show()
####除去美国外，英国。法国、德国、意大利，亚洲的日本和印度，北美的巴西
</code></pre><p><img src="/img/movie/3.png" alt=""></p>
<p><img src="/img/movie/4.png" alt=""></p>
<h2 id="推荐系统构建"><a href="#推荐系统构建" class="headerlink" title="推荐系统构建"></a>推荐系统构建</h2><p>在这次推荐系统的构建中，我们采用了三种算法来构建我们的推荐系统，基于这三种算法，包括基于用户投票的推荐算法、基于内容的推荐算法和协同过滤推荐算法，根据这些算法，最终来构建我们的电影推荐系统。</p>
<h3 id="基于用户投票的推荐算法"><a href="#基于用户投票的推荐算法" class="headerlink" title="基于用户投票的推荐算法"></a>基于用户投票的推荐算法</h3><p>作为国际知名的权威点评网站，在他们大名鼎鼎的TOP250榜单中，采用的就是贝叶斯算法，其公式如下：</p>
<p><img src="/img/movie/5.png" alt=""></p>
<p>其中，WR为加权得分，R为该电影的用户投票平均得分，V为该电影的投票人数，m为最低评分个数，C为所有电影的平均得分。<br>这个算法的提出基于这样一个现实问题：热门电影与冷门电影的平均得分，是否真的可比？举例来说，一部好莱坞大片有10000个观众投票，一部小成本的文艺片只有100个观众投票。这两者的投票结果，怎么比较？如果使用”威尔逊区间”，后者的得分将被大幅拉低，这样处理是否公平，能不能反映它们真正的质量？一个合理的思路是，如果要比较两部电影的好坏，至少应该请同样多的观众观看和评分。既然文艺片的观众人数偏少，那么应该设法为它增加一些观众。</p>
<p>根据这个思路，这个算法相当于给每部电影增加了m个选票，并且每个评分为平均得分C，然后用现有观众的投票进行修正，即v*R/(v+m)部分，使得得分更加接近于真实情况。这种算法由于给每部电影增加了m个选票，拉近了不同电影之间投票人数的差异，使得投票人数较少的电影也有可能名列前茅。</p>
<p>这个算法借鉴了“贝叶斯推断”的思想，既然不知道投票结果，那就预先估计一个值，然后不断用新的信息修正，使它接近于正确值。在式子中，m可以看作是先验概率，每一次新的投票都是一个调整因子，使总体平均分不断向该项目的真实投票结果靠近。投票人数越多，该项目的”贝叶斯平均”就越接近算术平均，对排名的影响就越小。因此这种方法可以让投票较少的项目，能够得到相对公平的排名。</p>
<p>我们针对所有电影，类似于IMDB我们计算出了TOP250，下图为基于贝叶斯统计的用户投票排名算法得出的所有电影的TOP250中选取出的TOP10，其中电影名红色的为实际IMDB中进入TOP10的电影，可以看出有3部电影存在于IMDB的TOP10，绿色标注的电影为TOP11-15的电影，有3部。总的来说，贝叶斯统计得出的排名还是比较接近于IMDB的排名，但是由于我们的算法考虑的因素较少，所以还是有一定的区别。</p>
<p><img src="/img/movie/6.jpg" alt=""></p>
<p>进一步的，我们从电影数据集中，根据电影的genre属性值中，分离出电影所属的不同属性，所有电影的类型分布（TOP10）如下图所示</p>
<p><img src="/img/movie/7.png" alt=""></p>
<p>可以看出，电影数据集中戏剧、喜剧、恐怖片、爱情片等数量较多，依次数量排名.针对数量超过3000的电影，我们也采取类似的方式计算了TOP250，部分电影类型的TOP10在下图中给出。</p>
<p><img src="/img/movie/8.png" alt=""></p>
<h3 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h3><p>基于投票排名的推荐算法给每个用户都是一样推荐按照TOP排名得出的电影，而不会根据特定的观众喜欢的电影去推荐相似的电影。为了能够给用户推荐相似的电影，我们首先需要对电影之间的相似性进行衡量，主要应用到电影的描述数据来完成基于内容的推荐，主要的实现过程包括：<br>•    对电影的关键词、描述信息、标语、主角、导演信息的提取<br>•    对上述信息进行词干提取<br>•    对上述信息进行特征抽取，转换成词向量<br>•    考虑评分情况，结合相似度完成推荐<br>首先我们对于电影的相关描述信息进行一个大致分析，制作了词云对所有电影的情况概览。</p>
<pre><code>df[&apos;title&apos;] = df[&apos;title&apos;].astype(&apos;str&apos;)
df[&apos;overview&apos;] = df[&apos;overview&apos;].astype(&apos;str&apos;)
title_corpus = &apos; &apos;.join(df[&apos;title&apos;])
overview_corpus = &apos; &apos;.join(df[&apos;overview&apos;])
from wordcloud import WordCloud, STOPWORDS
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color=&apos;white&apos;, height=2000, width=4000).generate(title_corpus)
plt.figure(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p><img src="/img/movie/9.png" alt=""></p>
<pre><code>overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color=&apos;white&apos;, height=2000, width=4000).generate(overview_corpus)
plt.figure(figsize=(16,8))
plt.imshow(overview_wordcloud)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p><img src="/img/movie/10.png" alt=""></p>
<p>上面两幅图分别是电影的标题和电影简述画出的词云，可以看到电影标题中Love、Girl、Man、Life，Love作为最高频的词，毕竟大多数电影都有爱情这条线。在电影的简述中，find、life、one是最高频的词，可以给我们反映大多数电影的主题。</p>
<p>在获得电影的关键词、描述信息、标语、主角、导演信息之后，我们需要对这些信息进行词干提取。在语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，即得到单词最一般的写法。计算机科学领域有很多词干提取的相应算法，我们使用了一个面向英语的词干提取器stemming，使用Python的NLTK库的stemming算法，实现的效果为要识别字符串“cats”、“catlike”和“catty”提取出词根“cat”；“stemmer”、“stemming”和“stemmed”提取出词根“stem”。</p>
<pre><code>from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(&apos;english&apos;)  #英语的词干提取
</code></pre><p>下一步需要将提取词干后的文档进行向量化处理，采用的是sklearn中的Countvectorizer。根据语料库中的词频排序从高到低进行选择，词汇表的最大含量由vocabsize超参数来指定，超参数minDF，则指定词汇表中的词语至少要在多少个不同文档中出现次数，产生文档关于词语的稀疏表示，在fitting过程中，countvectorizer将根据语料库中的词频排序选出前vocabsize个词，输出词向量。</p>
<pre><code>count = CountVectorizer(analyzer=&apos;word&apos;,ngram_range=(1, 2),min_df=0, stop_words=&apos;english&apos;)
count_matrix = count.fit_transform(smd[&apos;soup&apos;])   #基于词向量统计的矩阵
</code></pre><p>当然，基于内容的推荐算法还需要考虑到电影的评分，不然仅仅根据电影之间的相似度，很有可能就会出现给观众推荐很相似的电影，但却是“烂片”的这种情况，基于这种考虑，以电影《The Godfather》（教父）以及《The Lord of the Rings: The Return of the King》（指环王：王者归来）为例，推荐的相应10部电影结果如下图所示。</p>
<p><img src="/img/movie/11.png" alt=""></p>
<p>可以看到，都推荐了同类型的电影，比如针对教父推荐了一些剧情、犯罪电影，而针对指环王推荐了一些动作、奇幻类的电影，而且这两部电影都有其他续集，比如针对《教父1》推荐了其续集《教父2》及《教父3》也都相应地推荐了，推荐的电影也都是高分电影.</p>
<h3 id="协同过滤推荐"><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h3><p>从应用的场景来看，基于内容的推荐算法更多地适用于用户根据关键字或者电影名字来搜索相应的电影，然后推荐系统来进行相应的推荐。基于需求个性角度来看，基于内容的推荐算法还不够个人化，用户需要的是更加符合个人偏好的推荐结果，可以根据用户之前的打分情况，更有针对性地推荐一些可能喜欢的电影，这种情况下，应用的最多的就是协同过滤算法。</p>
<p>协同过滤通过用户和产品及用户的偏好信息产生推荐策略，最基本的策略有两种：一种是找到具有类似品味的人所喜欢的物品；另一种是从一个人喜欢的物品中找出类似的物品，即基于用户的推荐技术（User CF）和基于物品的推荐技术（Item CF）。在我们这个应用场景中，有大量的电影信息，但是用户已经打分的电影只占总量很少的一部分，将用户打分和电影信息构成一个矩阵，那么这个矩阵会存在严重的稀疏性，经过计算大约在1.5%左右，基于这种考虑，我们采取Item-based协同过滤算法。同样由于矩阵的稀疏性，在数据量很大的情况下一般采用矩阵分解来减少运算量，采用PMF矩阵分解算法来完成这个目标。</p>
<p>采用的是surprise库中的SVD算法，但是我看了surprise库中的SVD算法介绍，其实更准确地说是PMF（Probabilistic Matrix Factorization）算法，即概率矩阵分解算法，所以这里对PMF进行相应的介绍。</p>
<p>假定每个用户u都有一个D维的向量，表示他对不同风格的电影的偏好，每个电影i也有一个D维的向量表示不同风格的用户对它的偏好。 于是电影的评分矩阵可以这样来估计：</p>
<p><img src="/img/movie/12.png" alt=""></p>
<p>p 和q就是D维的向量。用梯度下降法训练p和q，迭代几十次就收敛了。但是这样的SVD很容易就过拟合，所以需要加入正则化项：</p>
<p><img src="/img/movie/13.png" alt=""></p>
<p>这样每次迭代的时候，更新公式为：</p>
<p><img src="/img/movie/14.png" alt=""></p>
<p>采用5折交叉验证</p>
<pre><code>import pandas as pd
import numpy as np
from surprise import Reader, Dataset, SVD, evaluate
from collections import defaultdict
import warnings; warnings.simplefilter(&apos;ignore&apos;)


reader = Reader()
ratings = pd.read_csv(&apos;ratings_small.csv&apos;)

#从DataFrame导入数据
data = Dataset.load_from_df(ratings[[&apos;userId&apos;, &apos;movieId&apos;, &apos;rating&apos;]], reader)
data.split(n_folds=5)
trainset = data.build_full_trainset()
#SVD算法
algo = SVD()
evaluate(algo, data, measures=[&apos;RMSE&apos;, &apos;MAE&apos;])

#训练模型
algo.train(trainset)
#对用户未评价的电影生成测试集
testset = trainset.build_anti_testset()
predictions = algo.test(testset)  #预测测试集结果


def get_top_n(predictions, n=10):
    &apos;&apos;&apos;对预测结果中的每个用户，返回n部电影，默认n=10
    返回值一个字典，包括：
    keys 为原始的userId，以及对应的values为一个元组
        [(raw item id, rating estimation), ...].
    &apos;&apos;&apos;

    # 预测结果取出，对应每个userId.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
    # 排序取出前n个
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]
    return top_n

top_n = get_top_n(predictions, n=10)
rec_result=np.zeros((671,11))  #定义二维矩阵来存放结果
i=0
for uid, user_ratings in top_n.items():
    rec_result[i,0]=uid
    rec_result[i,1:]=[iid for (iid, _) in user_ratings]
    i=i+1
rec_result=rec_result.astype(&apos;int&apos;)

#转变成DataFrame
rec_result=pd.DataFrame(rec_result,columns=[&apos;userId&apos;,&apos;rec1&apos;,&apos;rec2&apos;,&apos;rec3&apos;,&apos;rec4&apos;,&apos;rec5&apos;,
                                          &apos;rec6&apos;,&apos;rec7&apos;,&apos;rec8&apos;,&apos;rec9&apos;,&apos;rec10&apos;])
</code></pre><p>算法运行结果：</p>
<p><img src="/img/movie/15.png" alt=""></p>
<h2 id="简单的电影点评网站构建"><a href="#简单的电影点评网站构建" class="headerlink" title="简单的电影点评网站构建"></a>简单的电影点评网站构建</h2><p>整体框架</p>
<p><img src="/img/movie/16.png" alt=""></p>
<p>MySQL是一个关系型数据库管理系统，也是一种WEB应用最好的数据库。数据库作为中间件，搭建在寝室的主机，便于小组成员之间使用，其操纵代码：</p>
<pre><code>import pymysql
import pandas as pd
#连接数据库
conn = pymysql.connect(host=&apos;10.110.43.140&apos;,port= 3306,user = &apos;###&apos;,passwd=&apos;####&apos;,db=&apos;sys&apos;) #db：库名，用户名和密码这里我打了马赛克了，嘻嘻
#创建游标
cur = conn.cursor()
df=pd.read_sql(&apos;SELECT * FROM db_movies.tb_movies;&apos;,conn)
cur.close()
conn.close()
</code></pre><p>小伙伴应用Django框架，是一个开放源代码的Web应用框架，由Python写，时间仓促的情况下赶出了一个还是很不错的页面，点赞。</p>
<p><img src="/img/movie/17.png" alt=""></p>
<p>详细代码可见个人<a href="https://github.com/lkj1114889770/File_Recommend" target="_blank" rel="external">github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久没有更新博客了，最近刚做完数据挖掘的大作业，选了一个电影数据集挖掘的课题，做了一个推荐系统，在这里简单地记录一下。&lt;br&gt;电影推荐系统数据集来源于&lt;a href=&quot;https://www.kaggle.com/rounakbanik/the-movies-dataset&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;kaggle&lt;/a&gt;上的MovieLens完整的45,000条数据，电影数据包括2017年7月前发布的电影，包括270,000个用户的超过26,000,000条评论，以及从GroupLens官方网站获得的评分。基于此电影数据集，完成下面的数据挖掘目标。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="电影" scheme="http://yoursite.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>网格搜索与Pipeline</title>
    <link href="http://yoursite.com/2017/11/23/%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E4%B8%8E%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2017/11/23/网格搜索与交叉验证/</id>
    <published>2017-11-23T12:03:19.000Z</published>
    <updated>2019-03-19T10:06:32.431Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>学习机器学习有段时间了，第一参加了个比赛，京东JDD数据探索大赛，做了个<a href="http://jddjr.jd.com/item/1" target="_blank" rel="external">登陆识行为识别</a>。不得不说，在实际业务场景中用学的机器学习算法来解决问题，比想象中的难度还是大很多，毕竟实际问题其实比平时简单的算法应用复杂得多。虽然目前数据准确率还不是很高，这个数据集的正例和负例比例相差太大，而且数据上对于特征工程处理也有很大的难度。Anyway，毕竟是第一次参加这样的比赛，收获还是很大的，学到了很多新东西。比如网格搜索和Pipeline机制，以及一个神器Xgboost。先将网格搜索和交叉验证mark一下吧。<br><a id="more"></a></p>
<h2 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h2><p>实际机器学习应用场景中的一个利器，通俗点就是暴力搜索。机器学习在应用的的时候，调参是一个很重要的环节，而网格搜索就在于优化参数搜索选择，更直白地说，就是你选择可能的参数集给你的分类器，然后网格搜索把这些可能的参数情况都运行一遍，按照你设定的score计算方式，返回score最高的参数。</p>
<p>函数原型：</p>
<pre><code>GridSearchCV(estimator, param_grid, scoring=None,
   fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, 
   verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’)
</code></pre><p>常用参数<br><strong>estimator</strong>：所使用的分类器，如estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features=’sqrt’,random_state=10), 并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个scoring参数，或者score方法。<br><strong>param_grid</strong>：值为字典或者列表，即需要最优化的参数的取值，param_grid =param_test1，param_test1 = {‘n_estimators’:range(10,71,10)}。我用的Xgboost算法，优化的参数集为：</p>
<pre><code>parameters = {
    &apos;max_depth&apos;:[4,6],
    &apos;learning_rate&apos;:[0.1,0.3],
    &apos;subsample&apos;:[0.8,1.0],
    &apos;gamma&apos;:[0,3,5]
}
</code></pre><p><strong>scoring</strong> :准确度评价标准，默认None,这时需要使用score函数；或者如scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring官方给的参数选择为<a href="http://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/model_evaluation.html</a>，当然也可以自定义，我在比赛中就按照JDD给的评分要求自定义了：</p>
<pre><code>from sklearn.metrics import fbeta_score,make_scorer
#评估函数
JdScore = make_scorer(fbeta_score,beta=0.1,greater_is_better=True)
</code></pre><p><strong>cv</strong> :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，也可以是yield训练/测试数据的生成器。<br><strong>verbose</strong>：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出。<br><strong>n_jobs</strong>: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。</p>
<p>一个完整的网格搜索：</p>
<pre><code>from xgboost.sklearn import XGBClassifier
#xgb的配置
xgbFier = XGBClassifier(
learning_rate =0.1,
n_estimators=1000,
max_depth=5,
min_child_weight=1,
gamma=0,
subsample=0.8,
colsample_bytree=0.8,
objective= &apos;binary:logistic&apos;,
scale_pos_weight=1,
seed=27,
silent=0
)

#网格搜索实验
from sklearn.model_selection import GridSearchCV
parameters = {
    &apos;max_depth&apos;:[4,6],
    &apos;learning_rate&apos;:[0.1,0.3],
    &apos;subsample&apos;:[0.8,1.0],
    &apos;gamma&apos;:[0,3,5]
}
gSearch  =GridSearchCV(xgbFier,parameters,n_jobs=-1,scoring=JdScore,cv=5)

import time
start =time.time()
gSearch.fit(X_train,Y_train)
runtime=time.time()-start
print(&apos;run time:&apos;,runtime)
print(gSearch.best_params_,gSearch.best_score_)
</code></pre><p>输出结果为：</p>
<pre><code>run time: 4109.730866909027
{&apos;gamma&apos;: 3, &apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 6, &apos;subsample&apos;: 0.8} 0.738112481672
</code></pre><p>这样就找出了较优的参数，唉，现在只能得到0.78的线下score，还要继续修改啊。</p>
<h2 id="Pipeline机制"><a href="#Pipeline机制" class="headerlink" title="Pipeline机制"></a>Pipeline机制</h2><p>顾名思义就是管道机制，就是将机器学习整个流程流式化封装和管理，因为参数集在很多情况下对于测试集和训练集都是一样处理，他们有很多共同的步骤，这个机制就是便于这些步骤的共同使用。网上找到的一个很好解释的图如下，模型训练和预测过程中数据标准化、PCA降维之类的处理都可以通用，而且训练和预测用的是同一算法。</p>
<center> <br><br><img width="500" height="400" src="/img/pipeline/1.png"> <br><br></center>

]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习机器学习有段时间了，第一参加了个比赛，京东JDD数据探索大赛，做了个&lt;a href=&quot;http://jddjr.jd.com/item/1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;登陆识行为识别&lt;/a&gt;。不得不说，在实际业务场景中用学的机器学习算法来解决问题，比想象中的难度还是大很多，毕竟实际问题其实比平时简单的算法应用复杂得多。虽然目前数据准确率还不是很高，这个数据集的正例和负例比例相差太大，而且数据上对于特征工程处理也有很大的难度。Anyway，毕竟是第一次参加这样的比赛，收获还是很大的，学到了很多新东西。比如网格搜索和Pipeline机制，以及一个神器Xgboost。先将网格搜索和交叉验证mark一下吧。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EVD、SVD以及PCA整理</title>
    <link href="http://yoursite.com/2017/11/01/EVD%E3%80%81SVD%E4%BB%A5%E5%8F%8APCA/"/>
    <id>http://yoursite.com/2017/11/01/EVD、SVD以及PCA/</id>
    <published>2017-11-01T06:40:14.000Z</published>
    <updated>2019-03-19T08:48:36.061Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近的机器学习算法学习到了主成分分析（PCA），在人脸识别中对样本数据进行了降维，借此对特征值分解（EVD）、奇异值分解（SVD）进行了梳理整理。</p>
<h2 id="特征值分解（EVD"><a href="#特征值分解（EVD" class="headerlink" title="特征值分解（EVD)"></a>特征值分解（EVD)</h2><p>矩阵是一种线性变换，比如矩阵Ax=y中，矩阵A将向量x线性变换到另一个矩阵y，这个过程中包含3类效应：旋转、缩放以及投影。<br><a id="more"></a></p>
<p>对角矩阵对应缩放，比如<img src="/img/svd/1.png" alt=""></p>
<p>其对应的线性变换如下：</p>
<p><img alt="" src="/img/svd/111.png"></p>
<p>对与正交矩阵来说，对应的是向量的旋转，比如将向量OA从正交基e1e2中,到另一组正交基为e1’e2’中，</p>
<p><img src="/img/svd/2.png" alt=""><br><img alt="" src="/img/svd/112.png"></p>
<p>当矩阵A与x维度不一样时，得到的y的维度也与x不一样，即存在投影变换。</p>
<p>考虑一种特殊矩阵，对称阵的特征值分解，其实在机器学习中也经常是对XX’求特征向量，也就是对称阵。</p>
<p><img src="/img/svd/3.png" alt=""></p>
<p>其中：</p>
<p><img src="/img/svd/4.png" alt=""></p>
<p><img src="/img/svd/5.png" alt=""></p>
<p>这个时候用到对称阵的特性，U为正交矩阵，其逆矩阵等于转置。</p>
<p><img src="/img/svd/6.png" alt=""></p>
<p>即矩阵A将向量X转移到了U这组基的空间上，再进行缩放，而后又通过U正交基进行旋转，所以只有缩放，没有旋转和投影。</p>
<h2 id="奇异值分解（SVD"><a href="#奇异值分解（SVD" class="headerlink" title="奇异值分解（SVD)"></a>奇异值分解（SVD)</h2><p>奇异值分解其实类似于特征值分解，不过奇异值分解适用于更一般的矩阵，而不是方阵。</p>
<div align="center"><br>    <img src="/img/svd/7.png" width="250" height="50"><br></div>

<p>U、V都是一组正交基，表示一个向量从V这组正交基旋转到U这组正交基，同时也在每个方向进行缩放。</p>
<p>奇异值和特征值对应为：</p>
<p><img src="/img/svd/8.png" alt=""></p>
<p>v即为式子中的右奇异向量，同时也可以得到：</p>
<p><img src="/img/svd/9.png" alt=""></p>
<p>在奇异值按从小到大排序的情况下，很多情况下，前面部分的奇异值就占所有奇异值和的99%以上，所以我们可以取前r个奇异值来近似描述矩阵，可以用来数据降维。</p>
<p><div align="center"><br>    <img src="/img/svd/10.png" width="250" height="50"><br></div><br>这样可以还原出A矩阵，减少数据存储。<br>下面看如何利用SVD降维：</p>
<p><div align="center"><br>    <img src="/img/svd/11.png" width="300" height="200"><br></div><br>从而将A’从n <em> m降到n </em> r</p>
<p>SVD常用于推荐系统，有基于用户的协同过滤（User CF)和基于物品的协同过滤（Item CF)。这里给出一个Item CF实现。</p>
<p>网上找的一个实现代码，找不到出处了。。。</p>
<pre><code>#coding=utf-8
from numpy import *
from numpy import linalg as la

&apos;&apos;&apos;加载测试数据集&apos;&apos;&apos;
def loadExData():
    return mat([[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],
           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],
           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],
           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],
           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],
           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],
           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],
           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],
           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],
           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],
           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]])

&apos;&apos;&apos;以下是三种计算相似度的算法，分别是欧式距离、皮尔逊相关系数和余弦相似度,
注意三种计算方式的参数inA和inB都是列向量&apos;&apos;&apos;
def ecludSim(inA,inB):
    return 1.0/(1.0+la.norm(inA-inB))  #范数的计算方法linalg.norm()，这里的1/(1+距离)表示将相似度的范围放在0与1之间

def pearsSim(inA,inB):
    if len(inA)&lt;3: return 1.0
    return 0.5+0.5*corrcoef(inA,inB,rowvar=0)[0][1]  #皮尔逊相关系数的计算方法corrcoef()，参数rowvar=0表示对列求相似度，这里的0.5+0.5*corrcoef()是为了将范围归一化放到0和1之间

def cosSim(inA,inB):
    num=float(inA.T*inB)
    denom=la.norm(inA)*la.norm(inB)
    return 0.5+0.5*(num/denom) #将相似度归一到0与1之间

&apos;&apos;&apos;按照前k个奇异值的平方和占总奇异值的平方和的百分比percentage来确定k的值,
后续计算SVD时需要将原始矩阵转换到k维空间&apos;&apos;&apos;
def sigmaPct(sigma,percentage):
    sigma2=sigma**2 #对sigma求平方
    sumsgm2=sum(sigma2) #求所有奇异值sigma的平方和
    sumsgm3=0 #sumsgm3是前k个奇异值的平方和
    k=0
    for i in sigma:
        sumsgm3+=i**2
        k+=1
        if sumsgm3&gt;=sumsgm2*percentage:
            return k

&apos;&apos;&apos;函数svdEst()的参数包含：数据矩阵、用户编号、物品编号和奇异值占比的阈值，
数据矩阵的行对应用户，列对应物品，函数的作用是基于item的相似性对用户未评过分的物品进行预测评分&apos;&apos;&apos;
def svdEst(dataMat,user,simMeas,item,percentage):
    n=shape(dataMat)[1]
    simTotal=0.0;ratSimTotal=0.0
    u,sigma,vt=la.svd(dataMat)
    k=sigmaPct(sigma,percentage) #确定了k的值
    sigmaK=mat(eye(k)*sigma[:k])  #构建对角矩阵
    xformedItems=dataMat.T*u[:,:k]*sigmaK.I  #根据k的值将原始数据转换到k维空间(低维),xformedItems表示物品(item)在k维空间转换后的值
    for j in range(n):
        userRating=dataMat[user,j]
        if userRating==0 or j==item:continue
        similarity=simMeas(xformedItems[item,:].T,xformedItems[j,:].T) #计算物品item与物品j之间的相似度
        simTotal+=similarity #对所有相似度求和
        ratSimTotal+=similarity*userRating #用&quot;物品item和物品j的相似度&quot;乘以&quot;用户对物品j的评分&quot;，并求和
    if simTotal==0:return 0
    else:return ratSimTotal/simTotal #得到对物品item的预测评分

&apos;&apos;&apos;函数recommend()产生预测评分最高的N个推荐结果，默认返回5个；
参数包括：数据矩阵、用户编号、相似度衡量的方法、预测评分的方法、以及奇异值占比的阈值；
数据矩阵的行对应用户，列对应物品，函数的作用是基于item的相似性对用户未评过分的物品进行预测评分；
相似度衡量的方法默认用余弦相似度&apos;&apos;&apos;
def recommend(dataMat,user,N=5,simMeas=cosSim,estMethod=svdEst,percentage=0.9):
    unratedItems=nonzero(dataMat[user,:].A==0)[1]  #建立一个用户未评分item的列表
    print(unratedItems)
    if len(unratedItems)==0:return &apos;you rated everything&apos; #如果都已经评过分，则退出
    itemScores=[]
    for item in unratedItems:  #对于每个未评分的item，都计算其预测评分
        estimatedScore=estMethod(dataMat,user,simMeas,item,percentage)
        itemScores.append((item,estimatedScore))
    itemScores=sorted(itemScores,key=lambda x:x[1],reverse=True)#按照item的得分进行从大到小排序
    return itemScores[:N]  #返回前N大评分值的item名，及其预测评分值

testdata=loadExData()
print(recommend(testdata,1,N=3,percentage=0.8))#对编号为1的用户推荐评分较高的3件商品
</code></pre><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p>PCA也常用于数据降维，特别是在人脸识别对于图像数据的处理中，得到了广泛的运用。数据降维的原则是使得数据维度减小，即行向量方差尽可能大，但是同时信息保留最多，就要求行向量之间相关性尽量小，即行向量之间协方差为0.<br>对于数据：</p>
<p><img src="/img/svd/12.png" alt=""></p>
<p>标准化处理先：</p>
<p><img src="/img/svd/13.png" alt=""></p>
<p>那么协方差矩阵：</p>
<p><img src="/img/svd/14.png" alt=""></p>
<p>希望降维后协方差矩阵对角元素极可能大，非对角元素尽可能为0，即成为对角矩阵，则可对X进行线性变换，Y=QX,那么：</p>
<p><img src="/img/svd/15.png" alt=""></p>
<p>所以，Q为CX的特征向量，其方差为特征值，进行降维一般取前r个特征值对应的特征向量，转换结果为：</p>
<p><img src="/img/svd/16.png" alt=""></p>
<p>另一种证明方式可见：<a href="http://blog.csdn.net/zhongkejingwang/article/details/42264479" target="_blank" rel="external">http://blog.csdn.net/zhongkejingwang/article/details/42264479</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近的机器学习算法学习到了主成分分析（PCA），在人脸识别中对样本数据进行了降维，借此对特征值分解（EVD）、奇异值分解（SVD）进行了梳理整理。&lt;/p&gt;
&lt;h2 id=&quot;特征值分解（EVD&quot;&gt;&lt;a href=&quot;#特征值分解（EVD&quot; class=&quot;headerlink&quot; title=&quot;特征值分解（EVD)&quot;&gt;&lt;/a&gt;特征值分解（EVD)&lt;/h2&gt;&lt;p&gt;矩阵是一种线性变换，比如矩阵Ax=y中，矩阵A将向量x线性变换到另一个矩阵y，这个过程中包含3类效应：旋转、缩放以及投影。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>网易云音乐爬虫连载（1）之热门歌单音乐获取</title>
    <link href="http://yoursite.com/2017/10/25/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%88%AC%E8%99%AB%E8%BF%9E%E8%BD%BD%EF%BC%881%EF%BC%89%E4%B9%8B%E7%83%AD%E9%97%A8%E6%AD%8C%E5%8D%95%E9%9F%B3%E4%B9%90%E8%8E%B7%E5%8F%96/"/>
    <id>http://yoursite.com/2017/10/25/网易云音乐爬虫连载（1）之热门歌单音乐获取/</id>
    <published>2017-10-25T08:30:47.000Z</published>
    <updated>2019-03-19T08:52:28.854Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想做文本挖掘方面的工作，想到了获取网易云音乐平台的用户评论以及音乐数据，这可以作为一个文本挖掘以及推荐系统的很好的数据来源。诚然，获取大量的数据涉及到内容还是挺多的，因此从本文开始做一个连载，记录今后对网易云音乐数据的爬取，以及今后对于获取的数据进行分析，作为机器学习的素材进一步处理。</p>
<p>作为连载的第一篇，首先就是介绍基本的网易云音乐信息获取，以及音乐评论的获取。<br><a id="more"></a></p>
<p>为了获得大量的音乐数据，从网易云音乐首页的热门歌单中入手，获取音乐信息。</p>
<p><img src="/img/wangyiyun/1.png" alt=""></p>
<p>用谷歌开发者工具发现，其实获取热门歌单的时候，网易云的请求包中的网址并不是截图中浏览器的<a href="http://music.163.com/#/discover/playlist，而是http://music.163.com/discover/playlist" target="_blank" rel="external">http://music.163.com/#/discover/playlist，而是http://music.163.com/discover/playlist</a></p>
<p><img src="/img/wangyiyun/2.png" alt=""></p>
<p>所以说爬虫的时候，不能单纯看浏览器的url，还是得看真实发送的请求包中的数据。</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import codecs
url = &apos;http://music.163.com/discover/playlist&apos;
url_top = &apos;http://music.163.com&apos;
headers = {
    &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,
    &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.9&apos;,
    &apos;Host&apos;: &apos;music.163.com&apos;,
    &quot;User-Agent&quot;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3218.0 Safari/537.36&apos;,
}
</code></pre><p>分析获取的html文档，采用的是Beautiful Soup，这个时候首先查看html文档，找到歌单数据所在位置。</p>
<p><img src="/img/wangyiyun/3.png" alt=""></p>
<p>发现一个id属性，id属性在一个html文档中是独一无二的，可以据此定位找出我们要的歌单信息。</p>
<pre><code>a=requests.get(url,headers=headers)
html = a.content
soup=BeautifulSoup(html,&apos;lxml&apos;)
playlist={}
f=codecs.open(&apos;playlist.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)
PlaylistBlock = soup.select(&apos;#m-pl-container&apos;)[0].select(&apos;.msk&apos;)
for piece in PlaylistBlock:
    playlist[piece.get(&apos;title&apos;)]=piece.get(&apos;href&apos;)
    f.write(piece.get(&apos;title&apos;)+&apos;: &apos;+piece.get(&apos;href&apos;)+&apos;\n&apos;)
f.close()
</code></pre><p>现在仅仅是获取了第一页的热门歌单：</p>
<p><img src="/img/wangyiyun/4.png" alt=""></p>
<p>歌单的链接都是相对链接，只要加上<a href="http://music.163.com就可以访问到相应的具体歌单，来进一步获取歌单内的歌曲。" target="_blank" rel="external">http://music.163.com就可以访问到相应的具体歌单，来进一步获取歌单内的歌曲。</a></p>
<p><img src="/img/wangyiyun/5.png" alt=""></p>
<pre><code>f=codecs.open(&apos;musiclist.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)
musiclist = []

for description,url_music in playlist.items():
    html = requests.get(url_top+url_music,headers=headers).content
    soup = BeautifulSoup(html,&apos;lxml&apos;)
    songs= soup.find(&apos;ul&apos;,{&apos;class&apos;:&apos;f-hide&apos;}).find_all(&apos;a&apos;)
    music={}
    f.writelines(description+&apos;\n&apos;)
    for song in songs:
        music[song[&apos;href&apos;]] = song.string
        a = music_info_get(url_top+song[&apos;href&apos;],headers)
        f.write(song[&apos;href&apos;]+&apos;:  &apos;+song.string+a+&apos;\n&apos;)
    musiclist.append(music)
    f.write(&apos;---------------------------------&apos;+&apos;\n&apos;)

f.close()
</code></pre><p>由于在歌单页面没有查到歌曲的对应歌手、专辑信息（理论上应该有的，但是我并没有找到），所以考虑进一步进到歌曲页面，可以看到歌曲的详细信息，后期主要在歌曲页面进行信息获取，所以在这里先进到歌曲页面获取歌手、专辑信息。</p>
<p>以赵雷的《成都》为例，进入歌曲页面</p>
<p><img src="/img/wangyiyun/6.png" alt=""></p>
<p>提取出歌曲的详细信息：</p>
<pre><code>def music_info_get(url,headers):
    html = requests.get(url, headers=headers).content
    soup = BeautifulSoup(html, &apos;lxml&apos;)
    a = soup.find(&apos;meta&apos;, {&apos;name&apos;: &apos;description&apos;})[&apos;content&apos;]
    return a
</code></pre><p>《成都》的网页中包含歌词、评论等信息，但是在请求返回的数据包中并没有见到，刷新《成都》页面其实有很多请求，再仔细查看之后，在其他请求中看到了包括评论以及歌词信息。</p>
<p><img src="/img/wangyiyun/7.png" alt=""></p>
<p>在这个post请求包中，返回数据中有《成都》的音乐评论，post和get方式不同，post需要带参数，在request的header中可以看到有两个参数，</p>
<p><img src="/img/wangyiyun/8.png" alt=""></p>
<p>将这个数据带上，用request.post也确实得到了评论数据。</p>
<pre><code>params = &apos;5iLo/oxg1fK3aTLbh99GhtE6AnWBnEGVKMt4iDi6Qm9ag54eFjI/XRn2rI6QOAk8Zj6u2eS7NkRu04mUakNwntZMQrf9f6cdN6PWZuB16f0CgA0N/5IOl7tUXKZCbsduXzfpYCExtIvLDlOeu9LkGpUksFW3O0zq5ZTjRc1MrB49sxRvF8NA+U9LIMvhJHmO&apos;
encSecKey = &apos;3ae5b6afde65dede52224db59c2cc8e46aac937dd95915ba6538859aa0615cb6aa938a118fd6f473256fc5cf95d8c3821b07264d7189c07db922088b711a357e3f2092e5a10df5e3d6008a0314adcb8817fc3fe14a2ee657a0a2221597cc51a78534043a1429484a251e4b2b9128fe042d821b7e862114207773cbdba951c8a2&apos;
data={&quot;params&quot;: params, &quot;encSecKey&quot;: encSecKey}
a = requests.post(url,headers=headers,data=data)
</code></pre><p>但是post所带上的参数data，看起来应该是跟数据加密相关，每个请求应该不一样，在爬取大量音乐的时候没办法对每首歌的参数都去手动获得，这里在网上<a href="https://www.zhihu.com/question/36081767" target="_blank" rel="external">https://www.zhihu.com/question/36081767</a>看到了一个方法，之后可以参照来实现。<br>得到的评论数据：<br><img src="/img/wangyiyun/9.png" alt=""><br>还有就是请求歌曲的评论数据的url，对《成都》来说是<a href="http://music.163.com/weapi/v1/resource/comments/R_SO_4_436514312?csrf_token=，其规律就是R_SO_4_之后的数字为歌曲对应的id，比如《成都》的URL：http://music.163.com/#/song?id=436514312" target="_blank" rel="external">http://music.163.com/weapi/v1/resource/comments/R_SO_4_436514312?csrf_token=，其规律就是R_SO_4_之后的数字为歌曲对应的id，比如《成都》的URL：http://music.163.com/#/song?id=436514312</a></p>
<p>爬虫进行到这，仅仅是从歌单到歌曲，再到歌曲信息以及评论单纯地获取了一遍，对于今后大量歌单大规模的爬取，还需要考虑很多，比如多线程爬虫，结合数据库的爬虫数据存储，以及对于长时间爬虫如何应对发爬虫策略，仅仅是获取数据就还有这么多坑，先Mark一下，留着坑慢慢填。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近想做文本挖掘方面的工作，想到了获取网易云音乐平台的用户评论以及音乐数据，这可以作为一个文本挖掘以及推荐系统的很好的数据来源。诚然，获取大量的数据涉及到内容还是挺多的，因此从本文开始做一个连载，记录今后对网易云音乐数据的爬取，以及今后对于获取的数据进行分析，作为机器学习的素材进一步处理。&lt;/p&gt;
&lt;p&gt;作为连载的第一篇，首先就是介绍基本的网易云音乐信息获取，以及音乐评论的获取。&lt;br&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>BP网络及python实现</title>
    <link href="http://yoursite.com/2017/10/23/BP%E7%BD%91%E7%BB%9C%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2017/10/23/BP网络及python实现/</id>
    <published>2017-10-23T08:48:07.000Z</published>
    <updated>2019-03-19T10:05:25.947Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>BP神经网络改变了感知器的结构，引入了新的隐含层以及误差反向传播，基本上能够解决非线性分类问题，也是神经网络的基础网络结构，在此对BP神经网络算法进行总结，并用python对其进行了实现。<br><a id="more"></a></p>
<p>BP神经网络的典型结构如下图所示：</p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1508759278442&amp;di=35b034d166ee7a0c6e09c7154c096d3f&amp;imgtype=0&amp;src=http%3A%2F%2Fimgsrc.baidu.com%2Fbaike%2Fpic%2Fitem%2F9922720e0cf3d7ca65c52b8ef01fbe096b63a912.jpg" alt=""></p>
<p>隐含层通常为一层，也可以是多层，在BP网络中一般不超过2层。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>正向传播的过程与感知器类似，都是输入与权重的点积，隐含层和输出层都包含一个激活函数，BP网络常用sigmod函数。</p>
<p><img src="/img/bp/1.png" alt=""></p>
<p>但是现在好像不常用了，更多地是Tanh或者是ReLU，好像最近又出了一个全新的激活函数，后续还得去了解。<br>BP神经网络的误差函数是全局误差，将所有样本的误差都进行计算求和，所以在算法过程学习的时候，进行的是批量学习，等所有数据都进行批次计算之后，才进行权重调整。</p>
<p><img src="/img/bp/2.png" alt=""></p>
<h2 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h2><p>这个可以说是BP网络比较精髓的部分了，也是BP网络能够从数据中学习的关键，误差的反向传播过程就是两种情况，要么输出层神经元，要么是隐含层神经元。</p>
<p><img src="/img/bp/3.png" alt=""></p>
<p>对于输出神经元，权重的梯度修正法则为：</p>
<p><img src="/img/bp/4.png" alt=""></p>
<p>即权重增量等于学习率、局域梯度、输出层输出结果的乘积，对于局域梯度，其计算如下：</p>
<p><img src="/img/bp/5.png" alt=""></p>
<p>即为误差信号乘于激活函数的导数，其中n表示第n次迭代。<br>对于sigmod函数来说，其导数为：</p>
<p><img src="/img/bp/6.png" alt=""></p>
<p>对于隐藏层来说，情况更加复杂一点，需要经过上一层的误差传递。</p>
<p><img src="/img/bp/7.png" alt=""></p>
<p>隐藏层的局域梯度为：</p>
<p><img src="/img/bp/8.png" alt=""></p>
<p>上面式子的第一项，说明隐含层神经元j局域梯度的计算仅以来神经元j的激活函数的导数，但是第二项求和，是上一层神经元的局域梯度通过权重w进行了传递。</p>
<p>总的来说，反向传播算法中，权重的调整值规则为：</p>
<p>（权值调整）=（学习率参数） X （局域梯度） X（神经元j的输入信号）</p>
<p>BP算法中还有一个动量因子（mc），主要是网络调优，防止网络发生震荡或者收敛过慢，其基本思想就是在t时刻权重更新的时候考虑t-1时刻的梯度值。</p>
<pre><code>self.out_wb = self.out_wb + (1-self.mc)*self.eta*dout_wb + self.mc*self.eta*dout_wbold
self.hi_wb =self.hi_wb + (1-self.mc)*self.eta*dhi_wb + self.mc*self.eta*dhi_wbold
</code></pre><h2 id="BP网络分类算法"><a href="#BP网络分类算法" class="headerlink" title="BP网络分类算法"></a>BP网络分类算法</h2><p>首先构造的一个BP类</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Mon Oct 23 09:12:46 2017

@author: lkj
&quot;&quot;&quot;

import numpy as np
from numpy import *
import matplotlib.pyplot as plt

class BpNet(object):
    def __init__(self):
        # 以下参数需要手动设置  
        self.eb=0.01              # 误差容限，当误差小于这个值时，算法收敛，程序停止
        self.eta=0.1             # 学习率
        self.mc=0.3               # 动量因子：引入的一个调优参数，是主要的调优参数 
        self.maxiter=2000         # 最大迭代次数
        self.errlist=[]           # 误差列表
        self.dataMat=0            # 训练集
        self.classLabels=0        # 分类标签集
        self.nSampNum=0             # 样本集行数
        self.nSampDim=0             # 样本列数
        self.nHidden=4           # 隐含层神经元 
        self.nOut=1              # 输出层个数
        self.iterator=0            # 算法收敛时的迭代次数

    #激活函数
    def logistic(self,net):
        return 1.0/(1.0+exp(-net))

    #反向传播激活函数的导数
    def dlogistic(self,y):
        return (y*(1-y))

    #全局误差函数
    def errorfuc(self,x):
        return sum(x*x)*0.5

    #加载数据集
    def loadDataSet(self,FileName):
        data=np.loadtxt(FileName)
        m,n=shape(data)
        self.dataMat = np.ones((m,n))
        self.dataMat[:,:-1] = data[:,:-1] #除数据外一列全为1的数据，与权重矩阵中的b相乘
        self.nSampNum = m  #样本数量
        self.nSampDim = n-1  #样本维度
        self.classLabels =data[:,-1]    

    #数据集归一化，使得数据尽量处在同一量纲，这里采用了标准归一化
    #数据归一化应该针对的是属性，而不是针对每条数据
    def normalize(self,data):
        [m,n]=shape(data)
        for i in range(n-1):
            data[:,i]=(data[:,i]-mean(data[:,i]))/(std(data[:,i])+1.0e-10)
        return data

    #隐含层、输出层神经元权重初始化
    def init_WB(self):
        #隐含层
        self.hi_w = 2.0*(random.rand(self.nSampDim,self.nHidden)-0.5)
        self.hi_b = 2.0*(random.rand(1,self.nHidden)-0.5)
        self.hi_wb = vstack((self.hi_w,self.hi_b))

        #输出层
        self.out_w = 2.0*(random.rand(self.nHidden,self.nOut)-0.5)
        self.out_b = 2.0*(random.rand(1,self.nOut)-0.5)
        self.out_wb = vstack((self.out_w,self.out_b))

    def BpTrain(self):
        SampIn = self.dataMat
        expected = self.classLabels
        dout_wbold = 0.0
        dhi_wbold = 0.0 #记录隐含层和输出层前一次的权重值，初始化为0
        self.init_WB()

        for i in range(self.maxiter):
            #信号正向传播
            #输入层到隐含层
            hi_input = np.dot(SampIn,self.hi_wb)
            hi_output = self.logistic(hi_input)
            hi2out = np.hstack((hi_output,np.ones((self.nSampNum,1))))

            #隐含层到输出层
            out_input=np.dot(hi2out,self.out_wb)
            out_output = self.logistic(out_input)
            #计算误差
            error = expected.reshape(shape(out_output)) - out_output
            sse = self.errorfuc(error)
            self.errlist.append(sse)
            if sse&lt;=self.eb:
                self.iterator = i+1
                break

            #误差反向传播

            #DELTA输出层梯度
            DELTA = error*self.dlogistic(out_output)
            #delta隐含层梯度
            delta =  self.dlogistic(hi_output)*np.dot(DELTA,self.out_wb[:-1,:].T)
            dout_wb = np.dot(hi2out.T,DELTA)
            dhi_wb = np.dot(SampIn.T,delta)

            #更新输出层和隐含层权值
            if i==0:
                self.out_wb = self.out_wb + self.eta*dout_wb
                self.hi_wb = self.hi_wb + self.eta*dhi_wb
            else:
                #加入动量因子
               self.out_wb = self.out_wb + (1-self.mc)*self.eta*dout_wb + self.mc*self.eta*dout_wbold
               self.hi_wb =self.hi_wb + (1-self.mc)*self.eta*dhi_wb + self.mc*self.eta*dhi_wbold
            dout_wbold = dout_wb
            dhi_wbold = dhi_wb

    ##输入测试点，输出分类结果      
    def BpClassfier(self,start,end,steps=30):
        x=linspace(start,end,steps)
        xx=np.ones((steps,steps))
        xx[:,0:steps] = x
        yy = xx.T
        z = np.ones((steps,steps))
        for i in  range(steps):
            for j in range(steps):
                xi=array([xx[i,j],yy[i,j],1])
                hi_input = np.dot(xi,self.hi_wb)
                hi_out = self.logistic(hi_input)
                hi_out = mat(hi_out)
                m,n=shape(hi_out)
                hi_b = ones((m,n+1))
                hi_b[:,:n] = hi_out
                out_input = np.dot(hi_b,self.out_wb)
                out = self.logistic(out_input)
                z[i,j] = out
        return x,z

    def classfyLine(self,plt,x,z):
        #画出分类分隔曲线，用等高线画出
        plt.contour(x,x,z,1,colors=&apos;black&apos;)

    def errorLine(self,plt,color=&apos;r&apos;):
        x=linspace(0,self.maxiter,self.maxiter)
        y=log2(self.errlist)
        #y=y.reshape(())
        #print(shape(x),shape(y))
        plt.plot(x,y,color)

   # 绘制数据散点图
    def drawDataScatter(self,plt):
        i=0
        for data in self.dataMat:
            if(self.classLabels[i]==0):
                plt.scatter(data[0],data[1],c=&apos;blue&apos;,marker=&apos;o&apos;)
            else:
                plt.scatter(data[0],data[1],c=&apos;red&apos;,marker=&apos;s&apos;)
            i=i+1
</code></pre><p>利用分类器执行分类：</p>
<pre><code>from BpNet import *
import matplotlib.pyplot as plt 

# 数据集
bpnet = BpNet() 
bpnet.loadDataSet(&quot;testSet2.txt&quot;)
bpnet.dataMat = bpnet.normalize(bpnet.dataMat)

# 绘制数据集散点图
bpnet.drawDataScatter(plt)

# BP神经网络进行数据分类
bpnet.BpTrain()

print(bpnet.out_wb)
print(bpnet.hi_wb)

# 计算和绘制分类线
x,z = bpnet.BpClassfier(-3.0,3.0)
bpnet.classfyLine(plt,x,z)
plt.show()
# 绘制误差曲线
bpnet.errorLine(plt)
plt.show()            
</code></pre><p>输出结果为：</p>
<p><img src="/img/bp/9.png" alt=""></p>
<p>误差输出结果：</p>
<p><img src="/img/bp/10.png" alt=""></p>
<p>可以看到在1000次左右迭代就已经出现了比较好的结果了。<br>具体代码可见个人github仓库<a href="https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/BpNet" target="_blank" rel="external">https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/BpNet</a>        </p>
<p>除了分类，BP神经网络也常用在函数逼近，这时候输出层神经元激活函数一般就不会再采用sigmod函数了，通常采用线性函数。</p>
<p><strong>【参考文献】</strong><br>《神经网络与机器学习》（第3版） （加） Simon Haykin 著；<br>《机器学习算法原理与编程实践》 郑捷著；</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BP神经网络改变了感知器的结构，引入了新的隐含层以及误差反向传播，基本上能够解决非线性分类问题，也是神经网络的基础网络结构，在此对BP神经网络算法进行总结，并用python对其进行了实现。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost算法笔记</title>
    <link href="http://yoursite.com/2017/10/22/AdaBoost%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/10/22/AdaBoost算法/</id>
    <published>2017-10-22T07:35:19.000Z</published>
    <updated>2019-03-19T08:59:55.195Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>AdaBoost属于机器学习中的集成学习，其基本思想是基于“弱学习算法”集成强化为“强学习算法”，在分类中就是组合弱分类器得到强分类器。实际中很容易得到正确率不是非常高的弱分类器(当然，通常来说at least &gt;50%），通过Adaboost算法，来组合得到正确率很高的强分类器。<br><a id="more"></a><br>从上述就可以知道，AdaBoost算法有两部分，顶层是主算法，底层是其他的分类算法，可以是决策树，SVM之类的算法</p>
<p><img src="/img/adaboost/1.jpg" alt=""></p>
<p>AdaBoost算法的核心思想就是：加大分类错误数据集的权重，以便迭代中进一步分类；加大正确率高的分类器在最终分类结果表决中的权重。</p>
<p>对算法进一步阐述，考虑简单的二分类问题。</p>
<ol>
<li>初始化数据集的权值分布，初始化为相等的数值</li>
</ol>
<p><img src="/img/adaboost/2.png" alt=""></p>
<ol>
<li>使用训练集学习，得到基本分类器Gm,计算其分类误差率</li>
</ol>
<p><img src="/img/adaboost/3.png" alt=""></p>
<ol>
<li>计算分类器Gm的系数</li>
</ol>
<p><img src="/img/adaboost/4.png" alt=""></p>
<p>am随着em增大而减小，这样分类器在最终组合分类器中权重会下降，同时还需要据此来更新数据集的权重。</p>
<ol>
<li>更新数据集权重分布</li>
</ol>
<p><img src="/img/adaboost/5.png" alt=""></p>
<p>其中Zm为规范化因子，是的Dm+1成一个概率分布。</p>
<ol>
<li>构造弱分类器的组合<br>通过权重值的线性组合得到强分类器<br><img src="/img/adaboost/6.png" alt=""></li>
</ol>
<p>上述过程迭代M次，或者是分类精度达到要求</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AdaBoost属于机器学习中的集成学习，其基本思想是基于“弱学习算法”集成强化为“强学习算法”，在分类中就是组合弱分类器得到强分类器。实际中很容易得到正确率不是非常高的弱分类器(当然，通常来说at least &amp;gt;50%），通过Adaboost算法，来组合得到正确率很高的强分类器。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
