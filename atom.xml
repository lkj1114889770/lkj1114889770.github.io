<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>听歌的小孩</title>
  <subtitle>好好学习，好好科研</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-13T05:12:23.561Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>听歌的小孩</name>
    <email>kaijianliu@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Synthesize image for Chinese text recognition</title>
    <link href="http://yoursite.com/2018/09/12/Synthesize%20image%20for%20Chinese%20text%20recognition/"/>
    <id>http://yoursite.com/2018/09/12/Synthesize image for Chinese text recognition/</id>
    <published>2018-09-12T07:49:30.000Z</published>
    <updated>2018-09-13T05:12:23.561Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出<a href="http://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="external">SynthText</a>方法合成自然场景下的文本图片，github上有作者给出的<a href="https://github.com/ankush-me/SynthText" target="_blank" rel="external">官方代码</a>，也有国内大神改写的<a href="https://github.com/JarveeLee/SynthText_Chinese_version" target="_blank" rel="external">中文版本代码</a>。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。</p>
<a id="more"></a>
<h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><p>借鉴了SynthText的方法，而且包括语料库、图像背景图、字体、以及色彩模型文件，都是来源于@JarveeLee的中文版代码中的文件。</p>
<ol>
<li>读取语料库，此处来源为一些童话故事txt，</li>
<li>随机取一段字符串，满足所需长度，再随机选择字体、字号大小</li>
<li>在提供的背景图中，随机取一张图，计算裁剪图的Lab值标准差（标准差越小图像色彩分布就不会太过丰富、太过花哨），小于设定的阈值则再根据字体字号计算出的文本尺寸，在原图上进行随机裁剪，可以以一定概率使文本在最红图片中有一定偏移；可以以一定概率随机产生竖直文本。</li>
<li>通过聚类的方法，分析裁剪后图的色彩分布，在色彩模型提供的色彩库中选择与当前裁剪图像色彩偏差大的作为文本颜色，这样最终构成合成图片</li>
</ol>
<h2 id="构建方法"><a href="#构建方法" class="headerlink" title="构建方法"></a>构建方法</h2><p>主要实现代码只有一个文件，其他都是合成需要的文件，合成命令：</p>
<pre><code>python gen_dataset.py
</code></pre><p>newsgroup:文本来源的语料<br>models/colors_new.cp:从III-5K数据集学习到的色彩模型<br>fonts：包含合成时所需字体<br>所需图片bg_img来源于VGG组合成synth_80k时所用的图片集</p>
<ul>
<li>bg_img.tar.gz [8.9G]：压缩的图像文件（需要使用使用imnames.cp中的过滤），链接<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/ bg_img.tar.gz" title="http://zeus.robots.ox.ac.uk/textspot/static/db/ bg_img.tar.gz" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/ bg_img.tar.gz</a>：</li>
<li>imnames.cp[180K]：已过滤文件的名称，即，这些文件不包含文本,链接：<a href="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" title="http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp" target="_blank" rel="external">http://zeus.robots.ox.ac.uk/textspot/static/db/imnames.cp</a></li>
</ul>
<h2 id="一些实现结果样例"><a href="#一些实现结果样例" class="headerlink" title="一些实现结果样例"></a>一些实现结果样例</h2><p><img src="/img/img_1.jpg" alt=""></p>
<p><img src="/img/img_2.jpg" alt=""></p>
<p><img src="/img/img_3.jpg" alt=""></p>
<p><img src="/img/img_4.jpg" alt=""></p>
<p>详细实现代码可参见个人github。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本识别数据集需要大量的数据，特别是对于中文来说，中文字相对英文26个字母来说，更加复杂，数量多得多，所以需要有体量比较大的数据集才能训练得到不错的效果，目前也有一些合成的方法，VGG组就提出&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/scenetext/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SynthText&lt;/a&gt;方法合成自然场景下的文本图片，github上有作者给出的&lt;a href=&quot;https://github.com/ankush-me/SynthText&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方代码&lt;/a&gt;，也有国内大神改写的&lt;a href=&quot;https://github.com/JarveeLee/SynthText_Chinese_version&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;中文版本代码&lt;/a&gt;。但是生成的速度非常慢，而且生成机制有点复杂，总是报错，短时间内还没解决。我的需求场景仅仅是识别文字，并没有涉及到检测部分，所以不需要完整的场景图片，所以提出一种方法来合成中文文本图片用于文本识别，分享一下实现思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OCR" scheme="http://yoursite.com/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow输入数据处理框架</title>
    <link href="http://yoursite.com/2018/07/25/Tensorflow%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6/"/>
    <id>http://yoursite.com/2018/07/25/Tensorflow输入数据处理框架/</id>
    <published>2018-07-25T13:55:36.000Z</published>
    <updated>2018-07-25T14:45:55.926Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。</p>
<a id="more"></a>
<h2 id="TFrecords格式介绍"><a href="#TFrecords格式介绍" class="headerlink" title="TFrecords格式介绍"></a>TFrecords格式介绍</h2><p>TFrecords是一种二进制文件，通过tf.train.Example Protocol Buffer的格式存储数据，以下的代码给出了tf.train.Example的定义。</p>
<pre><code>message Example {
    Features features = 1;
};
message Features {
    map&lt;string, Feature&gt; feature = 1;
};
message Feature {
    oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
}
};
</code></pre><p>tf.train.Example包含了一个从属性名称到取值的字典，其中属性名称为一个字符串，属性取值可以是字符串(BytesList)，实数列表(FloatList)或者整数列表(Int64List），比如将解码前的图像存为一个字符串，将lable存为整数列表，或者将bounding box存为实数列表。</p>
<h2 id="COCO数据集的TFrecords文件制作"><a href="#COCO数据集的TFrecords文件制作" class="headerlink" title="COCO数据集的TFrecords文件制作"></a>COCO数据集的TFrecords文件制作</h2><p>COCO数据集是微软做的一个比较大的数据集，可以用来做图像的recognition、segmentation、captioning，我用来做物体检测识别。官方也提供了API操作数据集（<a href="https://github.com/cocodataset/cocoapi" title="https://github.com/cocodataset/cocoapi" target="_blank" rel="external">https://github.com/cocodataset/cocoapi</a>）。根据链接介绍下载安装python的API后，就可以开始Tfrecords的文件制作了。</p>
<pre><code>from pycocotools.coco import COCO
import tensorflow as tf
import numpy as np
from PIL import Image
from time import time
import os

dataDir=&apos;/home/zju/lkj/data/COCO Dataset&apos;
dataType=&apos;train2017&apos;
annFile=&apos;{}/annotations/instances_{}.json&apos;.format(dataDir,dataType)

classes = [&apos;backpack&apos;, &apos;umbrella&apos;, &apos;handbag&apos;, &apos;tie&apos;, &apos;suitcase&apos;, &apos;bottle&apos;, &apos;wine glass&apos;, &apos;cup&apos;, &apos;fork&apos;, &apos;knife&apos;,
            &apos;spoon&apos;, &apos;bowl&apos;, &apos;banana&apos;, &apos;apple&apos;, &apos;sandwich&apos;, &apos;orange&apos;, &apos;broccoli&apos;, &apos;carrot&apos;, &apos;hot dog&apos;,
            &apos;donut&apos;, &apos;cake&apos;, &apos;chair&apos;, &apos;couch&apos;, &apos;potted plant&apos;, &apos;bed&apos;, &apos;dining table&apos;, &apos;toilet&apos;, &apos;tv&apos;, &apos;laptop&apos;,
            &apos;mouse&apos;, &apos;remote&apos;, &apos;keyboard&apos;, &apos;cell phone&apos;, &apos;microwave&apos;, &apos;oven&apos;, &apos;toaster&apos;, &apos;sink&apos;, &apos;refrigerator&apos;,
            &apos;book&apos;, &apos;clock&apos;, &apos;vase&apos;, &apos;scissors&apos;, &apos;teddy bear&apos;, &apos;hair drier&apos;, &apos;toothbrush&apos;]

# initialize COCO api for instance annotations
coco = COCO(annFile)
classesId = coco.getCatIds(classes)
imgIds = coco.getImgIds()
img_filters=[]
for imgId in imgIds:
    Anns = coco.loadAnns(coco.getAnnIds(imgIds=imgId))
    annIds = list(map(lambda x:x[&apos;category_id&apos;],Anns))
    for annId in annIds:
        if annId in classesId:
            img_filters.append(imgId)
img_filters = set(img_filters)


# 归一化
# size: 图片大小
# box：[x,y,w,h]
# return 归一化结果
def convert(size,box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = box[0]+box[2]/2.0
    y = box[1]+box[3]/2.0
    x = x*dw
    w = box[2]*dw
    y = y*dh
    h = box[3]*dh
    return [x,y,w,h]


def convert_img(img_id):
    img_id_str = str(img_id).zfill(12)
    img_path = &apos;{}/{}/{}.jpg&apos;.format(dataDir, dataType, img_id_str)
    image = Image.open(img_path)
    resized_image = image.resize((416, 416), Image.BICUBIC)
    image_data = np.array(resized_image, dtype=&apos;float32&apos;) / 255
    if image_data.size != 519168: # 不为3通道
        return False
    img_raw = image_data.tobytes()
    return img_raw

def convert_annotation(image_id):
    img_info = coco.loadImgs(image_id)[0]  # 读入的是照片的详细信息，而非图像信息, 返回的是list，只有1个id输入时，取0
    w = int(img_info[&apos;width&apos;])
    h = int(img_info[&apos;height&apos;])
    bboxes = []
    Anns = coco.loadAnns(ids=coco.getAnnIds(imgIds=image_id))
    i = 0
    for Ann in Anns:
        if i&gt;29:
            break
        iscrowd = Ann[&apos;iscrowd&apos;]
        if iscrowd == 1:
            continue
        if Ann[&apos;category_id&apos;] not in classesId:
            continue
        cls_id = classesId.index(Ann[&apos;category_id&apos;])  # 取新的编号
        bbox = Ann[&apos;bbox&apos;]
        bb = convert((w, h), bbox) + [cls_id]
        bboxes.extend(bb)
        i = i + 1

    if len(bboxes) &lt; 30*5:
        bboxes = bboxes + [0, 0, 0, 0, 0]*(30-int(len(bboxes)/5))
    return np.array(bboxes, dtype=np.float32).flatten().tolist()

filename = os.path.join(&apos;train2017&apos;+&apos;.tfrecords&apos;)
writer = tf.python_io.TFRecordWriter(filename)
i=0
start = time()
for imgId in img_filters:
    xywhc = convert_annotation(imgId)
    img_raw = convert_img(imgId)
    if img_raw:
        example = tf.train.Example(features=tf.train.Features(feature={
            &apos;xywhc&apos;:
                    tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
            &apos;img&apos;:
                    tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
            }))
        writer.write(example.SerializeToString())
        # 显示制作进度，剩余时间
        if i%100==99:
            t = time()-start
            print(i,&apos;t={:0.4f}s/100 step&apos;.format(t),&apos;  left time={:0.4f}s&apos;.format((len(img_filters)-i)*t/100))
            start = time()
        i = i+1
print(&apos;Done!&apos;)
writer.close()
</code></pre><p>下面分段对代码进行介绍，这个数据制作是应用于物品检查与分割，并且只有部分物品，所以在程序开头有classes列举（总共45种，完整的COCO数据集包含91种）。COCO数据集中混有灰度图，所以在reshape的时候会一直报错，刚开始还一直想不清楚为什么，后来遍历原始数据集才发现有灰度图的存在，所以reshape成416*416*3会报错,所以程序有一个判断是否为3通道：</p>
<pre><code>image = Image.open(img_path)
resized_image = image.resize((416, 416), Image.BICUBIC)
image_data = np.array(resized_image, dtype=&apos;float32&apos;) / 255
if image_data.size != 519168: # 不为3通道
    return False
</code></pre><p>图像读取后转换成字符串(BytesList):</p>
<pre><code>img_raw = image_data.tobytes()
</code></pre><p>bounding box转换成实数列表(FloatList):</p>
<pre><code>return np.array(bboxes, dtype=np.float32).flatten().tolist()
</code></pre><p>基于此，核心的构建部分为：</p>
<pre><code>example = tf.train.Example(features=tf.train.Features(feature={
    &apos;xywhc&apos;:
            tf.train.Feature(float_list=tf.train.FloatList(value=xywhc)),
    &apos;img&apos;:
            tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),
    }))
writer.write(example.SerializeToString())
</code></pre><h2 id="TFrecords文件读取解析"><a href="#TFrecords文件读取解析" class="headerlink" title="TFrecords文件读取解析"></a>TFrecords文件读取解析</h2><p>对应构建时候的数据格式，进行解析，可以加一些程序对于读取后的图像文件的一些进一步处理，比如图像增强</p>
<pre><code>def parser(example):
    features = {
                &apos;xywhc&apos;: tf.FixedLenFeature([150], tf.float32),
                &apos;img&apos;: tf.FixedLenFeature((), tf.string)}
    feats = tf.parse_single_example(example, features)
    coord = feats[&apos;xywhc&apos;]
    coord = tf.reshape(coord, [30, 5])

    img = tf.decode_raw(feats[&apos;img&apos;], tf.float32)
    img = tf.reshape(img, [416, 416, 3])
    img = tf.image.resize_images(img, [cfg.train.image_resized, cfg.train.image_resized])
    rnd = tf.less(tf.random_uniform(shape=[], minval=0, maxval=2), 1)
    # 添加对于读取后的图像文件的一些进一步处理，图像增强
    def flip_img_coord(_img, _coord):
        zeros = tf.constant([[0, 0, 0, 0, 0]]*30, tf.float32)
        img_flipped = tf.image.flip_left_right(_img)
        idx_invalid = tf.reduce_all(tf.equal(coord, 0), axis=-1)
        coord_temp = tf.concat([tf.minimum(tf.maximum(1 - _coord[:, :1], 0), 1),
                               _coord[:, 1:]], axis=-1)
        coord_flipped = tf.where(idx_invalid, zeros, coord_temp)
        return img_flipped, coord_flipped

    img, coord = tf.cond(rnd, lambda: (tf.identity(img), tf.identity(coord)), lambda: flip_img_coord(img, coord))

    img = tf.image.random_hue(img, max_delta=0.1)
    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)
    img = tf.image.random_brightness(img, max_delta=0.1)
    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)
    img = tf.minimum(img, 1.0)
    img = tf.maximum(img, 0.0)
    return img, coord
</code></pre><p>然后构建一个data_pipeline来作为训练数据的输入框架：</p>
<pre><code>def data_pipeline(file_tfrecords, batch_size):
    dt = tf.data.TFRecordDataset(file_tfrecords)
    dt = dt.map(parser, num_parallel_calls=4)
    dt = dt.prefetch(batch_size)
    dt = dt.shuffle(buffer_size=20*batch_size)
    dt = dt.repeat()
    dt = dt.batch(batch_size)
    iterator = dt.make_one_shot_iterator()
    imgs, true_boxes = iterator.get_next()

    return imgs, true_boxes
</code></pre><p>测试一下整个数据输入模块：</p>
<pre><code>file_path = &apos;train2007.tfrecords&apos;
imgs, true_boxes = data_pipeline(file_path, cfg.batch_size)
sess = tf.Session()
imgs_, true_boxes_ = sess.run([imgs, true_boxes])
print(imgs_.shape, true_boxes_.shape)
for imgs_i, boxes_ in zip(imgs_, true_boxes_):
    valid = (np.sum(boxes_, axis=-1) &gt; 0).tolist()
    print([cfg.names[int(idx)] for idx in boxes_[:, 4][valid].tolist()])
    plt.figure()
    plt.imshow(imgs_i)
plt.show()
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensoflow提供了一种统一的数据格式来存储数据，这个格式就是TFrecords，基于TFrecords我们构建一个完整的TensorFlow输入数据处理框架，以COCO数据集为例，介绍了COCO数据集的TFrecords文件制作，以及读取解析的过程，以此来介绍一个构建文件处理框架的过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>表扬一下pycharm</title>
    <link href="http://yoursite.com/2018/07/07/%E8%A1%A8%E6%89%AC%E4%B8%80%E4%B8%8Bpycharm/"/>
    <id>http://yoursite.com/2018/07/07/表扬一下pycharm/</id>
    <published>2018-07-07T15:43:06.000Z</published>
    <updated>2018-07-07T16:02:15.579Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。</p>
<a id="more"></a>
<p><img src="https://i.imgur.com/Hh7lDmv.png" alt=""></p>
<p>炼丹过程中参数修改频繁，还忘了备份已经效果比较好的参数，结果改的调不回去了，本来还有个理想的结果，现在越来越差，真实欲哭无泪，直到发现了上面的那个功能，pycharm是真的优秀，按这样点开，就能发现一天的修改过程，如下图</p>
<p><img src="https://i.imgur.com/ppmhT6f.png" alt=""></p>
<p>可以点开看到修改历史，和现有版本进行对比，还可以导出修改历史。嗯，真的是良心IDE，特此表扬，以资鼓励。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;难得写一篇记录心情博客，表扬一下pycharm，拯救了一下我从大喜到大悲的悲伤，再回到大喜的刺激，其实就是发现了pycharm的一个记录修改过程的功能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="杂" scheme="http://yoursite.com/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>目标检测网络之YOLO学习笔记</title>
    <link href="http://yoursite.com/2018/06/15/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C%E4%B9%8BYOLO%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/06/15/目标检测网络之YOLO学习笔记/</id>
    <published>2018-06-15T06:59:34.000Z</published>
    <updated>2018-07-06T02:10:26.670Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。<br><a id="more"></a></p>
<p>YOLO项目主页<a href="https://pjreddie.com/yolo/" target="_blank" rel="external">地址</a><br>YOLO1 <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">论文</a><br>YOLO2 <a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">论文</a><br>YOLO3 <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">论文</a></p>
<h2 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h2><p>YOLO使用来自整张图片的feature map来预测bounding box和class，因此可以保持较高的精度。YOLO将整张图片分成S×S的网格，如果一个目标的中心落入到网格单元中，那么这个网格单元负责这个目标的检测。</p>
<div align="center"><br>    <img src="https://i.imgur.com/KVhyOJE.png" width="300" height="300"><br></div><br>每个网格单元预测B个bounding box和confidence score，confidence score反应了box包含目标的可信度，论文中将可confidence score定义为：<br><div align="center"><br>    <img src="https://i.imgur.com/6vox4OO.png" height="30"><br></div><br>，因此，如果没有目标存在confidence score为0，否则应该为IOU(intersection over union)，即真实框和预测框的交集部分。所以每个bounding box的预测值包括(x,y,w,h.confidence score). (x,y)表示预测的box中心相对于网格单元的的位置，(w,h)是用整个图片大小进行归一化的宽度和高度，另外，针对C个类别，每个类别需要预测一个条件概率，即：<br><div align="center"><br>    <img src="https://i.imgur.com/VgNV9Rc.png" height="30"><br></div><br>最终得到box中包含某个特定物品的概率为：<br><div align="center"><br>    <img src="https://i.imgur.com/sehftW1.png" height="40"><br></div><br>整个过程如下图所示。<br><div align="center"><br>    <img src="https://i.imgur.com/u90BPbt.png" width="700" height="400"><br></div>

<p>总结来说，YOLO网络将检测问题转换成regression，首先将整张图片转换成S×S的网格，并且每个网格单元预测<br>B个边界框，这些边界框的(x,y,w,h,confidence score)以及C个类别概率,这些预测被编码为S×S×(B*5+C)<br>的张量。</p>
<h3 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h3><p>YOLO1的网络结构设计借鉴了GoodLeNet模型，包含了24个卷积层和2个全连接层，YOLO未使用inception module，而是使用1x1卷积层和）3x3卷积层简单替代，交替出现的1x1卷积层实现了跨通道信息融合以及通道数目降低。</p>
<div align="center"><br>    <img src="https://i.imgur.com/BmDEK3e.png"><br></div>

<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ol>
<li>使用 ImageNet 1000 类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。</li>
<li>用上面得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，加入后面的4层卷积层以及2层全连接层进行detection的训练，detection通常需要有细密纹理的视觉信息,所以为提高图像精度，在训练检测模型时，将输入图像分辨率从224 × 224 resize到448x448。</li>
<li>最后一层预测类概率和边界框坐标。我们通过图像宽度和高度来规范(w,h)，使它们落在0和1之间。我们将边界框(x,y)坐标参数化为特定网格单元位置的偏移量，所以它们边界也在0和1之间。</li>
</ol>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>YOLO1的误差计算对于分类误差和定位误差用了不同的权重，对包含与不包含物品的box的误差权重也进行了区分。具体来说，论文中增加了边界框坐标预测损失，并减少了不包含目标边界框的置信度预测损失，使用两个参数λcoord和λnoobj来完成这个工作，论文中设置了λcoord=5和λnoobj=0.5。<br>另一个问题是平方和误差权重在大框和小框中进行了区分。相同的误差下，小框误差的重要性肯定更好，论文中用了一个很巧妙的方法，<strong>直接预测边界框宽度和高度的平方根，而不是宽度和高度</strong>。根据y=x^1/2的函数就可以知道，函数斜率是随着x的增大而减小的，这样就可以提高小框的误差权重，真的巧妙。<br>YOLO每个网格单元预测多个box。在训练时，每个目标我们只需要一个box来负责，选定的原则是与真实框具有最大的IOU。</p>
<div align="center"><br>    <img src="https://i.imgur.com/CsmNs9B.png"><br></div>

<h3 id="Shortcoming"><a href="#Shortcoming" class="headerlink" title="Shortcoming"></a>Shortcoming</h3><p>YOLO对边界框预测强加空间约束，因为每个网格单元只预测两个盒子，只能有一个类别。这个空间约束限制了我们的模型可以预测的邻近目标的数量，因此在小物品的检测上比较局限。</p>
<h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p>为提高物体定位精准性和召回率，YOLO2对网络结构的设计进行了改进，输出层使用卷积层替代YOLO的全连接层，联合使用coco物体检测标注数据和imagenet物体分类标注数据训练物体检测模型。相比YOLO，YOLO9000在识别种类、精度、速度、和定位准确性等方面都有大大提升。</p>
<h3 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h3><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>YOLO2取消了dropout，在所有的卷积层中加入Batch Normalization。</p>
<h4 id="High-Resolution-Classifier"><a href="#High-Resolution-Classifier" class="headerlink" title="High Resolution Classifier"></a>High Resolution Classifier</h4><p>YOLO2将ImageNet以448×448 的分辨率微调最初的分类网络，迭代10 epochs。</p>
<h4 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a>Convolutional With Anchor Boxes</h4><p>借鉴faster R-CNN的思想，引入anchor box，取消全连接层来进行预测，改用卷积层作为预测层对anchor box的offset和confidence进行预测。去除了一个池化层，使得输出特征具有更高的分辨率，将图片输入尺寸resize为416而非448，使得特征图大小为奇数，所以有一个中心单元格。目标，特别是大目标，倾向于占据图像的中心，所以在中心有一个单一的位置可以很好的预测这些目标，而不是四个位置都在中心附近。YOLO的卷积层将图像下采样32倍，所以通过使用输入图像416，我们得到13×13的输出特征图。同时，使用anchor box进行预测的时候，解耦空间位置预测与类别预测，对每个anchor box都预测object和class，仍然沿用YOLO1，目标检测仍然是预测proposed box和ground truth的IOU，类别预测（class predictions）仍然是存在object下的条件概率。</p>
<h4 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a>Dimension Clusters</h4><p>YOLO2不再采用手动挑选的box尺寸，而是对训练集的box尺寸进行k-means聚类，因为聚类的目的是想要更好的IOU，所以聚类的距离使用下列公式：</p>
<div align="center"><br>    <img src="https://i.imgur.com/F8AsPNZ.png" height="40"><br></div><br>对不同的k值采用k-means聚类算法，即对数据集的ground truth聚类，在VOC和COCO数据集上的bounding box得到的结果如下图：<br><div align="center"><br>    <img src="https://i.imgur.com/EkQOmSz.png"><br></div>

<p>根据上图，k=5的时候，模型的复杂度和IOU能够得到一个不错的trade off。</p>
<h4 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a>Direct location prediction</h4><p>对于位置坐标，YOLO2没有采用R-CNN的预测偏移，而是仍然类似于YOLO1中的，他预测相对于网格单元的位置坐标，将ground truth也限制在0-1之间，使用logistic activation 来实现。网络为每个边界框预测tx，ty，th，tw和to这5个坐标。如果网格单元从图像的左上角偏移（Cx，Cy），给定的anchor的宽度，高度分比为Pw，Ph那么预测结果为：</p>
<div align="center"><br>    <img src="https://i.imgur.com/XShUyy1.png"><br></div><br><div align="center"><br>    <img src="https://i.imgur.com/21a8YXR.png"><br></div>

<h4 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a>Fine-Grained Features</h4><p>在13×13特征图上检测可以很容易检测到大目标，从更小粒度的特征图中可以更好地检测小物体，YOLO2添加一个passthrough layer从前一层26×26的特征图进行融合。传递层通过将相邻特征堆叠到不同的通道而不是堆叠到空间位置，将较高分辨率特征与低分辨率特征相连，类似于ResNet中的标识映射。这将26×26×512特征映射转换为13×13×2048特征映射，其可以与原始特征连接。</p>
<h4 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a>Multi-Scale Training</h4><p>添加anchor box后，YOLO2将分辨率更改为416×416。然而，由于模型只使用卷积层和池化层，它可以在运行中调整大小。为了使YOLOv2能够在不同大小的图像上运行，相比于固定输入图像大小，YOLO2每隔几次迭代更改网络。每迭代10个batch网络随机选择一个新的图像尺寸大小。因为模型以32的因子下采样，YOLO2从以下32的倍数中抽取：{320,352，…，608}。因此，最小的选项是320×320，最大的是608×608.调整网络的大小，并继续训练。<br>这种训练方法迫使网络学习在各种输入维度上很好地预测。这意味着相同的网络可以预测不同分辨率的检测。网络在更小的尺寸下运行更快，因此YOLO2在速度和精度之间提供了一个简单的折衷。</p>
<h3 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h3><h4 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h4><p>YOLO2大多数3×3的过滤器，并在每个池化步骤后将通道数量加倍，使用全局平均池进行预测，使用1×1滤波器以压缩3×3卷积之间的特征，最终模型，称为Darknet-19，有19卷积层和5个最大池化层，详见下图。</p>
<div align="center"><br>    <img src="https://i.imgur.com/LCahDqp.png"><br></div>

<h4 id="Training-for-classification"><a href="#Training-for-classification" class="headerlink" title="Training for classification"></a>Training for classification</h4><p>使用Darknet19在标准ImageNet 1000类分类数据集上训练，在训练期间，使用数据增强技巧。</p>
<h4 id="Training-for-detection"><a href="#Training-for-detection" class="headerlink" title="Training for detection"></a>Training for detection</h4><p>为了训练检测器，修改上面的网络，移除最后的卷积层，添加3个3×3卷积层，最后增加1×1卷积层，其输出为我们需要的检测维度，如对于VOC数据集，预测5个box，每个具有5个坐标，每个box20个类，因此125个过滤器。还添加了从最后的3×3×512层到第二到最后的卷积层的传递层passthrough layer，使得模型可以使用细粒度特征。</p>
<h3 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h3><p>构建了一种分层分类模型（WordTree），提出了一种关于分类和检测数据的联合训练机制。</p>
<div align="center"><br>    <img src="https://i.imgur.com/zdvKkpA.png"><br></div><br><div align="center"><br>    <img src="https://i.imgur.com/TnEqUtA.png"><br></div>

<p>ImageNet数据量更大，用于训练分类，COCO和VOC用于训练检测，ImageN对应分类有9000多种，COCO只有80种对应目标检测，通过wordTree来combine，来自分类的图片只计算分类的loss，来自检测集的图片计算完整的loss。</p>
<h2 id="YOLO-v3"><a href="#YOLO-v3" class="headerlink" title="YOLO v3"></a>YOLO v3</h2><p>YOLO3 对于YOLO2有了一些改进，总的来说有几点：加深了网络，用了上采样，残差网络，多尺度预测，下面详细说明。</p>
<h3 id="Bounding-Box-Prediction"><a href="#Bounding-Box-Prediction" class="headerlink" title="Bounding Box Prediction"></a>Bounding Box Prediction</h3><p>坐标预测仍然沿用YOLO2的，yolov3对每个bounding box预测四个坐标值(tx, ty, tw, th)，对于预测的cell根据图像左上角的偏移(cx, cy)，以及之前得到bounding box的宽和高pw, ph可以对bounding box按如下的方式进行预测：</p>
<div align="center"><br>    <img src="https://i.imgur.com/XShUyy1.png"><br></div>

<p>训练的时候，loss的计算采用sum of squared error loss（平方和距离误差损失），yolov3对每个bounding box通过逻辑回归预测一个物体的得分，如果预测的这个bounding box与真实的边框值大部分重合且比其他所有预测的要好，那么这个值就为1.如果overlap没有达到一个阈值（yolov3中这里设定的阈值是0.5），那么这个预测的bounding box将会被忽略。YOLO3论文中使用的阈值是0.5.每个object只会分配一个bounding box，所以对应没有分配有ground truth object的box，其坐标损失和预测损失不需要计入，只需考虑objectness loss。If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.</p>
<h3 id="Class-Prediction"><a href="#Class-Prediction" class="headerlink" title="Class Prediction"></a>Class Prediction</h3><p>每个框预测分类，bounding box使用多标签分类（multi-label classification）。论文中说没有使用softmax分类，只是使用了简单的逻辑回归进行分类，采用的二值交叉熵损失（binary cross-entropy loss）。<br>Each box predicts the classes the bounding box may contain using multilabel classification. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classifiers. During training we use binary cross-entropy loss for the class predictions.<br>This formulation helps when we move to more complex domains like the Open Images Dataset. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data.</p>
<h3 id="Predictions-Across-Scales"><a href="#Predictions-Across-Scales" class="headerlink" title="Predictions Across Scales"></a>Predictions Across Scales</h3><p>YOLO3在三种不同尺度来预测box，应用一个类似于特征金字塔网络（feature pyramid network）上提取特征，如下图：</p>
<div align="center"><br>    <img src="https://i.imgur.com/AYQngjv.png"><br></div>

<p>对于第一个scale的预测，即base feature extractor，最后预测得到一个3-d tensor，包含bounding box,objectness,class prediction.比如在COCO数据集中有80类物品，每一个scale预测3个box，所以tensor得到为（N×N×[3*(4+1+80)]）。<br>next scale，从上一步2 layer previous的feature map中进行上采样，然后从特征提取网络中的取earlier feature 与上采样后的进行合并，得到更多信息的语义，以及从earlier feature map可以得到更细粒度的特征。最后的scale采用前述类似的方法进行。可能实际代码更能体现这个过程，如下：<br>三种跨尺度预测</p>
<pre><code>predict boxes at 3 different scales
&apos;&apos;&apos;
def build(self, feat_ex, res18, res10):
    self.conv52 = self.conv_layer(feat_ex, 1, 1, 1024, 512, True, &apos;conv_head_52&apos;)          # 13x512
    self.conv53 = self.conv_layer(self.conv52, 3, 1, 512, 1024, True, &apos;conv_head_53&apos;)   # 13x1024
    self.conv54 = self.conv_layer(self.conv53, 1, 1, 1024, 512, True, &apos;conv_head_54&apos;)   # 13x512
    self.conv55 = self.conv_layer(self.conv54, 3, 1, 512, 1024, True, &apos;conv_head_55&apos;)   # 13x1024
    self.conv56 = self.conv_layer(self.conv55, 1, 1, 1024, 512, True, &apos;conv_head_56&apos;)   # 13x512
    self.conv57 = self.conv_layer(self.conv56, 3, 1, 512, 1024, True, &apos;conv_head_57&apos;)   # 13x1024
    self.conv58 = self.conv_layer(self.conv57, 1, 1, 1024, 75, False, &apos;conv_head_58&apos;)   # 13x75
    # follow yolo layer mask = 6,7,8
    self.conv59 = self.conv_layer(self.conv56, 1, 1, 512, 256, True, &apos;conv_head_59&apos;)    # 13x256
    size = tf.shape(self.conv59)[1]
    self.upsample0 = tf.image.resize_nearest_neighbor(self.conv59, [2*size, 2*size],    # 上采样
                                                      name=&apos;upsample_0&apos;)                # 26x256
    self.route0 = tf.concat([self.upsample0, res18], axis=-1, name=&apos;route_0&apos;)           # 26x768
    self.conv60 = self.conv_layer(self.route0, 1, 1, 768, 256, True, &apos;conv_head_60&apos;)    # 26x256
    self.conv61 = self.conv_layer(self.conv60, 3, 1, 256, 512, True, &apos;conv_head_61&apos;)    # 26x512
    self.conv62 = self.conv_layer(self.conv61, 1, 1, 512, 256, True, &apos;conv_head_62&apos;)    # 26x256
    self.conv63 = self.conv_layer(self.conv62, 3, 1, 256, 512, True, &apos;conv_head_63&apos;)    # 26x512
    self.conv64 = self.conv_layer(self.conv63, 1, 1, 512, 256, True, &apos;conv_head_64&apos;)    # 26x256
    self.conv65 = self.conv_layer(self.conv64, 3, 1, 256, 512, True, &apos;conv_head_65&apos;)    # 26x512
    self.conv66 = self.conv_layer(self.conv65, 1, 1, 512, 75, False, &apos;conv_head_66&apos;)    # 26x75
    # follow yolo layer mask = 3,4,5
    self.conv67 = self.conv_layer(self.conv64, 1, 1, 256, 128, True, &apos;conv_head_67&apos;)    # 26x128
    size = tf.shape(self.conv67)[1]
    self.upsample1 = tf.image.resize_nearest_neighbor(self.conv67, [2 * size, 2 * size],
                                                      name=&apos;upsample_1&apos;)                # 52x128
    self.route1 = tf.concat([self.upsample1, res10], axis=-1, name=&apos;route_1&apos;)           # 52x384
    self.conv68 = self.conv_layer(self.route1, 1, 1, 384, 128, True, &apos;conv_head_68&apos;)    # 52x128
    self.conv69 = self.conv_layer(self.conv68, 3, 1, 128, 256, True, &apos;conv_head_69&apos;)    # 52x256
    self.conv70 = self.conv_layer(self.conv69, 1, 1, 256, 128, True, &apos;conv_head_70&apos;)    # 52x128
    self.conv71 = self.conv_layer(self.conv70, 3, 1, 128, 256, True, &apos;conv_head_71&apos;)    # 52x256
    self.conv72 = self.conv_layer(self.conv71, 1, 1, 256, 128, True, &apos;conv_head_72&apos;)    # 52x128
    self.conv73 = self.conv_layer(self.conv72, 3, 1, 128, 256, True, &apos;conv_head_73&apos;)    # 52x256
    self.conv74 = self.conv_layer(self.conv73, 1, 1, 256, 75, False, &apos;conv_head_74&apos;)    # 52x75
    # follow yolo layer mask = 0,1,2

    return self.conv74, self.conv66, self.conv58
</code></pre><p>上面是最后的预测部分，需要输入的三个特征从Darknet-53网络中得到的，输出地方做了注释，Darknet-53网络结构如下：</p>
<pre><code>def build(self, img, istraining, decay_bn=0.99):
   self.phase_train = istraining
   self.decay_bn = decay_bn
   self.conv0 = self.conv_layer(bottom=img, size=3, stride=1, in_channels=3,   # 416x3
                                out_channels=32, name=&apos;conv_0&apos;)                # 416x32
   self.conv1 = self.conv_layer(bottom=self.conv0, size=3, stride=2, in_channels=32,
                                out_channels=64, name=&apos;conv_1&apos;)                # 208x64
   self.conv2 = self.conv_layer(bottom=self.conv1, size=1, stride=1, in_channels=64,
                                out_channels=32, name=&apos;conv_2&apos;)                # 208x32
   self.conv3 = self.conv_layer(bottom=self.conv2, size=3, stride=1, in_channels=32,
                                out_channels=64, name=&apos;conv_3&apos;)                # 208x64
   self.res0 = self.conv3 + self.conv1                                         # 208x64
   self.conv4 = self.conv_layer(bottom=self.res0, size=3, stride=2, in_channels=64,
                                out_channels=128, name=&apos;conv_4&apos;)               # 104x128
   self.conv5 = self.conv_layer(bottom=self.conv4, size=1, stride=1, in_channels=128,
                                out_channels=64, name=&apos;conv_5&apos;)                # 104x64
   self.conv6 = self.conv_layer(bottom=self.conv5, size=3, stride=1, in_channels=64,
                                out_channels=128, name=&apos;conv_6&apos;)               # 104x128
   self.res1 = self.conv6 + self.conv4     # 128                               # 104x128
   self.conv7 = self.conv_layer(bottom=self.res1, size=1, stride=1, in_channels=128,
                                out_channels=64, name=&apos;conv_7&apos;)                # 104x64
   self.conv8 = self.conv_layer(bottom=self.conv7, size=3, stride=1, in_channels=64,
                                out_channels=128, name=&apos;conv_8&apos;)               # 104x128
   self.res2 = self.conv8 + self.res1      # 128                               # 104x128
   self.conv9 = self.conv_layer(bottom=self.res2, size=3, stride=2, in_channels=128,
                                out_channels=256, name=&apos;conv_9&apos;)               # 52x256
   self.conv10 = self.conv_layer(bottom=self.conv9, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_10&apos;)             # 52x128
   self.conv11 = self.conv_layer(bottom=self.conv10, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_11&apos;)             # 52x256
   self.res3 = self.conv11 + self.conv9                                        # 52x256
   self.conv12 = self.conv_layer(bottom=self.res3, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_12&apos;)             # 52x128
   self.conv13 = self.conv_layer(bottom=self.conv12, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_13&apos;)             # 52x256
   self.res4 = self.conv13 + self.res3                                         # 52x256
   self.conv14 = self.conv_layer(bottom=self.res4, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_14&apos;)             # 52x128
   self.conv15 = self.conv_layer(bottom=self.conv14, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_15&apos;)             # 52x256
   self.res5 = self.conv15 + self.res4                                         # 52x256
   self.conv16 = self.conv_layer(bottom=self.res5, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_16&apos;)             # 52x128
   self.conv17 = self.conv_layer(bottom=self.conv16, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_17&apos;)             # 52x256
   self.res6 = self.conv17 + self.res5                                         # 52x256
   self.conv18 = self.conv_layer(bottom=self.res6, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_18&apos;)             # 52x128
   self.conv19 = self.conv_layer(bottom=self.conv18, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_19&apos;)             # 52x256
   self.res7 = self.conv19 + self.res6                                         # 52x256
   self.conv20 = self.conv_layer(bottom=self.res7, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_20&apos;)             # 52x128
   self.conv21 = self.conv_layer(bottom=self.conv20, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_21&apos;)             # 52x256
   self.res8 = self.conv21 + self.res7                                         # 52x256
   self.conv22 = self.conv_layer(bottom=self.res8, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_22&apos;)             # 52x128
   self.conv23 = self.conv_layer(bottom=self.conv22, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_23&apos;)             # 52x256
   self.res9 = self.conv23 + self.res8                                         # 52x256
   self.conv24 = self.conv_layer(bottom=self.res9, size=1, stride=1, in_channels=256,
                                 out_channels=128, name=&apos;conv_24&apos;)             # 52x128
   self.conv25 = self.conv_layer(bottom=self.conv24, size=3, stride=1, in_channels=128,
                                 out_channels=256, name=&apos;conv_25&apos;)             # 52x256
   self.res10 = self.conv25 + self.res9                                        # 52x256 一个输出的特征尺度
   self.conv26 = self.conv_layer(bottom=self.res10, size=3, stride=2, in_channels=256,
                                 out_channels=512, name=&apos;conv_26&apos;)             # 26x512
   self.conv27 = self.conv_layer(bottom=self.conv26, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_27&apos;)             # 26x256
   self.conv28 = self.conv_layer(bottom=self.conv27, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_28&apos;)             # 26x512
   self.res11 = self.conv28 + self.conv26                                      # 26x512
   self.conv29 = self.conv_layer(bottom=self.res11, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_29&apos;)             # 26x256
   self.conv30 = self.conv_layer(bottom=self.conv29, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_30&apos;)             # 26x512
   self.res12 = self.conv30 + self.res11                                       # 26x512
   self.conv31 = self.conv_layer(bottom=self.res12, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_31&apos;)             # 26x256
   self.conv32 = self.conv_layer(bottom=self.conv31, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_32&apos;)             # 26x512
   self.res13 = self.conv32 + self.res12                                       # 26x512
   self.conv33 = self.conv_layer(bottom=self.res13, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_33&apos;)             # 26x256
   self.conv34 = self.conv_layer(bottom=self.conv33, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_34&apos;)             # 26x512
   self.res14 = self.conv34 + self.res13                                       # 26x512
   self.conv35 = self.conv_layer(bottom=self.res14, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_35&apos;)             # 26x256
   self.conv36 = self.conv_layer(bottom=self.conv35, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_36&apos;)             # 26x512
   self.res15 = self.conv36 + self.res14                                       # 26x512
   self.conv37 = self.conv_layer(bottom=self.res15, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_37&apos;)             # 26x256
   self.conv38 = self.conv_layer(bottom=self.conv37, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_38&apos;)             # 26x512
   self.res16 = self.conv38 + self.res15                                       # 26x512
   self.conv39 = self.conv_layer(bottom=self.res16, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_39&apos;)             # 26x256
   self.conv40 = self.conv_layer(bottom=self.conv39, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_40&apos;)             # 26x512
   self.res17 = self.conv40 + self.res16                                       # 26x512
   self.conv41 = self.conv_layer(bottom=self.res17, size=1, stride=1, in_channels=512,
                                 out_channels=256, name=&apos;conv_41&apos;)             # 26x256
   self.conv42 = self.conv_layer(bottom=self.conv41, size=3, stride=1, in_channels=256,
                                 out_channels=512, name=&apos;conv_42&apos;)             # 26x512
   self.res18 = self.conv42 + self.res17                                       # 26x512，一个输出的特征尺度
   self.conv43 = self.conv_layer(bottom=self.res18, size=3, stride=2, in_channels=512,
                                 out_channels=1024, name=&apos;conv_43&apos;)            # 13x1024
   self.conv44 = self.conv_layer(bottom=self.conv43, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_44&apos;)             # 13x512
   self.conv45 = self.conv_layer(bottom=self.conv44, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_45&apos;)            # 13x1024
   self.res19 = self.conv45 + self.conv43                                      # 13x1024
   self.conv46 = self.conv_layer(bottom=self.res19, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_46&apos;)             # 13x512
   self.conv47 = self.conv_layer(bottom=self.conv44, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_47&apos;)            # 13x1024
   self.res20 = self.conv47 + self.res19                                       # 13x1024
   self.conv48 = self.conv_layer(bottom=self.res20, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_48&apos;)             # 13x512
   self.conv49 = self.conv_layer(bottom=self.conv48, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_49&apos;)            # 13x1024
   self.res21 = self.conv49 + self.res20                                       # 13x1024
   self.conv50 = self.conv_layer(bottom=self.res21, size=1, stride=1, in_channels=1024,
                                 out_channels=512, name=&apos;conv_50&apos;)             # 13x512
   self.conv51 = self.conv_layer(bottom=self.conv50, size=3, stride=1, in_channels=512,
                                 out_channels=1024, name=&apos;conv_51&apos;)            # 13x1024
   self.res23 = self.conv51 + self.res21                                       # 13x1024
   return self.res23  # 最后输出特征
</code></pre><p>同样采用k-means聚类的到anchor box的尺寸。选取了9种，3中不同的scale：(10×13); (16×30); (33×23); (30×61); (62×45); (59×119); (116 × 90); (156 × 198); (373 × 326).</p>
<h3 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h3><p>YOLO3的新的更深的网络，Darknet-53，实现细节可参见上面的代码</p>
<div align="center"><br>    <img src="https://i.imgur.com/ZxUzfO8.png"><br></div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO是一种全新的，与R-CNN思想截然不同的目标检测的方法。R-CNN系列网络是通过proposal region产生可能包含目标物体的bounding box，再通分类器判断是否包含物品以及物品类别，用regression对bounding的坐标、大小进行修正。YOLO则是一种end to end的方式，用一个神经网络，实现了预测出bounding box 的坐标、box中包含物体的置信度和物体的probabilities，因此检测速度更快，训练相对更加简单，当然相对来说也带来一些其他缺点。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>PCL的python库安装for Ubuntu16.04</title>
    <link href="http://yoursite.com/2018/05/11/PCL%E7%9A%84python%E5%BA%93%E5%AE%89%E8%A3%85for-Ubuntu16-04/"/>
    <id>http://yoursite.com/2018/05/11/PCL的python库安装for-Ubuntu16-04/</id>
    <published>2018-05-11T11:43:04.000Z</published>
    <updated>2018-06-19T03:13:38.575Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库<a href="https://github.com/strawlab/python-pcl" target="_blank" rel="external">python_pcl实现库</a> ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。</p>
<a id="more"></a>
<h2 id="PCL安装"><a href="#PCL安装" class="headerlink" title="PCL安装"></a>PCL安装</h2><p>不用编译源码，一行命令直接apt安装，顺带安装各种依赖的乱七八糟的库</p>
<pre><code>sudo apt-get install libpcl-dev 
</code></pre><p>再安装一些pcl可视化等软件包</p>
<pre><code>sudo apt-get install pcl_tools
</code></pre><h2 id="安装-python-pcl"><a href="#安装-python-pcl" class="headerlink" title="安装 python_pcl"></a>安装 python_pcl</h2><p>首先下载python_pcl源文件</p>
<pre><code>git clone https://github.com/strawlab/python-pcl.git
</code></pre><p>编译、安装</p>
<pre><code>python setup.py build_ext -i
python setup.py install
</code></pre><p>在此之前常出现的一个编译问题是cython版本问题，所以在执行上一步之前首先：</p>
<pre><code>pip install cython==0.25.2
</code></pre><h2 id="解决常出现的链接失败的问题"><a href="#解决常出现的链接失败的问题" class="headerlink" title="解决常出现的链接失败的问题"></a>解决常出现的链接失败的问题</h2><p>由于我的默认python为anaconda3的python，可能是anaconda3自带的链接库的问题，所以出现了如下错误：</p>
<pre><code>./lib/libgomp.so.1: version `GOMP_4.0&apos; not found (required by /home/lkj/anaconda3/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so) 
</code></pre><p>上面的意思是anaconda3/lib/libgomp.so.1中没有‘GOMP_4.0’，这个可以使用strings命令查看libgomp.so.1这个文件，显示并无4.0版本，因此寻找其他路径的链接库替代，用locate命令搜索系统中所有的libgomp.so.1，得到：<br><img src="https://i.imgur.com/tSz0fdU.png" alt=""><br>然后用strings查看这些文件信息，</p>
<pre><code>/usr/lib/x86_64-linux-gnu/libgomp.so.1 |grep GOMP
</code></pre><p>发现x86_64-linux-gnu/libgomp.so.1包含GOMP_4.0<br><img src="https://i.imgur.com/Hi1QcUV.png" alt=""><br>因此可以删掉原有的libgomp.so.1，重新做一个新的链接。</p>
<pre><code>ln -s /usr/lib/x86_64-linux-gnu/libgomp.so.1 libgomp.so.1 
</code></pre><p>然后再次在python里面import pcl,又提示libstdc++.so.6出现类似的问题，对上述做类似处理，如果还有链接库的问题，也可以用同样的方法处理,至此实现了python的pcl库安装。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCL（Point Cloud Library）是包含各种点云算法的大型跨平台开源C++编程库，是吸纳了大量点云相关算法，包括点云获取、滤波、分割、特征提取、曲面重建以及可视化等各种算法，然而现在我主要使用的是python语言，网上目前又有公布的python_pcl实现库&lt;a href=&quot;https://github.com/strawlab/python-pcl&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;python_pcl实现库&lt;/a&gt; ，然而针对Ubuntu16.04按照官方给的方法没有能够实现安装，踩了无数坑之后，博客记录一种简单且成功安装的方法。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>基于深度学习的目标检测技术学习笔记(R-CNN系列)</title>
    <link href="http://yoursite.com/2018/04/20/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(R-CNN%E7%B3%BB%E5%88%97)/"/>
    <id>http://yoursite.com/2018/04/20/基于深度学习的目标检测技术学习笔记(R-CNN系列)/</id>
    <published>2018-04-20T03:03:17.000Z</published>
    <updated>2018-07-06T01:43:59.027Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。<br><a id="more"></a></p>
<p>传统的目标检测算法一般是基于滑动窗口选中图中的某一部分作为候选区域，然后提取候选区域的特征，利用分类器（如常见的SVM)进行识别。2014年提出的region proposal+CNN代替传统目标检测使用的滑动窗口+特征工程的方法，设计了R-CNN算法，开启了基于深度学习的目标检测的大门。</p>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="https://i.imgur.com/2YwfkRz.png" alt=""></p>
<p>R-CNN算法流程为：</p>
<ol>
<li>输入图像，根据SS（selective search）算法提取2000个左右的region proposal（候选框）</li>
<li>将候选框crop/wrap为固定大小后输入CNN中，得到固定维度的输出特征</li>
<li>对提取的CNN特征，利用SVM分类器分类得到对应类别</li>
<li>边界回归（bouding-box regression），用线性回归模型修正候选框的位置</li>
</ol>
<p>R-CNN使得识别的精度和速度都有了提升，但是也存在很大问题，每次候选框都需要经过CNN操作，计算量很大，有很多重复计算；训练步骤繁多。</p>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>R-CNN需要每次将候选框resize到固定大小作为CNN输入，这样有很多重复计算。SPP-net的主要思想是去掉了原始图像上的crop/warp等操作，换成了在卷积特征上的空间金字塔池化层（Spatial Pyramid Pooling，SPP）。</p>
<p><img src="https://i.imgur.com/21xD4eJ.png" alt=""></p>
<p>SPP Net对整幅图像只进行一次CNN操作得到特征图，这样原图中的每一个候选框都对应于特征图上大小不同的某一区域，通过SPP可以将这些不同大小的区域映射为相同的维度，作为之后的输入，这样就能保证只进行一次CNN操作了。SPP包含一种可伸缩的池化层，输出固定尺寸特征。</p>
<p>基于SPP的思想，Fast R-CNN加入了一个ROI Pooling，将不同大小输入映射到一个固定大小的输出。R-CNN之前的操作是目标识别（classification）以及边界回归（bouding-box regression）分开进行。Fast R-CNN做的改进就是将这两个过程合并在一起，这两个任务共享CNN特征图，即成为了一个multi-task模型。</p>
<p><img src="https://i.imgur.com/31zXWV3.png" alt=""></p>
<p>多任务自然对应multi-loss，损失函数包括分类误差以及边框回归误差。<br>L<em>cls</em>为分类误差：</p>
<p><img src="https://i.imgur.com/MR7Wgbl.png" alt=""></p>
<p>分类误差只考虑对应的类别被正确分类到的概率，即P<em>l</em>为label对应的概率，当P<em>l</em>=1时，Loss为0，即正确分类的概率越大，loss越小。</p>
<p>L<em>reg</em>为边框回归误差：</p>
<p><img src="https://i.imgur.com/3zsjUKR.png" alt=""></p>
<p>对预测的边框四个位置描述参数与真实分类对应边框的四个参数偏差进行评估作为损失函数，g函数为smooth L1函数，这样对于噪声点不敏感，鲁棒性强，在|x|&gt;1时，变为线性，降低噪声影响。</p>
<p><img src="https://i.imgur.com/plKB3T1.png" alt=""></p>
<p><img src="https://i.imgur.com/UEijJWR.png" alt=""></p>
<p>这样加权得到的最终损失函数为：</p>
<p><img src="https://i.imgur.com/rcLlJWB.png" alt=""></p>
<p>foreground理解为前景，即对应有目标物体，这个时候需要考虑边框回归误差；background为背景，没有包含目标物品，所以不需考虑边框回归误差。</p>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>Faster R-CNN对Fast R-CNN又进行了改进，使得Faster。主要是将候选框的选取也引入到网络中，代替了之前SS选取候选框的方式，即引入了RPN（Region Proposal Network），将找候选框的工作也交给了神经网络了。</p>
<p><img src="https://i.imgur.com/Aso7UhH.png" alt=""></p>
<p>提到RPN网络，就不能不说anchors，即锚点，对应的是一组矩形框，在实际中有3种形状width:height = [1:1, 1:2, 2:1]，对应3种尺寸，所以共计9个矩形框。</p>
<p><img src="https://i.imgur.com/LyWLO9K.jpg" alt=""></p>
<p>这个矩形框对应的是原始输入图像里面的，并非是卷积特征图上的。即对卷积特征图上每一个点，可以对应原始图上的一个anchors，为其配备9个框作为原始检测框，当然一开始肯定是不准确的，可以在后续的bounding box regression修正检测框位置。</p>
<p><img src="https://i.imgur.com/9PmRpnN.png" alt=""></p>
<p>为了生成区域建议框，在最后一个共享的卷积层输出的卷积特征映射上滑动小网络，这个网络全连接到输入卷积特征映射的nxn的空间窗口上。每个滑动窗口映射到一个低维向量上（对于ZF最后卷积层的输出是256channel，即生成256张特征图，所以小网络滑窗在特征图上的点生成向量是256-d，对于VGG是512-d，每个特征映射的一个滑动窗口对应一个数值）。这个向量输出给两个同级的全连接的层——包围盒回归层（reg）和包围盒分类层（cls）。论文中n=3，由于小网络是滑动窗口的形式，所以全连接的层（nxn的）被所有空间位置共享（指所有位置用来计算内积的nxn的层参数相同）。这种结构实现为nxn的卷积层，后接两个同级的1x1的卷积层（分别对应reg和cls）。<br>在每一个滑动窗口的位置，我们同时预测k个区域建议，所以reg层有4k个输出，即k个box的坐标编码。cls层输出2k个得分，即对每个建议框是目标/非目标的估计概率（为简单起见，是用二类的softmax层实现的cls层，还可以用logistic回归来生成k个得分）。k个建议框被相应的k个称为anchor的box参数化。每个anchor以当前滑动窗口中心为中心，并对应一种尺度和长宽比，我们使用3种尺度和3种长宽比，这样在每一个滑动位置就有k=9个anchor。对于大小为WxH（典型值约2,400）的卷积特征映射，总共有WHk个anchor。</p>
<p>Faster R-CNN的损失函数为：</p>
<p><img src="https://i.imgur.com/ytSoqQU.png" alt=""></p>
<p>这里，i是一个mini-batch中anchor的索引，Pi是anchor i是目标的预测概率。如果anchor为正，Pi<em> 就是1，如果anchor为负，Pi</em> 就是0。ti是一个向量，表示预测的包围盒的4个参数化坐标，t<em>i</em>是与正anchor对应的GT（groundtruth）包围盒的坐标向量。Pi<em> L</em>reg<em>这一项意味着只有正anchor（Pi</em> =1）才有回归损失，其他情况就没有（Pi<em> =0）。cls层和reg层的输出分别由{pi}和{ti}组成，这两项分别由N</em>cls<em>和N</em>reg*以及一个平衡权重λ归一化。<br>边框回归损失函数，用采取类似fast R-CNN介绍的方法。具体地，学习的时候，对于四个参数进行如下处理：</p>
<p><img src="https://i.imgur.com/w449Eaq.png" alt=""></p>
<p>x，y，w，h指的是包围盒中心的（x,y）坐标、宽、高。变量x，xa，x* 分别指预测的包围盒、anchor的包围盒、GT的包围盒（对y，w，h也是一样）的x坐标，可以理解为从anchor包围盒到附近的GT包围盒的包围盒回归。</p>
<p>Fast R-CNN训练依赖于固定的目标建议框，而Faster R-CNN中的卷积层是共享的，所以RPN和Fast R-CNN都不能独立训练，论文中提出的是4步训练算法，通过交替优化来学习共享的特征。 </p>
<ol>
<li>训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调用于区域建议任务。</li>
<li>利用第一步的RPN生成的建议框，由Fast R-CNN训练一个单独的检测网络，这个检测网络同样是由ImageNet预训练的模型初始化的，这时候两个网络还没有共享卷积层。</li>
<li>用检测网络初始化RPN训练，但我们固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了。</li>
<li>保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像的目标检测（object detection）主要包括两个任务，一是要标注出目标物体的位置（localization），而是要识别出目标物体的类别（classification）。通俗来说，就是解决图像中多个目标在哪里，是什么的一个问题。这个问题的涉及，主要是目前参加了天池大赛的一个目标识别方面的问题，所以阅读了一些相关方面的文献，在此做一个学习总结，主要来介绍R-CNN（Regions with CNN features）系列的算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>NHD自动登陆签到脚本</title>
    <link href="http://yoursite.com/2018/03/05/NHD%E8%87%AA%E5%8A%A8%E7%99%BB%E9%99%86%E7%AD%BE%E5%88%B0%E8%84%9A%E6%9C%AC/"/>
    <id>http://yoursite.com/2018/03/05/NHD自动登陆签到脚本/</id>
    <published>2018-03-05T12:46:38.000Z</published>
    <updated>2018-03-09T00:58:43.060Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站<a href="http://www.nexushd.org/index.php" target="_blank" rel="external">NHD</a>，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。<br><a id="more"></a></p>
<p>首先还是老套路，先用浏览器实现一遍登陆以及签到过程，用谷歌浏览器的开发者模式获取这过程中的信息。可以看到首次登陆之后，response的header有set-cookie字段</p>
<p><img src="https://i.imgur.com/blGVz73.png" alt=""></p>
<p>而后这个服务器返回的set-cookie字段的值在下一次会话中又出现了</p>
<p><img src="https://i.imgur.com/gI5d49F.png" alt=""></p>
<p>其实总体思路还是很简单的，与之前的爬虫不同，这次需要用户登陆，所以是POST方法，并将用户名和密码以data参数代入，登陆之后服务器会返回cookie来标识用户。因为http是一种无状态协议，用户首次访问web站点的时候，服务器对用户一无所知。而Cookie就像是服务器给每个来访问的用户贴的标签，而这些标签就是对来访问的客户端的独有的身份的一个标识，这里就如同每个人的身份证一样，带着你的个人信息。而当一个客户端第一次连接过来的时候，服务端就会给他打一个标签，这里就如同给你发了一个身份证，所以之后的访问服务器再带上这个cookie就标识了该账户，具体流程网上找到一张很好的图可以解释。</p>
<p><img src="https://i.imgur.com/LRa6WPr.png" alt=""></p>
<p>这样的话，只需要第一次输入密码，后面浏览器再次访问只要带上这个服务器返回的cookie，服务器就可以知道是该账户在访问，所以python程序也模拟该过程。利用request库中的session对象来创建类似于图中的过程，session对象会保存访问过程中的cookie用于之后对服务器的继续访问。</p>
<pre><code>url = &apos;http://www.nexushd.org/takelogin.php&apos;  #登陆界面
a = session.post(url=url, cookies=cookies, headers=headers, data=data)
#这里的cookie是浏览器首次访问的使用的cookie，之后服务器
#设置的cookie会保存在session对象中
time.sleep(2)

url2 = &apos;http://www.nexushd.org/signin.php&apos;  #签到界面
b = session.get(url=url2, headers=headers)
time.sleep(2)

url3 = &apos;http://www.nexushd.org/signin.php?&apos;
qiandao = {&apos;action&apos;:&apos;post&apos;,&apos;content&apos;:&apos;lalala2333&apos;} #签到信息随便填，lalala2333
r = session.post(url=url3, headers=headers, data=qiandao)
</code></pre><p>而后就是一个判断是否登陆成功的程序，依然使用BeautifulSoup来解析，得到已签到之后退出循环，并将日志信息记录到日志文件。</p>
<pre><code>r = session.post(url=url3, headers=headers, data=qiandao)
r = BeautifulSoup(r.content,&apos;lxml&apos;)
message1 = r.find_all(&apos;a&apos;,{&apos;href&apos;:&quot;signin.php&quot;})[0].contents[0]
message2 = r.find_all(&apos;h2&apos;)[0].getText()
if message2 == &apos;签到成功&apos;:
    f = codecs.open(&apos;logging.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;)
    str = time.strftime(&apos;%Y-%m-%d  %H:%M:%S&apos;, time.localtime(time.time())) + &apos;-----签到成功&apos; + &apos;\n&apos;
    f.write(str)  # 记录日志信息到日志文件
    f.close()
    print(r.find_all(&apos;span&apos;, {&apos;class&apos;: &apos;medium&apos;})[0].getText())
    print(r.find_all(&apos;td&apos;, {&apos;class&apos;: &apos;text&apos;})[-1].getText().split(&apos;。&apos;)[0])
    break
elif message1 == &apos;已签到&apos;: #如果已经签到
    print(&apos;已经签到过了哦&apos;)
    break
if maxtry &lt; 30:
    print(&apos;签到失败，第&apos;+str(maxtry+1)+&apos;次重试&apos;)
    maxtry = maxtry+1
    time.sleep(5)
else:
    print(&quot;自动签到失败，请手动签到，或者检查网络连接&quot;)
    break
</code></pre><p>为了能够开机自动运行程序，将该程序添加至windows启动运行。代码中读取在配置文件中的账户信息，并且通过读取上一次签到成功时间来判断是否成功签到过。</p>
<pre><code>maxtry=0  #记录重试次数
f = codecs.open(&apos;profile.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;)  #读取配置文件，包含账户及密码
line=f.readline()
f.close()

username = line.split()[0] #你的用户名
password = line.split()[1]  #你的密码
data = {&apos;username&apos;: username, &apos;password&apos;:password}

flag = True
day_now = time.localtime(time.time()).tm_mday
f = codecs.open(&apos;logging.txt&apos;, &apos;r&apos;, encoding=&apos;utf-8&apos;)
lines = f.readlines()
f.close()
#######更换账户登陆时，最好清除以前账户的日志信息
try:  #如果第一次使用可能没有签到记录
    day_log = int(lines[-1].split()[0].split(&apos;-&apos;)[-1])
except:
    day_log=33

day_log = int(lines[-1].split()[0].split(&apos;-&apos;)[-1])
if day_now == day_log:
    print(username+&apos;今天签到过了哦&apos;)
    flag = False
</code></pre><p>将配置文件profile.txt和日志文件logging.txt以及代码qiandao.py放入windows的启动运行的文件夹，这个文件夹可以通过在cmd窗口下输入</p>
<pre><code>shell:Startup
</code></pre><p>打开</p>
<p>在这个启动文件夹下写一个bat脚本来运行python代码</p>
<pre><code>D:
cd D:\simulation file\pyCharm\python3\qiandao
python qiandao.py
pause
</code></pre><p>至此就完整实现了电脑开机自动登陆签到NHD啦。效果如下<br>测试结果：<br><img src="https://i.imgur.com/UgfuNIl.png" alt=""></p>
<p>完整代码详见个人<a href="https://github.com/lkj1114889770/WebScraping/tree/master/qiandao" target="_blank" rel="external">github</a>了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到现在做过很多爬虫的小玩意，今天用爬虫来做一个更实际的东西。学校内部有个电影资源分享网站&lt;a href=&quot;http://www.nexushd.org/index.php&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;NHD&lt;/a&gt;，虽然说是免费下载，但是需要消耗魔力值，魔力值可以通过每天签到获得，连续签到的话每天得到的魔力值数量是可观的，但是有时候会忘了签到，因此写了一个小程序来实现每天自动登陆NHD并实现自动签到。&lt;br&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>我从崖边跌落</title>
    <link href="http://yoursite.com/2018/02/12/%E6%88%91%E4%BB%8E%E5%B4%96%E8%BE%B9%E8%B7%8C%E8%90%BD/"/>
    <id>http://yoursite.com/2018/02/12/我从崖边跌落/</id>
    <published>2018-02-12T10:58:56.000Z</published>
    <updated>2018-02-12T11:09:11.983Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我从崖边跌落<br>落入星空辽阔<br>银河不清不浊<br>不知何以摆脱</p>
<p><a href="http://music.163.com/#/song?id=415086030&amp;market=baiduqk" target="_blank" rel="external">谢春花《我从崖边跌落》</a></p>
<p><img src="http://img.mp.itc.cn/upload/20170408/f70dc16bc19b47e68670c262e8b8ccc9_th.jpeg" alt=""></p>
<a id="more"></a>
<p><img src="https://i.imgur.com/jMS0zwA.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我从崖边跌落&lt;br&gt;落入星空辽阔&lt;br&gt;银河不清不浊&lt;br&gt;不知何以摆脱&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://music.163.com/#/song?id=415086030&amp;amp;market=baiduqk&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;谢春花《我从崖边跌落》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://img.mp.itc.cn/upload/20170408/f70dc16bc19b47e68670c262e8b8ccc9_th.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>电影推荐系统构建</title>
    <link href="http://yoursite.com/2018/01/11/%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/01/11/电影推荐系统构建/</id>
    <published>2018-01-11T06:12:27.000Z</published>
    <updated>2018-01-11T07:10:43.912Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>很久没有更新博客了，最近刚做完数据挖掘的大作业，选了一个电影数据集挖掘的课题，做了一个推荐系统，在这里简单地记录一下。<br>电影推荐系统数据集来源于<a href="https://www.kaggle.com/rounakbanik/the-movies-dataset" target="_blank" rel="external">kaggle</a>上的MovieLens完整的45,000条数据，电影数据包括2017年7月前发布的电影，包括270,000个用户的超过26,000,000条评论，以及从GroupLens官方网站获得的评分。基于此电影数据集，完成下面的数据挖掘目标。<br><a id="more"></a><br>•    电影数据集处理及可视化分析<br>•    基于用户投票的推荐算法<br>•    基于内容的推荐算法<br>•    基于协同过滤的推荐算法<br>•    数据库技术的应用<br>•    简单的电影推荐网站构建<br>当然这次代码也有很大程度上参考了这个数据集下的大佬分享的kernel，提供了许多不错的精致代码。</p>
<h2 id="数据集介绍及分析"><a href="#数据集介绍及分析" class="headerlink" title="数据集介绍及分析"></a>数据集介绍及分析</h2><p>movies_metadata.csv: 电影基本信息描述文件，包括 45000部电影的演员、工作人员、情节关键字、预算、收入、海报、发布日期、语言、制作公司、国家、TMDB投票计数和平均投票信息.<br>keywords.csv: 包含电影的关键词信息，每条数据为json格式.。<br>credits.csv: 演员和电影工作人员的信息，每条数据为json格式。<br>links.csv: 包含所有电影TMDB IDs和IMDB IDs 对应信息。<br>links_small.csv: 9,000部电影的TMDB IDs和IMDB IDs 对应信息.<br>rating.csv:用户对于所有电影的打分，1-5。<br>ratings_small.csv: 电影打分子集，700个用户对于9,000部电影的100,000个评分。<br>针对电影的情况，首先我们看一下电影的平均投票分布，如下图所示，由图中可以看出，电影集中分布在6分左右，也是比较符合实际情况，一般的电影居多，高分电影以及烂片数量相对较少。</p>
<pre><code>%matplotlib inline
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(&apos;ignore&apos;)
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv(&apos;movies_metadata.csv&apos;)
df[&apos;vote_average&apos;] = df[&apos;vote_average&apos;].replace(0, np.nan)
sns.distplot(df[&apos;vote_average&apos;].fillna(df[&apos;vote_average&apos;].median()))
</code></pre><p><img src="https://i.imgur.com/jBazUei.png" alt=""></p>
<pre><code>df[&apos;year&apos;] = pd.to_datetime(df[&apos;release_date&apos;], errors=&apos;coerce&apos;).apply(lambda x: str(x).split(&apos;-&apos;)[0] if x != np.nan else np.nan)
year_gen = pd.DataFrame(df[&apos;year&apos;].value_counts()).reset_index()
year_gen.columns = [&apos;year&apos;, &apos;counts&apos;]
year_gen.drop([87,135,],inplace=True)
year_gen[&apos;year&apos;]=year_gen[&apos;year&apos;].astype(&apos;int&apos;)
plt.plot(year_gen.year,year_gen.counts)
</code></pre><p><img src="https://i.imgur.com/SdNwqOI.png" alt=""></p>
<p>从上图中的电影分布可以看出，从1880年左右以来，电影的数量基本上是逐年增长的趋势，特别是进入21实际以来，增长速度很快（出现一段下降是因为2017年的完整数据收集不完整）。<br>下面再分析数据集中的电影的区域分布，利用一个比较强的可视化工具plotly，画出电影数量的区域分布，因为美国的电影产出相对其他国家高出太多，所以画图是先忽略了美国，这样画其他国家的数量之间的比较才会更加明显。</p>
<pre><code>data = [ dict(
        type = &apos;choropleth&apos;,
        locations = con_df[&apos;country&apos;],
        locationmode = &apos;country names&apos;,
        z = con_df[&apos;num_movies&apos;],
        text = con_df[&apos;country&apos;],
        colorscale = [[0,&apos;rgb(255, 255, 255)&apos;],[1,&apos;rgb(255, 0, 0)&apos;]],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = &apos;rgb(180,180,180)&apos;,
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            tickprefix = &apos;&apos;,
            title = &apos;数量图例&apos;),
      ) ]

layout = dict(
    title = &apos;电影数据集中电影数量分布（除美国外）&apos;,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = &apos;Mercator&apos;
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename=&apos;d3-world-map&apos; )
plt.figure(figsize=(12,5))
sns.barplot(x=&apos;country&apos;, y=&apos;num_movies&apos;, data=country)
plt.show()
####除去美国外，英国。法国、德国、意大利，亚洲的日本和印度，北美的巴西
</code></pre><p><img src="https://i.imgur.com/oroLVyE.png" alt=""></p>
<p><img src="https://i.imgur.com/ZuPNuyb.png" alt=""></p>
<h2 id="推荐系统构建"><a href="#推荐系统构建" class="headerlink" title="推荐系统构建"></a>推荐系统构建</h2><p>在这次推荐系统的构建中，我们采用了三种算法来构建我们的推荐系统，基于这三种算法，包括基于用户投票的推荐算法、基于内容的推荐算法和协同过滤推荐算法，根据这些算法，最终来构建我们的电影推荐系统。</p>
<h3 id="基于用户投票的推荐算法"><a href="#基于用户投票的推荐算法" class="headerlink" title="基于用户投票的推荐算法"></a>基于用户投票的推荐算法</h3><p>作为国际知名的权威点评网站，在他们大名鼎鼎的TOP250榜单中，采用的就是贝叶斯算法，其公式如下：</p>
<p><img src="https://i.imgur.com/YasOosa.png" alt=""></p>
<p>其中，WR为加权得分，R为该电影的用户投票平均得分，V为该电影的投票人数，m为最低评分个数，C为所有电影的平均得分。<br>这个算法的提出基于这样一个现实问题：热门电影与冷门电影的平均得分，是否真的可比？举例来说，一部好莱坞大片有10000个观众投票，一部小成本的文艺片只有100个观众投票。这两者的投票结果，怎么比较？如果使用”威尔逊区间”，后者的得分将被大幅拉低，这样处理是否公平，能不能反映它们真正的质量？一个合理的思路是，如果要比较两部电影的好坏，至少应该请同样多的观众观看和评分。既然文艺片的观众人数偏少，那么应该设法为它增加一些观众。</p>
<p>根据这个思路，这个算法相当于给每部电影增加了m个选票，并且每个评分为平均得分C，然后用现有观众的投票进行修正，即v*R/(v+m)部分，使得得分更加接近于真实情况。这种算法由于给每部电影增加了m个选票，拉近了不同电影之间投票人数的差异，使得投票人数较少的电影也有可能名列前茅。</p>
<p>这个算法借鉴了“贝叶斯推断”的思想，既然不知道投票结果，那就预先估计一个值，然后不断用新的信息修正，使它接近于正确值。在式子中，m可以看作是先验概率，每一次新的投票都是一个调整因子，使总体平均分不断向该项目的真实投票结果靠近。投票人数越多，该项目的”贝叶斯平均”就越接近算术平均，对排名的影响就越小。因此这种方法可以让投票较少的项目，能够得到相对公平的排名。</p>
<p>我们针对所有电影，类似于IMDB我们计算出了TOP250，下图为基于贝叶斯统计的用户投票排名算法得出的所有电影的TOP250中选取出的TOP10，其中电影名红色的为实际IMDB中进入TOP10的电影，可以看出有3部电影存在于IMDB的TOP10，绿色标注的电影为TOP11-15的电影，有3部。总的来说，贝叶斯统计得出的排名还是比较接近于IMDB的排名，但是由于我们的算法考虑的因素较少，所以还是有一定的区别。</p>
<p><img src="https://i.imgur.com/Nd6D8YN.jpg" alt=""></p>
<p>进一步的，我们从电影数据集中，根据电影的genre属性值中，分离出电影所属的不同属性，所有电影的类型分布（TOP10）如下图所示</p>
<p><img src="https://i.imgur.com/UMbWZN9.png" alt=""></p>
<p>可以看出，电影数据集中戏剧、喜剧、恐怖片、爱情片等数量较多，依次数量排名.针对数量超过3000的电影，我们也采取类似的方式计算了TOP250，部分电影类型的TOP10在下图中给出。</p>
<p><img src="https://i.imgur.com/upDJLWF.png" alt=""></p>
<h3 id="基于内容的推荐算法"><a href="#基于内容的推荐算法" class="headerlink" title="基于内容的推荐算法"></a>基于内容的推荐算法</h3><p>基于投票排名的推荐算法给每个用户都是一样推荐按照TOP排名得出的电影，而不会根据特定的观众喜欢的电影去推荐相似的电影。为了能够给用户推荐相似的电影，我们首先需要对电影之间的相似性进行衡量，主要应用到电影的描述数据来完成基于内容的推荐，主要的实现过程包括：<br>•    对电影的关键词、描述信息、标语、主角、导演信息的提取<br>•    对上述信息进行词干提取<br>•    对上述信息进行特征抽取，转换成词向量<br>•    考虑评分情况，结合相似度完成推荐<br>首先我们对于电影的相关描述信息进行一个大致分析，制作了词云对所有电影的情况概览。</p>
<pre><code>df[&apos;title&apos;] = df[&apos;title&apos;].astype(&apos;str&apos;)
df[&apos;overview&apos;] = df[&apos;overview&apos;].astype(&apos;str&apos;)
title_corpus = &apos; &apos;.join(df[&apos;title&apos;])
overview_corpus = &apos; &apos;.join(df[&apos;overview&apos;])
from wordcloud import WordCloud, STOPWORDS
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color=&apos;white&apos;, height=2000, width=4000).generate(title_corpus)
plt.figure(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p><img src="https://i.imgur.com/Fckk4jq.png" alt=""></p>
<pre><code>overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color=&apos;white&apos;, height=2000, width=4000).generate(overview_corpus)
plt.figure(figsize=(16,8))
plt.imshow(overview_wordcloud)
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><p><img src="https://i.imgur.com/5mNPVdG.png" alt=""></p>
<p>上面两幅图分别是电影的标题和电影简述画出的词云，可以看到电影标题中Love、Girl、Man、Life，Love作为最高频的词，毕竟大多数电影都有爱情这条线。在电影的简述中，find、life、one是最高频的词，可以给我们反映大多数电影的主题。</p>
<p>在获得电影的关键词、描述信息、标语、主角、导演信息之后，我们需要对这些信息进行词干提取。在语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，即得到单词最一般的写法。计算机科学领域有很多词干提取的相应算法，我们使用了一个面向英语的词干提取器stemming，使用Python的NLTK库的stemming算法，实现的效果为要识别字符串“cats”、“catlike”和“catty”提取出词根“cat”；“stemmer”、“stemming”和“stemmed”提取出词根“stem”。</p>
<pre><code>from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(&apos;english&apos;)  #英语的词干提取
</code></pre><p>下一步需要将提取词干后的文档进行向量化处理，采用的是sklearn中的Countvectorizer。根据语料库中的词频排序从高到低进行选择，词汇表的最大含量由vocabsize超参数来指定，超参数minDF，则指定词汇表中的词语至少要在多少个不同文档中出现次数，产生文档关于词语的稀疏表示，在fitting过程中，countvectorizer将根据语料库中的词频排序选出前vocabsize个词，输出词向量。</p>
<pre><code>count = CountVectorizer(analyzer=&apos;word&apos;,ngram_range=(1, 2),min_df=0, stop_words=&apos;english&apos;)
count_matrix = count.fit_transform(smd[&apos;soup&apos;])   #基于词向量统计的矩阵
</code></pre><p>当然，基于内容的推荐算法还需要考虑到电影的评分，不然仅仅根据电影之间的相似度，很有可能就会出现给观众推荐很相似的电影，但却是“烂片”的这种情况，基于这种考虑，以电影《The Godfather》（教父）以及《The Lord of the Rings: The Return of the King》（指环王：王者归来）为例，推荐的相应10部电影结果如下图所示。</p>
<p><img src="https://i.imgur.com/JemBXZC.png" alt=""></p>
<p>可以看到，都推荐了同类型的电影，比如针对教父推荐了一些剧情、犯罪电影，而针对指环王推荐了一些动作、奇幻类的电影，而且这两部电影都有其他续集，比如针对《教父1》推荐了其续集《教父2》及《教父3》也都相应地推荐了，推荐的电影也都是高分电影.</p>
<h3 id="协同过滤推荐"><a href="#协同过滤推荐" class="headerlink" title="协同过滤推荐"></a>协同过滤推荐</h3><p>从应用的场景来看，基于内容的推荐算法更多地适用于用户根据关键字或者电影名字来搜索相应的电影，然后推荐系统来进行相应的推荐。基于需求个性角度来看，基于内容的推荐算法还不够个人化，用户需要的是更加符合个人偏好的推荐结果，可以根据用户之前的打分情况，更有针对性地推荐一些可能喜欢的电影，这种情况下，应用的最多的就是协同过滤算法。</p>
<p>协同过滤通过用户和产品及用户的偏好信息产生推荐策略，最基本的策略有两种：一种是找到具有类似品味的人所喜欢的物品；另一种是从一个人喜欢的物品中找出类似的物品，即基于用户的推荐技术（User CF）和基于物品的推荐技术（Item CF）。在我们这个应用场景中，有大量的电影信息，但是用户已经打分的电影只占总量很少的一部分，将用户打分和电影信息构成一个矩阵，那么这个矩阵会存在严重的稀疏性，经过计算大约在1.5%左右，基于这种考虑，我们采取Item-based协同过滤算法。同样由于矩阵的稀疏性，在数据量很大的情况下一般采用矩阵分解来减少运算量，采用PMF矩阵分解算法来完成这个目标。</p>
<p>采用的是surprise库中的SVD算法，但是我看了surprise库中的SVD算法介绍，其实更准确地说是PMF（Probabilistic Matrix Factorization）算法，即概率矩阵分解算法，所以这里对PMF进行相应的介绍。</p>
<p>假定每个用户u都有一个D维的向量，表示他对不同风格的电影的偏好，每个电影i也有一个D维的向量表示不同风格的用户对它的偏好。 于是电影的评分矩阵可以这样来估计：</p>
<p><img src="https://i.imgur.com/Zj6DD9w.png" alt=""></p>
<p>p 和q就是D维的向量。用梯度下降法训练p和q，迭代几十次就收敛了。但是这样的SVD很容易就过拟合，所以需要加入正则化项：</p>
<p><img src="https://i.imgur.com/gc43bcW.png" alt=""></p>
<p>这样每次迭代的时候，更新公式为：</p>
<p><img src="https://i.imgur.com/MRt6hwJ.png" alt=""></p>
<p>采用5折交叉验证</p>
<pre><code>import pandas as pd
import numpy as np
from surprise import Reader, Dataset, SVD, evaluate
from collections import defaultdict
import warnings; warnings.simplefilter(&apos;ignore&apos;)


reader = Reader()
ratings = pd.read_csv(&apos;ratings_small.csv&apos;)

#从DataFrame导入数据
data = Dataset.load_from_df(ratings[[&apos;userId&apos;, &apos;movieId&apos;, &apos;rating&apos;]], reader)
data.split(n_folds=5)
trainset = data.build_full_trainset()
#SVD算法
algo = SVD()
evaluate(algo, data, measures=[&apos;RMSE&apos;, &apos;MAE&apos;])

#训练模型
algo.train(trainset)
#对用户未评价的电影生成测试集
testset = trainset.build_anti_testset()
predictions = algo.test(testset)  #预测测试集结果


def get_top_n(predictions, n=10):
    &apos;&apos;&apos;对预测结果中的每个用户，返回n部电影，默认n=10
    返回值一个字典，包括：
    keys 为原始的userId，以及对应的values为一个元组
        [(raw item id, rating estimation), ...].
    &apos;&apos;&apos;

    # 预测结果取出，对应每个userId.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
    # 排序取出前n个
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]
    return top_n

top_n = get_top_n(predictions, n=10)
rec_result=np.zeros((671,11))  #定义二维矩阵来存放结果
i=0
for uid, user_ratings in top_n.items():
    rec_result[i,0]=uid
    rec_result[i,1:]=[iid for (iid, _) in user_ratings]
    i=i+1
rec_result=rec_result.astype(&apos;int&apos;)

#转变成DataFrame
rec_result=pd.DataFrame(rec_result,columns=[&apos;userId&apos;,&apos;rec1&apos;,&apos;rec2&apos;,&apos;rec3&apos;,&apos;rec4&apos;,&apos;rec5&apos;,
                                          &apos;rec6&apos;,&apos;rec7&apos;,&apos;rec8&apos;,&apos;rec9&apos;,&apos;rec10&apos;])
</code></pre><p>算法运行结果：</p>
<p><img src="https://i.imgur.com/wgzL9uA.png" alt=""></p>
<h2 id="简单的电影点评网站构建"><a href="#简单的电影点评网站构建" class="headerlink" title="简单的电影点评网站构建"></a>简单的电影点评网站构建</h2><p>整体框架</p>
<p><img src="https://i.imgur.com/bFDYLcO.png" alt=""></p>
<p>MySQL是一个关系型数据库管理系统，也是一种WEB应用最好的数据库。数据库作为中间件，搭建在寝室的主机，便于小组成员之间使用，其操纵代码：</p>
<pre><code>import pymysql
import pandas as pd
#连接数据库
conn = pymysql.connect(host=&apos;10.110.43.140&apos;,port= 3306,user = &apos;###&apos;,passwd=&apos;####&apos;,db=&apos;sys&apos;) #db：库名，用户名和密码这里我打了马赛克了，嘻嘻
#创建游标
cur = conn.cursor()
df=pd.read_sql(&apos;SELECT * FROM db_movies.tb_movies;&apos;,conn)
cur.close()
conn.close()
</code></pre><p>小伙伴应用Django框架，是一个开放源代码的Web应用框架，由Python写，时间仓促的情况下赶出了一个还是很不错的页面，点赞。</p>
<p><img src="https://i.imgur.com/qF14GhU.png" alt=""></p>
<p>详细代码可见个人<a href="https://github.com/lkj1114889770/File_Recommend" target="_blank" rel="external">github</a>。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久没有更新博客了，最近刚做完数据挖掘的大作业，选了一个电影数据集挖掘的课题，做了一个推荐系统，在这里简单地记录一下。&lt;br&gt;电影推荐系统数据集来源于&lt;a href=&quot;https://www.kaggle.com/rounakbanik/the-movies-dataset&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;kaggle&lt;/a&gt;上的MovieLens完整的45,000条数据，电影数据包括2017年7月前发布的电影，包括270,000个用户的超过26,000,000条评论，以及从GroupLens官方网站获得的评分。基于此电影数据集，完成下面的数据挖掘目标。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="电影" scheme="http://yoursite.com/tags/%E7%94%B5%E5%BD%B1/"/>
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>网格搜索与Pipeline</title>
    <link href="http://yoursite.com/2017/11/23/%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E4%B8%8E%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/"/>
    <id>http://yoursite.com/2017/11/23/网格搜索与交叉验证/</id>
    <published>2017-11-23T12:03:19.000Z</published>
    <updated>2017-11-23T13:02:31.773Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>学习机器学习有段时间了，第一参加了个比赛，京东JDD数据探索大赛，做了个<a href="http://jddjr.jd.com/item/1" target="_blank" rel="external">登陆识行为识别</a>。不得不说，在实际业务场景中用学的机器学习算法来解决问题，比想象中的难度还是大很多，毕竟实际问题其实比平时简单的算法应用复杂得多。虽然目前数据准确率还不是很高，这个数据集的正例和负例比例相差太大，而且数据上对于特征工程处理也有很大的难度。Anyway，毕竟是第一次参加这样的比赛，收获还是很大的，学到了很多新东西。比如网格搜索和Pipeline机制，以及一个神器Xgboost。先将网格搜索和交叉验证mark一下吧。<br><a id="more"></a></p>
<h2 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h2><p>实际机器学习应用场景中的一个利器，通俗点就是暴力搜索。机器学习在应用的的时候，调参是一个很重要的环节，而网格搜索就在于优化参数搜索选择，更直白地说，就是你选择可能的参数集给你的分类器，然后网格搜索把这些可能的参数情况都运行一遍，按照你设定的score计算方式，返回score最高的参数。</p>
<p>函数原型：</p>
<pre><code>GridSearchCV(estimator, param_grid, scoring=None,
   fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, 
   verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’)
</code></pre><p>常用参数<br><strong>estimator</strong>：所使用的分类器，如estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features=’sqrt’,random_state=10), 并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个scoring参数，或者score方法。<br><strong>param_grid</strong>：值为字典或者列表，即需要最优化的参数的取值，param_grid =param_test1，param_test1 = {‘n_estimators’:range(10,71,10)}。我用的Xgboost算法，优化的参数集为：</p>
<pre><code>parameters = {
    &apos;max_depth&apos;:[4,6],
    &apos;learning_rate&apos;:[0.1,0.3],
    &apos;subsample&apos;:[0.8,1.0],
    &apos;gamma&apos;:[0,3,5]
}
</code></pre><p><strong>scoring</strong> :准确度评价标准，默认None,这时需要使用score函数；或者如scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring官方给的参数选择为<a href="http://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank" rel="external">http://scikit-learn.org/stable/modules/model_evaluation.html</a>，当然也可以自定义，我在比赛中就按照JDD给的评分要求自定义了：</p>
<pre><code>from sklearn.metrics import fbeta_score,make_scorer
#评估函数
JdScore = make_scorer(fbeta_score,beta=0.1,greater_is_better=True)
</code></pre><p><strong>cv</strong> :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，也可以是yield训练/测试数据的生成器。<br><strong>verbose</strong>：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出。<br><strong>n_jobs</strong>: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。</p>
<p>一个完整的网格搜索：</p>
<pre><code>from xgboost.sklearn import XGBClassifier
#xgb的配置
xgbFier = XGBClassifier(
learning_rate =0.1,
n_estimators=1000,
max_depth=5,
min_child_weight=1,
gamma=0,
subsample=0.8,
colsample_bytree=0.8,
objective= &apos;binary:logistic&apos;,
scale_pos_weight=1,
seed=27,
silent=0
)

#网格搜索实验
from sklearn.model_selection import GridSearchCV
parameters = {
    &apos;max_depth&apos;:[4,6],
    &apos;learning_rate&apos;:[0.1,0.3],
    &apos;subsample&apos;:[0.8,1.0],
    &apos;gamma&apos;:[0,3,5]
}
gSearch  =GridSearchCV(xgbFier,parameters,n_jobs=-1,scoring=JdScore,cv=5)

import time
start =time.time()
gSearch.fit(X_train,Y_train)
runtime=time.time()-start
print(&apos;run time:&apos;,runtime)
print(gSearch.best_params_,gSearch.best_score_)
</code></pre><p>输出结果为：</p>
<pre><code>run time: 4109.730866909027
{&apos;gamma&apos;: 3, &apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 6, &apos;subsample&apos;: 0.8} 0.738112481672
</code></pre><p>这样就找出了较优的参数，唉，现在只能得到0.78的线下score，还要继续修改啊。</p>
<h2 id="Pipeline机制"><a href="#Pipeline机制" class="headerlink" title="Pipeline机制"></a>Pipeline机制</h2><p>顾名思义就是管道机制，就是将机器学习整个流程流式化封装和管理，因为参数集在很多情况下对于测试集和训练集都是一样处理，他们有很多共同的步骤，这个机制就是便于这些步骤的共同使用。网上找到的一个很好解释的图如下，模型训练和预测过程中数据标准化、PCA降维之类的处理都可以通用，而且训练和预测用的是同一算法。</p>
<center> <br><br><img width="500" height="400" src="http://img.blog.csdn.net/20160115095855517"> <br><br></center>

]]></content>
    
    <summary type="html">
    
      &lt;p&gt;学习机器学习有段时间了，第一参加了个比赛，京东JDD数据探索大赛，做了个&lt;a href=&quot;http://jddjr.jd.com/item/1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;登陆识行为识别&lt;/a&gt;。不得不说，在实际业务场景中用学的机器学习算法来解决问题，比想象中的难度还是大很多，毕竟实际问题其实比平时简单的算法应用复杂得多。虽然目前数据准确率还不是很高，这个数据集的正例和负例比例相差太大，而且数据上对于特征工程处理也有很大的难度。Anyway，毕竟是第一次参加这样的比赛，收获还是很大的，学到了很多新东西。比如网格搜索和Pipeline机制，以及一个神器Xgboost。先将网格搜索和交叉验证mark一下吧。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>EVD、SVD以及PCA整理</title>
    <link href="http://yoursite.com/2017/11/01/EVD%E3%80%81SVD%E4%BB%A5%E5%8F%8APCA/"/>
    <id>http://yoursite.com/2017/11/01/EVD、SVD以及PCA/</id>
    <published>2017-11-01T06:40:14.000Z</published>
    <updated>2017-11-01T12:34:49.152Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近的机器学习算法学习到了主成分分析（PCA），在人脸识别中对样本数据进行了降维，借此对特征值分解（EVD）、奇异值分解（SVD）进行了梳理整理。</p>
<h2 id="特征值分解（EVD"><a href="#特征值分解（EVD" class="headerlink" title="特征值分解（EVD)"></a>特征值分解（EVD)</h2><p>矩阵是一种线性变换，比如矩阵Ax=y中，矩阵A将向量x线性变换到另一个矩阵y，这个过程中包含3类效应：旋转、缩放以及投影。<br><a id="more"></a></p>
<p>对角矩阵对应缩放，比如<img src="https://i.imgur.com/s5t13lw.png" alt=""></p>
<p>其对应的线性变换如下：</p>
<p><img alt="" src="http://img.blog.csdn.net/20140118132422328"></p>
<p>对与正交矩阵来说，对应的是向量的旋转，比如将向量OA从正交基e1e2中,到另一组正交基为e1’e2’中，</p>
<p><img src="https://i.imgur.com/GMLZwPD.png" alt=""><br><img alt="" src="http://img.blog.csdn.net/20150123124108372?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
<p>当矩阵A与x维度不一样时，得到的y的维度也与x不一样，即存在投影变换。</p>
<p>考虑一种特殊矩阵，对称阵的特征值分解，其实在机器学习中也经常是对XX’求特征向量，也就是对称阵。</p>
<p><img src="https://i.imgur.com/LLiUV34.png" alt=""></p>
<p>其中：</p>
<p><img src="https://i.imgur.com/2SkuNLs.png" alt=""></p>
<p><img src="https://i.imgur.com/5abYT6K.png" alt=""></p>
<p>这个时候用到对称阵的特性，U为正交矩阵，其逆矩阵等于转置。</p>
<p><img src="https://i.imgur.com/Jstukbo.png" alt=""></p>
<p>即矩阵A将向量X转移到了U这组基的空间上，再进行缩放，而后又通过U正交基进行旋转，所以只有缩放，没有旋转和投影。</p>
<h2 id="奇异值分解（SVD"><a href="#奇异值分解（SVD" class="headerlink" title="奇异值分解（SVD)"></a>奇异值分解（SVD)</h2><p>奇异值分解其实类似于特征值分解，不过奇异值分解适用于更一般的矩阵，而不是方阵。</p>
<div align="center"><br>    <img src="https://i.imgur.com/Gp5sZub.png" width="250" height="50"><br></div>

<p>U、V都是一组正交基，表示一个向量从V这组正交基旋转到U这组正交基，同时也在每个方向进行缩放。</p>
<p>奇异值和特征值对应为：</p>
<p><img src="https://i.imgur.com/FbOWs0v.png" alt=""></p>
<p>v即为式子中的右奇异向量，同时也可以得到：</p>
<p><img src="https://i.imgur.com/TUH2piG.png" alt=""></p>
<p>在奇异值按从小到大排序的情况下，很多情况下，前面部分的奇异值就占所有奇异值和的99%以上，所以我们可以取前r个奇异值来近似描述矩阵，可以用来数据降维。</p>
<p><div align="center"><br>    <img src="https://i.imgur.com/kckZiod.png" width="250" height="50"><br></div><br>这样可以还原出A矩阵，减少数据存储。<br>下面看如何利用SVD降维：</p>
<p><div align="center"><br>    <img src="https://i.imgur.com/fKT2IuH.png" width="300" height="200"><br></div><br>从而将A’从n <em> m降到n </em> r</p>
<p>SVD常用于推荐系统，有基于用户的协同过滤（User CF)和基于物品的协同过滤（Item CF)。这里给出一个Item CF实现。</p>
<p>网上找的一个实现代码，找不到出处了。。。</p>
<pre><code>#coding=utf-8
from numpy import *
from numpy import linalg as la

&apos;&apos;&apos;加载测试数据集&apos;&apos;&apos;
def loadExData():
    return mat([[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],
           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],
           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],
           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],
           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],
           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],
           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],
           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],
           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],
           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],
           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]])

&apos;&apos;&apos;以下是三种计算相似度的算法，分别是欧式距离、皮尔逊相关系数和余弦相似度,
注意三种计算方式的参数inA和inB都是列向量&apos;&apos;&apos;
def ecludSim(inA,inB):
    return 1.0/(1.0+la.norm(inA-inB))  #范数的计算方法linalg.norm()，这里的1/(1+距离)表示将相似度的范围放在0与1之间

def pearsSim(inA,inB):
    if len(inA)&lt;3: return 1.0
    return 0.5+0.5*corrcoef(inA,inB,rowvar=0)[0][1]  #皮尔逊相关系数的计算方法corrcoef()，参数rowvar=0表示对列求相似度，这里的0.5+0.5*corrcoef()是为了将范围归一化放到0和1之间

def cosSim(inA,inB):
    num=float(inA.T*inB)
    denom=la.norm(inA)*la.norm(inB)
    return 0.5+0.5*(num/denom) #将相似度归一到0与1之间

&apos;&apos;&apos;按照前k个奇异值的平方和占总奇异值的平方和的百分比percentage来确定k的值,
后续计算SVD时需要将原始矩阵转换到k维空间&apos;&apos;&apos;
def sigmaPct(sigma,percentage):
    sigma2=sigma**2 #对sigma求平方
    sumsgm2=sum(sigma2) #求所有奇异值sigma的平方和
    sumsgm3=0 #sumsgm3是前k个奇异值的平方和
    k=0
    for i in sigma:
        sumsgm3+=i**2
        k+=1
        if sumsgm3&gt;=sumsgm2*percentage:
            return k

&apos;&apos;&apos;函数svdEst()的参数包含：数据矩阵、用户编号、物品编号和奇异值占比的阈值，
数据矩阵的行对应用户，列对应物品，函数的作用是基于item的相似性对用户未评过分的物品进行预测评分&apos;&apos;&apos;
def svdEst(dataMat,user,simMeas,item,percentage):
    n=shape(dataMat)[1]
    simTotal=0.0;ratSimTotal=0.0
    u,sigma,vt=la.svd(dataMat)
    k=sigmaPct(sigma,percentage) #确定了k的值
    sigmaK=mat(eye(k)*sigma[:k])  #构建对角矩阵
    xformedItems=dataMat.T*u[:,:k]*sigmaK.I  #根据k的值将原始数据转换到k维空间(低维),xformedItems表示物品(item)在k维空间转换后的值
    for j in range(n):
        userRating=dataMat[user,j]
        if userRating==0 or j==item:continue
        similarity=simMeas(xformedItems[item,:].T,xformedItems[j,:].T) #计算物品item与物品j之间的相似度
        simTotal+=similarity #对所有相似度求和
        ratSimTotal+=similarity*userRating #用&quot;物品item和物品j的相似度&quot;乘以&quot;用户对物品j的评分&quot;，并求和
    if simTotal==0:return 0
    else:return ratSimTotal/simTotal #得到对物品item的预测评分

&apos;&apos;&apos;函数recommend()产生预测评分最高的N个推荐结果，默认返回5个；
参数包括：数据矩阵、用户编号、相似度衡量的方法、预测评分的方法、以及奇异值占比的阈值；
数据矩阵的行对应用户，列对应物品，函数的作用是基于item的相似性对用户未评过分的物品进行预测评分；
相似度衡量的方法默认用余弦相似度&apos;&apos;&apos;
def recommend(dataMat,user,N=5,simMeas=cosSim,estMethod=svdEst,percentage=0.9):
    unratedItems=nonzero(dataMat[user,:].A==0)[1]  #建立一个用户未评分item的列表
    print(unratedItems)
    if len(unratedItems)==0:return &apos;you rated everything&apos; #如果都已经评过分，则退出
    itemScores=[]
    for item in unratedItems:  #对于每个未评分的item，都计算其预测评分
        estimatedScore=estMethod(dataMat,user,simMeas,item,percentage)
        itemScores.append((item,estimatedScore))
    itemScores=sorted(itemScores,key=lambda x:x[1],reverse=True)#按照item的得分进行从大到小排序
    return itemScores[:N]  #返回前N大评分值的item名，及其预测评分值

testdata=loadExData()
print(recommend(testdata,1,N=3,percentage=0.8))#对编号为1的用户推荐评分较高的3件商品
</code></pre><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><p>PCA也常用于数据降维，特别是在人脸识别对于图像数据的处理中，得到了广泛的运用。数据降维的原则是使得数据维度减小，即行向量方差尽可能大，但是同时信息保留最多，就要求行向量之间相关性尽量小，即行向量之间协方差为0.<br>对于数据：</p>
<p><img src="https://i.imgur.com/t8KFr7N.png" alt=""></p>
<p>标准化处理先：</p>
<p><img src="https://i.imgur.com/dRX80IG.png" alt=""></p>
<p>那么协方差矩阵：</p>
<p><img src="https://i.imgur.com/KFs2TqU.png" alt=""></p>
<p>希望降维后协方差矩阵对角元素极可能大，非对角元素尽可能为0，即成为对角矩阵，则可对X进行线性变换，Y=QX,那么：</p>
<p><img src="https://i.imgur.com/MsufQmF.png" alt=""></p>
<p>所以，Q为CX的特征向量，其方差为特征值，进行降维一般取前r个特征值对应的特征向量，转换结果为：</p>
<p><img src="https://i.imgur.com/N9dKJmz.png" alt=""></p>
<p>另一种证明方式可见：<a href="http://blog.csdn.net/zhongkejingwang/article/details/42264479" target="_blank" rel="external">http://blog.csdn.net/zhongkejingwang/article/details/42264479</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近的机器学习算法学习到了主成分分析（PCA），在人脸识别中对样本数据进行了降维，借此对特征值分解（EVD）、奇异值分解（SVD）进行了梳理整理。&lt;/p&gt;
&lt;h2 id=&quot;特征值分解（EVD&quot;&gt;&lt;a href=&quot;#特征值分解（EVD&quot; class=&quot;headerlink&quot; title=&quot;特征值分解（EVD)&quot;&gt;&lt;/a&gt;特征值分解（EVD)&lt;/h2&gt;&lt;p&gt;矩阵是一种线性变换，比如矩阵Ax=y中，矩阵A将向量x线性变换到另一个矩阵y，这个过程中包含3类效应：旋转、缩放以及投影。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>网易云音乐爬虫连载（1）之热门歌单音乐获取</title>
    <link href="http://yoursite.com/2017/10/25/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%88%AC%E8%99%AB%E8%BF%9E%E8%BD%BD%EF%BC%881%EF%BC%89%E4%B9%8B%E7%83%AD%E9%97%A8%E6%AD%8C%E5%8D%95%E9%9F%B3%E4%B9%90%E8%8E%B7%E5%8F%96/"/>
    <id>http://yoursite.com/2017/10/25/网易云音乐爬虫连载（1）之热门歌单音乐获取/</id>
    <published>2017-10-25T08:30:47.000Z</published>
    <updated>2017-10-25T10:32:15.836Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想做文本挖掘方面的工作，想到了获取网易云音乐平台的用户评论以及音乐数据，这可以作为一个文本挖掘以及推荐系统的很好的数据来源。诚然，获取大量的数据涉及到内容还是挺多的，因此从本文开始做一个连载，记录今后对网易云音乐数据的爬取，以及今后对于获取的数据进行分析，作为机器学习的素材进一步处理。</p>
<p>作为连载的第一篇，首先就是介绍基本的网易云音乐信息获取，以及音乐评论的获取。<br><a id="more"></a></p>
<p>为了获得大量的音乐数据，从网易云音乐首页的热门歌单中入手，获取音乐信息。</p>
<p><img src="https://i.imgur.com/IFZFT1q.png" alt=""></p>
<p>用谷歌开发者工具发现，其实获取热门歌单的时候，网易云的请求包中的网址并不是截图中浏览器的<a href="http://music.163.com/#/discover/playlist，而是http://music.163.com/discover/playlist" target="_blank" rel="external">http://music.163.com/#/discover/playlist，而是http://music.163.com/discover/playlist</a></p>
<p><img src="https://i.imgur.com/XeyP1qq.png" alt=""></p>
<p>所以说爬虫的时候，不能单纯看浏览器的url，还是得看真实发送的请求包中的数据。</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import codecs
url = &apos;http://music.163.com/discover/playlist&apos;
url_top = &apos;http://music.163.com&apos;
headers = {
    &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,
    &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.9&apos;,
    &apos;Host&apos;: &apos;music.163.com&apos;,
    &quot;User-Agent&quot;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3218.0 Safari/537.36&apos;,
}
</code></pre><p>分析获取的html文档，采用的是Beautiful Soup，这个时候首先查看html文档，找到歌单数据所在位置。</p>
<p><img src="https://i.imgur.com/vxTTQWY.png" alt=""></p>
<p>发现一个id属性，id属性在一个html文档中是独一无二的，可以据此定位找出我们要的歌单信息。</p>
<pre><code>a=requests.get(url,headers=headers)
html = a.content
soup=BeautifulSoup(html,&apos;lxml&apos;)
playlist={}
f=codecs.open(&apos;playlist.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)
PlaylistBlock = soup.select(&apos;#m-pl-container&apos;)[0].select(&apos;.msk&apos;)
for piece in PlaylistBlock:
    playlist[piece.get(&apos;title&apos;)]=piece.get(&apos;href&apos;)
    f.write(piece.get(&apos;title&apos;)+&apos;: &apos;+piece.get(&apos;href&apos;)+&apos;\n&apos;)
f.close()
</code></pre><p>现在仅仅是获取了第一页的热门歌单：</p>
<p><img src="https://i.imgur.com/OK1vv6L.png" alt=""></p>
<p>歌单的链接都是相对链接，只要加上<a href="http://music.163.com就可以访问到相应的具体歌单，来进一步获取歌单内的歌曲。" target="_blank" rel="external">http://music.163.com就可以访问到相应的具体歌单，来进一步获取歌单内的歌曲。</a></p>
<p><img src="https://i.imgur.com/RKF87RZ.png" alt=""></p>
<pre><code>f=codecs.open(&apos;musiclist.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)
musiclist = []

for description,url_music in playlist.items():
    html = requests.get(url_top+url_music,headers=headers).content
    soup = BeautifulSoup(html,&apos;lxml&apos;)
    songs= soup.find(&apos;ul&apos;,{&apos;class&apos;:&apos;f-hide&apos;}).find_all(&apos;a&apos;)
    music={}
    f.writelines(description+&apos;\n&apos;)
    for song in songs:
        music[song[&apos;href&apos;]] = song.string
        a = music_info_get(url_top+song[&apos;href&apos;],headers)
        f.write(song[&apos;href&apos;]+&apos;:  &apos;+song.string+a+&apos;\n&apos;)
    musiclist.append(music)
    f.write(&apos;---------------------------------&apos;+&apos;\n&apos;)

f.close()
</code></pre><p>由于在歌单页面没有查到歌曲的对应歌手、专辑信息（理论上应该有的，但是我并没有找到），所以考虑进一步进到歌曲页面，可以看到歌曲的详细信息，后期主要在歌曲页面进行信息获取，所以在这里先进到歌曲页面获取歌手、专辑信息。</p>
<p>以赵雷的《成都》为例，进入歌曲页面</p>
<p><img src="https://i.imgur.com/0lHWxPm.png" alt=""></p>
<p>提取出歌曲的详细信息：</p>
<pre><code>def music_info_get(url,headers):
    html = requests.get(url, headers=headers).content
    soup = BeautifulSoup(html, &apos;lxml&apos;)
    a = soup.find(&apos;meta&apos;, {&apos;name&apos;: &apos;description&apos;})[&apos;content&apos;]
    return a
</code></pre><p>《成都》的网页中包含歌词、评论等信息，但是在请求返回的数据包中并没有见到，刷新《成都》页面其实有很多请求，再仔细查看之后，在其他请求中看到了包括评论以及歌词信息。</p>
<p><img src="https://i.imgur.com/T59hAoY.png" alt=""></p>
<p>在这个post请求包中，返回数据中有《成都》的音乐评论，post和get方式不同，post需要带参数，在request的header中可以看到有两个参数，</p>
<p><img src="https://i.imgur.com/mtOQ9xQ.png" alt=""></p>
<p>将这个数据带上，用request.post也确实得到了评论数据。</p>
<pre><code>params = &apos;5iLo/oxg1fK3aTLbh99GhtE6AnWBnEGVKMt4iDi6Qm9ag54eFjI/XRn2rI6QOAk8Zj6u2eS7NkRu04mUakNwntZMQrf9f6cdN6PWZuB16f0CgA0N/5IOl7tUXKZCbsduXzfpYCExtIvLDlOeu9LkGpUksFW3O0zq5ZTjRc1MrB49sxRvF8NA+U9LIMvhJHmO&apos;
encSecKey = &apos;3ae5b6afde65dede52224db59c2cc8e46aac937dd95915ba6538859aa0615cb6aa938a118fd6f473256fc5cf95d8c3821b07264d7189c07db922088b711a357e3f2092e5a10df5e3d6008a0314adcb8817fc3fe14a2ee657a0a2221597cc51a78534043a1429484a251e4b2b9128fe042d821b7e862114207773cbdba951c8a2&apos;
data={&quot;params&quot;: params, &quot;encSecKey&quot;: encSecKey}
a = requests.post(url,headers=headers,data=data)
</code></pre><p>但是post所带上的参数data，看起来应该是跟数据加密相关，每个请求应该不一样，在爬取大量音乐的时候没办法对每首歌的参数都去手动获得，这里在网上<a href="https://www.zhihu.com/question/36081767" target="_blank" rel="external">https://www.zhihu.com/question/36081767</a>看到了一个方法，之后可以参照来实现。<br>得到的评论数据：<br><img src="https://i.imgur.com/RIr58UA.png" alt=""><br>还有就是请求歌曲的评论数据的url，对《成都》来说是<a href="http://music.163.com/weapi/v1/resource/comments/R_SO_4_436514312?csrf_token=，其规律就是R_SO_4_之后的数字为歌曲对应的id，比如《成都》的URL：http://music.163.com/#/song?id=436514312" target="_blank" rel="external">http://music.163.com/weapi/v1/resource/comments/R_SO_4_436514312?csrf_token=，其规律就是R_SO_4_之后的数字为歌曲对应的id，比如《成都》的URL：http://music.163.com/#/song?id=436514312</a></p>
<p>爬虫进行到这，仅仅是从歌单到歌曲，再到歌曲信息以及评论单纯地获取了一遍，对于今后大量歌单大规模的爬取，还需要考虑很多，比如多线程爬虫，结合数据库的爬虫数据存储，以及对于长时间爬虫如何应对发爬虫策略，仅仅是获取数据就还有这么多坑，先Mark一下，留着坑慢慢填。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近想做文本挖掘方面的工作，想到了获取网易云音乐平台的用户评论以及音乐数据，这可以作为一个文本挖掘以及推荐系统的很好的数据来源。诚然，获取大量的数据涉及到内容还是挺多的，因此从本文开始做一个连载，记录今后对网易云音乐数据的爬取，以及今后对于获取的数据进行分析，作为机器学习的素材进一步处理。&lt;/p&gt;
&lt;p&gt;作为连载的第一篇，首先就是介绍基本的网易云音乐信息获取，以及音乐评论的获取。&lt;br&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>BP网络及python实现</title>
    <link href="http://yoursite.com/2017/10/23/BP%E7%BD%91%E7%BB%9C%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2017/10/23/BP网络及python实现/</id>
    <published>2017-10-23T08:48:07.000Z</published>
    <updated>2017-10-23T11:41:30.598Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>BP神经网络改变了感知器的结构，引入了新的隐含层以及误差反向传播，基本上能够解决非线性分类问题，也是神经网络的基础网络结构，在此对BP神经网络算法进行总结，并用python对其进行了实现。<br><a id="more"></a></p>
<p>BP神经网络的典型结构如下图所示：</p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1508759278442&amp;di=35b034d166ee7a0c6e09c7154c096d3f&amp;imgtype=0&amp;src=http%3A%2F%2Fimgsrc.baidu.com%2Fbaike%2Fpic%2Fitem%2F9922720e0cf3d7ca65c52b8ef01fbe096b63a912.jpg" alt=""></p>
<p>隐含层通常为一层，也可以是多层，在BP网络中一般不超过2层。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>正向传播的过程与感知器类似，都是输入与权重的点积，隐含层和输出层都包含一个激活函数，BP网络常用sigmod函数。</p>
<p><img src="https://i.imgur.com/5FpsaS2.png" alt=""></p>
<p>但是现在好像不常用了，更多地是Tanh或者是ReLU，好像最近又出了一个全新的激活函数，后续还得去了解。<br>BP神经网络的误差函数是全局误差，将所有样本的误差都进行计算求和，所以在算法过程学习的时候，进行的是批量学习，等所有数据都进行批次计算之后，才进行权重调整。</p>
<p><img src="https://i.imgur.com/vW9ndOr.png" alt=""></p>
<h2 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h2><p>这个可以说是BP网络比较精髓的部分了，也是BP网络能够从数据中学习的关键，误差的反向传播过程就是两种情况，要么输出层神经元，要么是隐含层神经元。</p>
<p><img src="https://i.imgur.com/1luEa3W.png" alt=""></p>
<p>对于输出神经元，权重的梯度修正法则为：</p>
<p><img src="https://i.imgur.com/vc9TjCN.png" alt=""></p>
<p>即权重增量等于学习率、局域梯度、输出层输出结果的乘积，对于局域梯度，其计算如下：</p>
<p><img src="https://i.imgur.com/xvHCU1P.png" alt=""></p>
<p>即为误差信号乘于激活函数的导数，其中n表示第n次迭代。<br>对于sigmod函数来说，其导数为：</p>
<p><img src="https://i.imgur.com/nQVO1tw.png" alt=""></p>
<p>对于隐藏层来说，情况更加复杂一点，需要经过上一层的误差传递。</p>
<p><img src="https://i.imgur.com/Ep6t5KR.png" alt=""></p>
<p>隐藏层的局域梯度为：</p>
<p><img src="https://i.imgur.com/pC56jAc.png" alt=""></p>
<p>上面式子的第一项，说明隐含层神经元j局域梯度的计算仅以来神经元j的激活函数的导数，但是第二项求和，是上一层神经元的局域梯度通过权重w进行了传递。</p>
<p>总的来说，反向传播算法中，权重的调整值规则为：</p>
<p>（权值调整）=（学习率参数） X （局域梯度） X（神经元j的输入信号）</p>
<p>BP算法中还有一个动量因子（mc），主要是网络调优，防止网络发生震荡或者收敛过慢，其基本思想就是在t时刻权重更新的时候考虑t-1时刻的梯度值。</p>
<pre><code>self.out_wb = self.out_wb + (1-self.mc)*self.eta*dout_wb + self.mc*self.eta*dout_wbold
self.hi_wb =self.hi_wb + (1-self.mc)*self.eta*dhi_wb + self.mc*self.eta*dhi_wbold
</code></pre><h2 id="BP网络分类算法"><a href="#BP网络分类算法" class="headerlink" title="BP网络分类算法"></a>BP网络分类算法</h2><p>首先构造的一个BP类</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Mon Oct 23 09:12:46 2017

@author: lkj
&quot;&quot;&quot;

import numpy as np
from numpy import *
import matplotlib.pyplot as plt

class BpNet(object):
    def __init__(self):
        # 以下参数需要手动设置  
        self.eb=0.01              # 误差容限，当误差小于这个值时，算法收敛，程序停止
        self.eta=0.1             # 学习率
        self.mc=0.3               # 动量因子：引入的一个调优参数，是主要的调优参数 
        self.maxiter=2000         # 最大迭代次数
        self.errlist=[]           # 误差列表
        self.dataMat=0            # 训练集
        self.classLabels=0        # 分类标签集
        self.nSampNum=0             # 样本集行数
        self.nSampDim=0             # 样本列数
        self.nHidden=4           # 隐含层神经元 
        self.nOut=1              # 输出层个数
        self.iterator=0            # 算法收敛时的迭代次数

    #激活函数
    def logistic(self,net):
        return 1.0/(1.0+exp(-net))

    #反向传播激活函数的导数
    def dlogistic(self,y):
        return (y*(1-y))

    #全局误差函数
    def errorfuc(self,x):
        return sum(x*x)*0.5

    #加载数据集
    def loadDataSet(self,FileName):
        data=np.loadtxt(FileName)
        m,n=shape(data)
        self.dataMat = np.ones((m,n))
        self.dataMat[:,:-1] = data[:,:-1] #除数据外一列全为1的数据，与权重矩阵中的b相乘
        self.nSampNum = m  #样本数量
        self.nSampDim = n-1  #样本维度
        self.classLabels =data[:,-1]    

    #数据集归一化，使得数据尽量处在同一量纲，这里采用了标准归一化
    #数据归一化应该针对的是属性，而不是针对每条数据
    def normalize(self,data):
        [m,n]=shape(data)
        for i in range(n-1):
            data[:,i]=(data[:,i]-mean(data[:,i]))/(std(data[:,i])+1.0e-10)
        return data

    #隐含层、输出层神经元权重初始化
    def init_WB(self):
        #隐含层
        self.hi_w = 2.0*(random.rand(self.nSampDim,self.nHidden)-0.5)
        self.hi_b = 2.0*(random.rand(1,self.nHidden)-0.5)
        self.hi_wb = vstack((self.hi_w,self.hi_b))

        #输出层
        self.out_w = 2.0*(random.rand(self.nHidden,self.nOut)-0.5)
        self.out_b = 2.0*(random.rand(1,self.nOut)-0.5)
        self.out_wb = vstack((self.out_w,self.out_b))

    def BpTrain(self):
        SampIn = self.dataMat
        expected = self.classLabels
        dout_wbold = 0.0
        dhi_wbold = 0.0 #记录隐含层和输出层前一次的权重值，初始化为0
        self.init_WB()

        for i in range(self.maxiter):
            #信号正向传播
            #输入层到隐含层
            hi_input = np.dot(SampIn,self.hi_wb)
            hi_output = self.logistic(hi_input)
            hi2out = np.hstack((hi_output,np.ones((self.nSampNum,1))))

            #隐含层到输出层
            out_input=np.dot(hi2out,self.out_wb)
            out_output = self.logistic(out_input)
            #计算误差
            error = expected.reshape(shape(out_output)) - out_output
            sse = self.errorfuc(error)
            self.errlist.append(sse)
            if sse&lt;=self.eb:
                self.iterator = i+1
                break

            #误差反向传播

            #DELTA输出层梯度
            DELTA = error*self.dlogistic(out_output)
            #delta隐含层梯度
            delta =  self.dlogistic(hi_output)*np.dot(DELTA,self.out_wb[:-1,:].T)
            dout_wb = np.dot(hi2out.T,DELTA)
            dhi_wb = np.dot(SampIn.T,delta)

            #更新输出层和隐含层权值
            if i==0:
                self.out_wb = self.out_wb + self.eta*dout_wb
                self.hi_wb = self.hi_wb + self.eta*dhi_wb
            else:
                #加入动量因子
               self.out_wb = self.out_wb + (1-self.mc)*self.eta*dout_wb + self.mc*self.eta*dout_wbold
               self.hi_wb =self.hi_wb + (1-self.mc)*self.eta*dhi_wb + self.mc*self.eta*dhi_wbold
            dout_wbold = dout_wb
            dhi_wbold = dhi_wb

    ##输入测试点，输出分类结果      
    def BpClassfier(self,start,end,steps=30):
        x=linspace(start,end,steps)
        xx=np.ones((steps,steps))
        xx[:,0:steps] = x
        yy = xx.T
        z = np.ones((steps,steps))
        for i in  range(steps):
            for j in range(steps):
                xi=array([xx[i,j],yy[i,j],1])
                hi_input = np.dot(xi,self.hi_wb)
                hi_out = self.logistic(hi_input)
                hi_out = mat(hi_out)
                m,n=shape(hi_out)
                hi_b = ones((m,n+1))
                hi_b[:,:n] = hi_out
                out_input = np.dot(hi_b,self.out_wb)
                out = self.logistic(out_input)
                z[i,j] = out
        return x,z

    def classfyLine(self,plt,x,z):
        #画出分类分隔曲线，用等高线画出
        plt.contour(x,x,z,1,colors=&apos;black&apos;)

    def errorLine(self,plt,color=&apos;r&apos;):
        x=linspace(0,self.maxiter,self.maxiter)
        y=log2(self.errlist)
        #y=y.reshape(())
        #print(shape(x),shape(y))
        plt.plot(x,y,color)

   # 绘制数据散点图
    def drawDataScatter(self,plt):
        i=0
        for data in self.dataMat:
            if(self.classLabels[i]==0):
                plt.scatter(data[0],data[1],c=&apos;blue&apos;,marker=&apos;o&apos;)
            else:
                plt.scatter(data[0],data[1],c=&apos;red&apos;,marker=&apos;s&apos;)
            i=i+1
</code></pre><p>利用分类器执行分类：</p>
<pre><code>from BpNet import *
import matplotlib.pyplot as plt 

# 数据集
bpnet = BpNet() 
bpnet.loadDataSet(&quot;testSet2.txt&quot;)
bpnet.dataMat = bpnet.normalize(bpnet.dataMat)

# 绘制数据集散点图
bpnet.drawDataScatter(plt)

# BP神经网络进行数据分类
bpnet.BpTrain()

print(bpnet.out_wb)
print(bpnet.hi_wb)

# 计算和绘制分类线
x,z = bpnet.BpClassfier(-3.0,3.0)
bpnet.classfyLine(plt,x,z)
plt.show()
# 绘制误差曲线
bpnet.errorLine(plt)
plt.show()            
</code></pre><p>输出结果为：</p>
<p><img src="https://i.imgur.com/GYv2q53.png" alt=""></p>
<p>误差输出结果：</p>
<p><img src="https://i.imgur.com/xAXcgfX.png" alt=""></p>
<p>可以看到在1000次左右迭代就已经出现了比较好的结果了。<br>具体代码可见个人github仓库<a href="https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/BpNet" target="_blank" rel="external">https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/BpNet</a>        </p>
<p>除了分类，BP神经网络也常用在函数逼近，这时候输出层神经元激活函数一般就不会再采用sigmod函数了，通常采用线性函数。</p>
<p><strong>【参考文献】</strong><br>《神经网络与机器学习》（第3版） （加） Simon Haykin 著；<br>《机器学习算法原理与编程实践》 郑捷著；</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BP神经网络改变了感知器的结构，引入了新的隐含层以及误差反向传播，基本上能够解决非线性分类问题，也是神经网络的基础网络结构，在此对BP神经网络算法进行总结，并用python对其进行了实现。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://yoursite.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost算法笔记</title>
    <link href="http://yoursite.com/2017/10/22/AdaBoost%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/10/22/AdaBoost算法/</id>
    <published>2017-10-22T07:35:19.000Z</published>
    <updated>2018-07-05T14:31:16.616Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>AdaBoost属于机器学习中的集成学习，其基本思想是基于“弱学习算法”集成强化为“强学习算法”，在分类中就是组合弱分类器得到强分类器。实际中很容易得到正确率不是非常高的弱分类器(当然，通常来说at least &gt;50%），通过Adaboost算法，来组合得到正确率很高的强分类器。<br><a id="more"></a><br>从上述就可以知道，AdaBoost算法有两部分，顶层是主算法，底层是其他的分类算法，可以是决策树，SVM之类的算法</p>
<p><img src="https://i.imgur.com/YrWP4SU.jpg" alt=""></p>
<p>AdaBoost算法的核心思想就是：加大分类错误数据集的权重，以便迭代中进一步分类；加大正确率高的分类器在最终分类结果表决中的权重。</p>
<p>对算法进一步阐述，考虑简单的二分类问题。</p>
<ol>
<li>初始化数据集的权值分布，初始化为相等的数值</li>
</ol>
<p><img src="https://i.imgur.com/ryYjqNg.png" alt=""></p>
<ol>
<li>使用训练集学习，得到基本分类器Gm,计算其分类误差率</li>
</ol>
<p><img src="https://i.imgur.com/kelrKCR.png" alt=""></p>
<ol>
<li>计算分类器Gm的系数</li>
</ol>
<p><img src="https://i.imgur.com/4cl86ii.png" alt=""></p>
<p>am随着em增大而减小，这样分类器在最终组合分类器中权重会下降，同时还需要据此来更新数据集的权重。</p>
<ol>
<li>更新数据集权重分布</li>
</ol>
<p><img src="https://i.imgur.com/amKwnm4.png" alt=""></p>
<p>其中Zm为规范化因子，是的Dm+1成一个概率分布。</p>
<ol>
<li>构造弱分类器的组合<br>通过权重值的线性组合得到强分类器<br><img src="https://i.imgur.com/0cjwKb5.png" alt=""></li>
</ol>
<p>上述过程迭代M次，或者是分类精度达到要求</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;AdaBoost属于机器学习中的集成学习，其基本思想是基于“弱学习算法”集成强化为“强学习算法”，在分类中就是组合弱分类器得到强分类器。实际中很容易得到正确率不是非常高的弱分类器(当然，通常来说at least &amp;gt;50%），通过Adaboost算法，来组合得到正确率很高的强分类器。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>下完这场雨</title>
    <link href="http://yoursite.com/2017/10/20/%E4%B8%8B%E5%AE%8C%E8%BF%99%E5%9C%BA%E9%9B%A8/"/>
    <id>http://yoursite.com/2017/10/20/下完这场雨/</id>
    <published>2017-10-20T13:30:47.000Z</published>
    <updated>2018-06-08T14:34:35.060Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1508516341880&amp;di=9aa12487fe827b6b8a9082e544536914&amp;imgtype=0&amp;src=http%3A%2F%2Fimg.ylq.com%2F2016%2F0819%2F20160819055353748.png" alt=""></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="530" height="86" src="//music.163.com/outchain/player?type=2&id=434550534&auto=0&height=66"></iframe>

<p>偶然间网易云推送了这首《下完这场雨》，多久没有再听过后弦的歌了，这首17年的可以说是新歌，歌词一贯写得好，一如六七年前的清新中国风。江南烟雨中，伞下约无果；下完这场雨，此生难再遇。</p>
<a id="more"></a>
<p><img src="https://i.imgur.com/UPsFARi.png" alt=""></p>
<p>仿佛又回到了六七年前，戴着耳机，揣着MP3，走在凤凰花园去学校的路上，穿过西厢，看过娃娃脸，有这样喜欢的清新中国风，有许嵩，有庐州月，断桥残雪，清明雨上，半城烟沙。。。<br>真好。<br>如果可以 别下完这场雨</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://timgsa.baidu.com/timg?image&amp;amp;quality=80&amp;amp;size=b9999_10000&amp;amp;sec=1508516341880&amp;amp;di=9aa12487fe827b6b8a9082e544536914&amp;amp;imgtype=0&amp;amp;src=http%3A%2F%2Fimg.ylq.com%2F2016%2F0819%2F20160819055353748.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;530&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=434550534&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;偶然间网易云推送了这首《下完这场雨》，多久没有再听过后弦的歌了，这首17年的可以说是新歌，歌词一贯写得好，一如六七年前的清新中国风。江南烟雨中，伞下约无果；下完这场雨，此生难再遇。&lt;/p&gt;
    
    </summary>
    
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>12306车次查询与短信提醒</title>
    <link href="http://yoursite.com/2017/10/02/12306%E8%BD%A6%E6%AC%A1%E6%9F%A5%E8%AF%A2%E4%B8%8E%E7%9F%AD%E4%BF%A1%E6%8F%90%E9%86%92/"/>
    <id>http://yoursite.com/2017/10/02/12306车次查询与短信提醒/</id>
    <published>2017-10-02T02:33:05.000Z</published>
    <updated>2017-10-02T05:35:41.814Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>正值国庆，不想出去玩看人，也无法在实验室知识的海洋里遨游整个假期，就干脆回家，无奈却买不到回来的票（国庆真是堪比春运啊），只能买到途径站的票。12306官方给的自动刷新车票的系统太渣，经常自己卡死停掉，因此想到用python爬取12306网站车票信息，利用Twilio在有票的情况下短信通知我。</p>
<p><img src="https://i.imgur.com/DzxBB97.png" alt=""></p>
<a id="more"></a>
<p>首先打开12306的查票页面<a href="https://kyfw.12306.cn/otn/leftTicket/init" target="_blank" rel="external">https://kyfw.12306.cn/otn/leftTicket/init </a>输入出发地和目的地进行搜索。<br>下面就是常见的套路，对于谷歌浏览器按F12打开开发者工具，获取查询的时候的数据信息。按下查询之后，可以发现有两个数据请求：</p>
<p><img src="https://i.imgur.com/fGEXVjb.png" alt=""></p>
<p>显然第二个请求返回数据含有我们需要的车次信息，因此根据第二个请求的header来封装我们requests.get时候的请求包，以及如何封装cookies也可见之前的<a href="http://blog.lkj666.top/2017/08/08/也看战狼2：爬取豆瓣影评做词云/#more" target="_blank" rel="external">爬虫博客</a>。</p>
<pre><code>url = &apos;https://kyfw.12306.cn/otn/leftTicket/queryX?leftTicketDTO.train_date=2017-10-08&amp;leftTicketDTO.from_station=YTG&amp;leftTicketDTO.to_station=HZH&amp;purpose_codes=ADULT&apos;
headers = {
    &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.9&apos;,
    &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.9&apos;,
    &apos;Host&apos;: &apos;kyfw.12306.cn&apos;,
    &quot;User-Agent&quot;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3218.0 Safari/537.36&apos;,
}
</code></pre><p>由于12306网站还有SSL验证，所以用常规的方法去get数据都会有SSLError，最简单的处理方式就是直接忽略这个验证。</p>
<pre><code>a = requests.get(url, cookies=cookies,headers=headers,verify = False)
</code></pre><p>12306网站在访问的时候经常有时候请求不到数据，这个时候需要进行判别，以防止后续处理信息报错，有计算机网络知识就知道，正常的http response的状态code就是会返回200，可以据此来判别。</p>
<pre><code>while (a.status_code !=200):
    time.sleep(10)
    a = requests.get(url, cookies=cookies, headers=headers, verify=False)
</code></pre><p>对返回的车次信息进行查看，返回的信息类似于json格式的文件，车次信息包含在里面。</p>
<p><img src="https://i.imgur.com/PESBfPR.png" alt=""></p>
<p>通过json.load进行读取后返回字典格式文件，取出其中的车次此案次，存为csv文件后用excel打开，以便更加直观地分析。</p>
<p><img src="https://i.imgur.com/PpUZ7Qk.png" alt=""></p>
<pre><code>data = json.loads(a.content.decode(&apos;utf-8&apos;,&apos;ignore&apos;))
train_infos = data[&apos;data&apos;][&apos;result&apos;]
train_infos_csv=[]
infos = train_infos
for info in infos:
    train_infos_csv.append((info.replace(&apos;|&apos;,&apos;,&apos;))+&apos;\n&apos;)
f = open(&apos;train_infos.csv&apos;,&apos;w&apos;)
for info in train_infos_csv:
    f.write(info)

f.close()
csv_reader = csv.reader(open(&apos;train_infos.csv&apos;))
</code></pre><p>下面需要取出所需要的信息，我要的是高铁或者动车的二等座，再进行一次过滤：</p>
<pre><code>HSR_infos=[]
for info in csv_reader:
    if(&apos;G&apos; in info[3]):
        HSR_infos.append(info)
    elif(&apos;D&apos; in info[3]):
        HSR_infos.append(info)

Ticket_avaliable=&apos;&apos;
for info in HSR_infos:
    if(info[-6] != u&apos;无&apos;):
        Ticket_avaliable=Ticket_avaliable + info[3] +&apos;,&apos;
Body=u&apos;这些车次还有票&apos; + Ticket_avaliable
if(Ticket_avaliable ==&apos;&apos;):
    print(u&apos;好悲伤，没票了&apos;)
    time.sleep(5)
else:
    print(Body)
    messeage = client.messages.create(to=To, from_=From, body=Body)
    time.sleep(600)
</code></pre><p>上面的client.messages.create就是发送短信模块，需要用到Twilio，首先需要到Twilio官方网站注册之后账号，有账户的SID 和TOKEN后进行配置，就能发送了</p>
<pre><code>SID = &apos;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&apos;
TOKEN = &apos;xxxxxxxxxxxxxxxxxxxxxxxxxxxxx&apos;
client = Client(SID,TOKEN)
To = &apos;+86xxxxxxxxxxxxxxx&apos;
From = &apos;xxxxxxxxxxxxxx&apos;
Body = u&apos;啦啦啦啦啦&apos;
messeage = client.messages.create(to=To,from_=From,body=Body)
</code></pre><p>上面的xxxx部分都需要根据自己的账户替换自己的信息。<br>这样就实现了有票自动提醒，由于Twilio接收短信比较麻烦，所以没法通过短信终止程序，只能在查到票之后先终止运行10分钟，其实可以考虑通过邮件来控制程序，这个还是比较好操作的。</p>
<p>完整代码可见<a href="https://github.com/lkj1114889770/WebScraping/tree/master/12306_Webscraping" target="_blank" rel="external">Github</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正值国庆，不想出去玩看人，也无法在实验室知识的海洋里遨游整个假期，就干脆回家，无奈却买不到回来的票（国庆真是堪比春运啊），只能买到途径站的票。12306官方给的自动刷新车票的系统太渣，经常自己卡死停掉，因此想到用python爬取12306网站车票信息，利用Twilio在有票的情况下短信通知我。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/DzxBB97.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>自组织特征映射神经网络（SOM）</title>
    <link href="http://yoursite.com/2017/09/30/%E8%87%AA%E7%BB%84%E7%BB%87%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88SOM%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/自组织特征映射神经网络（SOM）/</id>
    <published>2017-09-30T02:39:06.000Z</published>
    <updated>2017-09-30T05:53:29.316Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自组织特征映射神经网络（SOM）设计思想基于人体大脑，外界信息输入时，大脑皮层对应区域的部分神经元会产生兴奋，位置临近的神经云也会有相近的刺激。大脑神经元的这种特点，不是先天安排好的，而是通过后天的自学习组织形成的。芬兰Helsinki大学的Kohonen教授提出了一种成为自组织特征映射的神经网络模型。<br><a id="more"></a></p>
<h2 id="SOM介绍"><a href="#SOM介绍" class="headerlink" title="SOM介绍"></a>SOM介绍</h2><p>SOM与kmeans算法有点相似，其基本思想也是，将距离小的个体集合划分为同一类别，而将距离大的个体划分为不同的类别。</p>
<p><img src="http://appwk.baidu.com/naapi/doc/view?ih=584&amp;o=png_6_0_0_436_680_396_210_876_1252.5&amp;iw=1100&amp;ix=0&amp;iy=414&amp;aimw=1100&amp;rn=1&amp;doc_id=d6e8e02c647d27284b735162&amp;pn=1&amp;sign=b919aed799fa017c1e7f322b49ebb944&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt="SOM结构图"></p>
<p>从结构上看，SOM比较简单，只有两层，输入层和竞争层，只有输入层到竞争层的权重向量需要训练，不同于其他神经网络，竞争层同层之间的神经元还有侧向连接，在学习的过程中还会相互影响。竞争层神经元的竞争通过神经元对应的权重向量和输入样本的距离比较，距离最近的神经元成为获胜节点。</p>
<p>常见的相互连接的调整方式有以下几种</p>
<ol>
<li>墨西哥草帽函数：获胜节点，有最大的权值调整量，离获胜节点越远，调整量越小，甚至达到某一距离，调整量还会变为负值。如图a所示。</li>
<li>大礼帽函数：墨西哥草帽函数的简化，如图b所示。</li>
<li>厨师帽函数：大礼帽函数的简化，如图c所示。</li>
</ol>
<p><img src="https://i.imgur.com/JtGnkfW.png" alt=""></p>
<h2 id="SOM算法学习过程"><a href="#SOM算法学习过程" class="headerlink" title="SOM算法学习过程"></a>SOM算法学习过程</h2><h3 id="网络初始化"><a href="#网络初始化" class="headerlink" title="网络初始化"></a>网络初始化</h3><p>输入层网络节点数与输入样本维度（列数）相同，通常需要进行数据归一化，常见的方法是标准归一化。<br>竞争层网络根据数据维度以及分类类别数来确定，二维数据和4种分类的话<br>，竞争层含4个节点，但是权重矩阵为4X2，权重矩阵的初始化一般·随机给一个0-1之间的随机值。对于分类类别数目不清楚的情况，可以定义竞争层多于可能的实际分类的节点，这样最后训练结果中不对应分类结果的节点始终不会收到刺激而兴奋，即抑制。</p>
<p>学习率会影响收敛速度，一般定义一个动态的学习率，随迭代次数增加而递减。优胜邻域半径也定义为一个动态收缩的，随着迭代次数增加而递减。</p>
<pre><code># 学习率和学习半径函数    
def ratecalc(self,indx):
    lrate = self.lratemax-(float(indx)+1.0)/float(self.Steps)*(self.lratemax-self.lratemin) 
    r = self.rmax-(float(indx)+1.0)/float(self.Steps)*(self.rmax-self.rmin)
    return lrate,r
</code></pre><h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>训练过程如下：</p>
<ol>
<li>随机抽取一个数据样本，计算竞争层中神经元对应的权重矩阵与数据样本的距离，找到距离最近的为获胜节点。</li>
<li>根据优胜邻域半径，找出此邻域内的所有节点。</li>
<li>根据学习率调整优胜邻域半径内的所有节点，然后回到步骤1进行迭代，直到到达相应的迭代次数</li>
<li>根据最终的迭代结果，为分类结果分配标签。</li>
</ol>
<pre><code># 主算法    
def train(self):
    #1 构建输入层网络
    dm,dn = shape(self.dataMat) 
    normDataset = self.normalize(self.dataMat) # 归一化数据x
    #2 构建分类网格
    grid = self.init_grid() # 初始化第二层分类网格 
    #3 构建两层之间的权重向量
    self.w = random.rand(dn,self.M*self.N); #随机初始化权值 w
    distM = self.distEclud   # 确定距离公式
    #4 迭代求解
    if self.Steps &lt; 10*dm:    self.Steps = 10*dm   # 设定最小迭代次数
    for i in range(self.Steps):     
        lrate,r = self.ratecalc(i) # 计算学习率和分类半径
        self.lratelist.append(lrate);self.rlist.append(r)
        # 随机生成样本索引，并抽取一个样本
        k = random.randint(0,dm) 
        mySample = normDataset[k,:]     

        # 计算最优节点：返回最小距离的索引值
        minIndx= (distM(mySample,self.w)).argmin()
        d1 = int(round(minIndx - r));
        d2 = int(round(minIndx + r));

        if(d1&lt;0): 
            d1=0
        if(d2&gt;(shape(self.w)[1]-1)):
            d2= shape(self.w)[1]-1
        di=d1;
        #print(d1,d2)
        while(di&lt;=d2):
            self.w[:,di] = self.w[:,di]+lrate*(mySample[0]-self.w[:,di])
            di=di+1

    # 分配类别标签
    for i in range(dm):
        self.classLabel.append(distM(normDataset[i,:],self.w).argmin())
    self.classLabel = mat(self.classLabel)        
</code></pre><p>具体实现代码可见：<a href="https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/SOM" target="_blank" rel="external">https://github.com/lkj1114889770/Machine-Leanring-Algorithm/tree/master/SOM</a></p>
<p>针对数据集的分类结果：</p>
<p><img src="https://i.imgur.com/KSlpNem.png" alt=""></p>
<p><strong><br>参考文献：</strong></p>
<ol>
<li>《机器学习算法与编程实践》 郑捷著；</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自组织特征映射神经网络（SOM）设计思想基于人体大脑，外界信息输入时，大脑皮层对应区域的部分神经元会产生兴奋，位置临近的神经云也会有相近的刺激。大脑神经元的这种特点，不是先天安排好的，而是通过后天的自学习组织形成的。芬兰Helsinki大学的Kohonen教授提出了一种成为自组织特征映射的神经网络模型。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://yoursite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>浓烟下的诗歌电台</title>
    <link href="http://yoursite.com/2017/09/22/%E6%B5%93%E7%83%9F%E4%B8%8B%E7%9A%84%E8%AF%97%E6%AD%8C%E7%94%B5%E5%8F%B0/"/>
    <id>http://yoursite.com/2017/09/22/浓烟下的诗歌电台/</id>
    <published>2017-09-22T14:41:45.000Z</published>
    <updated>2017-09-22T14:55:39.838Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="http://clubimg.dbankcdn.com/data/attachment/forum/201612/08/192259beyumvep5wmavc1e.jpg" alt=""></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="530" height="86" src="//music.163.com/outchain/player?type=2&id=31445772&auto=0&height=66"></iframe>

<p>诗化的语言<br>堆砌的意向<br>磁性的烟嗓<br>浓烟下的诗歌电台</p>
<p>有颗远方的心，依然想去追寻。</p>
]]></content>
    
    <summary type="html">
    
      &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
    
    </summary>
    
    
      <category term="音乐" scheme="http://yoursite.com/tags/%E9%9F%B3%E4%B9%90/"/>
    
  </entry>
  
  <entry>
    <title>K-means Clustering</title>
    <link href="http://yoursite.com/2017/09/22/K_means%20Clustering/"/>
    <id>http://yoursite.com/2017/09/22/K_means Clustering/</id>
    <published>2017-09-22T06:56:04.000Z</published>
    <updated>2017-09-22T07:47:58.569Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>分类通常是一种监督式学习算法，事先都知道标签信息，但是实际上很多情况下都不知道标签信息，这个时候就经常用到聚类算法（Clustering），属于无监督学习的一种，本文介绍无监督学习中典型的一种k-means聚类算法。<br><a id="more"></a></p>
<h2 id="conventional-k-means"><a href="#conventional-k-means" class="headerlink" title="conventional k-means"></a>conventional k-means</h2><p>k-means聚类算法的思想很简单，就是将数据相似度最大的聚集在一起为一类，如何衡量数据之间的相似度，通常用欧几里得距离来表示：</p>
<p><img src="https://i.imgur.com/lhYz2GX.png" alt=""></p>
<p>有时候也用余弦向量来度量：</p>
<p><img src="https://i.imgur.com/6DqsyIq.png" alt=""></p>
<p>算法过程也比较简单：</p>
<ol>
<li>从数据集D中随机取k个元素，作为初始k个簇的中心</li>
<li>计算剩下的元素到k个中心的距离，并将其归类到离自己最近的簇中心点对应的簇</li>
<li>重新计算每个簇的中心，采用簇中所有元素各个维度的算数平均数</li>
<li>若新的簇的中心不变，或者在变化阈值内，则聚类结束，否则重新回到第2步。</li>
</ol>
<h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means++"></a>k-means++</h2><p>k-means算法的一个弊端就是，初始选取的随机k个中心会对最终实际聚类效果影响很大，基于此对于初始k个点选取进行了改进，即k-measn++算法。<br>基本思想就是，选取的初始k个点的距离尽可能远。</p>
<ol>
<li>首先随机选择一个点作为第一个簇中心点</li>
<li>计算其余点与最近的一个簇中心点的距离D(x)保存在一个数组，并累加得到和sum（D(X))。</li>
<li>再在（0，1）取一个随机值Random，sum*Random对应的D(x)区间即为选中的下一个聚类中心（因为D(X)越大，被选中的概率越大）。</li>
<li>重复2 3步骤知直到k个初始聚类中心都被找出来</li>
<li>再进行上面的k-means聚类算法。</li>
</ol>
<h2 id="Kernel-k-means"><a href="#Kernel-k-means" class="headerlink" title="Kernel k-means"></a>Kernel k-means</h2><p>当数据无法线性可分的时候，k-means算法也无法进行分类，类似于SVM，将分类空间推广到更广义的度量空间，即为kernel k-means.</p>
<p><img src="https://i.imgur.com/FkIdHw1.jpg" alt=""></p>
<p>将点从原来的空间映射到更高维度的特征空间，则距离公式变成：</p>
<p><img src="https://i.imgur.com/QSX5K1g.png" alt=""></p>
<p>常见的核函数有：<br>Linear Kernel:</p>
<p><img src="https://i.imgur.com/Pamhtvp.png" alt=""></p>
<p>Polynomial Kernel:</p>
<p><img src="https://i.imgur.com/8c4UlBi.png" alt=""></p>
<p>Gaussian Kernel：</p>
<p><img src="https://i.imgur.com/Q0fK08M.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分类通常是一种监督式学习算法，事先都知道标签信息，但是实际上很多情况下都不知道标签信息，这个时候就经常用到聚类算法（Clustering），属于无监督学习的一种，本文介绍无监督学习中典型的一种k-means聚类算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://yoursite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯分类</title>
    <link href="http://yoursite.com/2017/09/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/"/>
    <id>http://yoursite.com/2017/09/19/朴素贝叶斯分类/</id>
    <published>2017-09-19T12:36:16.000Z</published>
    <updated>2017-09-19T13:16:18.084Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>贝叶斯算法是分类算法的一种，算法的核心是概率论中的贝叶斯定理，而朴素贝叶斯则是贝叶斯分类算法中最简单的一种。<br><a id="more"></a></p>
<h2 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h2><p>贝叶斯算法的核心就是贝叶斯定理：</p>
<p><img src="https://i.imgur.com/Mc59jtP.png" alt=""></p>
<p>而在分类算法中，B为某个类别，A为特征，P(B|A)为某个待分类个体的特征取不同类别的概率，显然是计算取不同类别的概率，取最大概率作为该个体的分类类别；上式右边的三个式子的取值通常可以根据已知数据集即训练集求取。</p>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯的“朴素”体现在样本的各个特征都是条件独立的，这样就极大地简化了算法，当然这是一个比较强的条件，在实际应用中不一定符合。算法流程如下：</p>
<ol>
<li>样本x的特征集合{a1,a2,…am}</li>
<li>所有分类类别取值{y1,y2,…yn}</li>
<li>计算所有P(y1|x),P(y2|x)….,P(yn|x)</li>
<li>取3中最大概率对应的分类为x的分类</li>
</ol>
<p>根据贝叶斯定理，可以转换成求解：</p>
<p><img src="https://i.imgur.com/rRISY1C.png" alt=""></p>
<p>因为特征之间的独立性，那么有：</p>
<p><img src="https://i.imgur.com/4NRfZsa.png" alt=""></p>
<p>而对于P(yi)可以从样本中得到，分母特征的取值概率通常对于一组特定的特征组合为常数，可以不用求解，至此完成了朴素贝叶斯算法的介绍。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;贝叶斯算法是分类算法的一种，算法的核心是概率论中的贝叶斯定理，而朴素贝叶斯则是贝叶斯分类算法中最简单的一种。&lt;br&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
